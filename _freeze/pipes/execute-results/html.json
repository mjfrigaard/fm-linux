{
  "hash": "64714a8da991255b942f038e1634d7f7",
  "result": {
    "engine": "knitr",
    "markdown": "---\nengine: knitr\nknitr:\n  opts_chunk: \n    collapse: true\n    comment: '#'\n---\n\n\n\n\n\n# Pipes {#sec-pipes}\n\n\n\n\n\n\n\n\n\n\n:::: {.callout-warning collapse='false' appearance='default' icon=false}\n\n## [Caution]{style='font-weight: bold; font-size: 1.25em;'}\n\n::: {style='font-size: 1.00em; color: #282b2d;'}\n\n\nThis section is being revised. Thank you for your patience.\n\n::: \n\n::::\n\n\n\n\n\nThe Unix pipe, denoted by the vertical bar `|`, is a powerful feature of Unix and Unix-like operating systems that allows the output of one command (`stdout`) to be used as the input to another (`stdin`). This capability forms the basis of the Unix philosophy of building small, modular utilities that do one thing well and connecting them together to perform complex tasks.\n\n## [Fundamental Concept]{style=\"font-size: 1.05em; font-weight: bold;\"} \n\nThe pipe is placed between two commands and directs the standard output (`stdout`) of the command to the left of the pipe to the standard input (`stdin`) of the command to the right.\n\n\n\n\n\n\n:::: {.callout-important collapse='true' appearance='simple' icon=false}\n\n## [Standard Input and Output]{style='font-weight: bold; font-size: 1.15em;'}\n\n::: {style='font-size: 1.00em; color: #282b2d;'}\n\n\n\n\n<br>\n\n+ `stdin` (standard input) is a text stream from which a command reads its input. By default, it's the keyboard, but it can be redirected to read from a file or another command's output.\n\n  \n+ `stdout` (standard output) is a text stream where a command writes its output. Typically, this is the terminal screen, but it can be redirected to a file or another command's input.\n\n::: \n\n::::\n\n\n\n\n\n**Example**\n\n**`echo \"Hello, World!\" | wc -w`** sends the output of the <code>@sec-echo</code> command to <code>@sec-wc</code>, which then counts the words.\n\n\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\necho \"Hello, World!\" | wc -w\n#        2\n```\n:::\n\n\n\n\n\nThe output is `2`, indicating there are two words in \"Hello, World!\".\n\n## [Combining Pipes]{style=\"font-size: 1.05em; font-weight: bold;\"} \n\nCommands can be chained together using multiple pipes, allowing for the creation of command pipelines where data is processed in stages.\n\n**Example**\n\n**<code>@sec-ps</code>`aux | `<code>@sec-grep</code> `httpd`** lists all processes, filters those containing \"httpd\" (HTTPD = web server processes running):\n\n\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nps aux | grep httpd\n# mjfrigaard        3323   0.0  0.0 33606232    648   ??  S     9:15AM   0:00.00 grep httpd\n# mjfrigaard        3321   0.0  0.0 33597548    904   ??  S     9:15AM   0:00.00 bash -c ps aux | grep httpd\n# mjfrigaard        3320   0.0  0.0 33597548    924   ??  S     9:15AM   0:00.01 sh -c 'bash'  -c 'ps aux | grep httpd' 2>&1\n```\n:::\n\n\n\n\n\n**Example**\n\n**<code>@sec-wc</code>` -l`** counts the number of lines:\n\n\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nps aux | grep httpd | wc -l\n#        3\n```\n:::\n\n\n\n\n\n### Filtering and Processing\n\n**Example 1**\n\n**<code>@sec-cat</code>`data/roxanne.txt | `<code>@sec-grep</code>` \"night\"`** displays lines from `data/roxanne.txt` that contain the number `\"2\"`.\n\n\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncat data/roxanne.txt | grep \"night\"\n# You don't have to sell your body to the night\n# You don't have to wear that dress tonight\n```\n:::\n\n\n\n\n\nHere, <code>@sec-cat</code> outputs the file's contents, which <code>@sec-grep</code> filters.\n\n**Example 2**\n\n**<code>@sec-ls</code>` -l data | `<code>@sec-sort</code>` -r`** lists the files in `data` in a detailed format, then sorts them in reverse order.\n\n\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nls -l data | sort -r\n# total 224\n# -rw-r--r--@ 1 mjfrigaard  staff  12057 Apr 22 14:11 pwrds.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff  12057 Apr 22 14:11 pwrds.csv\n# -rw-r--r--@ 1 mjfrigaard  staff   9067 Apr 26 22:13 README.md\n# -rw-r--r--@ 1 mjfrigaard  staff   6122 Apr 10 14:04 music_vids.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff   4417 Apr 10 14:01 trees.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff   1315 Apr  6 05:38 roxanne.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    462 Apr 15 14:07 wu_tang.dat\n# -rw-r--r--@ 1 mjfrigaard  staff    381 May  1 10:17 who_tb_data.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    381 Mar 28  2023 who_tb_data.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff    281 Apr 10 09:38 wu_tang.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    263 Apr 10 09:39 wu_tang.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff    263 Apr 10 09:34 wu_tang.csv\n# -rw-r--r--@ 1 mjfrigaard  staff    117 Apr 18 09:27 roxanne_rev.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    113 Apr 18 09:27 roxanne_orig.txt\n# -rw-r--r--  1 mjfrigaard  staff  12531 Apr 13 20:39 ajperlis_epigrams.txt\n# -rw-r--r--  1 mjfrigaard  staff   4814 Apr 10 14:07 vg_hof.tsv\n```\n:::\n\n\n\n\n\nIt showcases how to reverse the listing of directory contents.\n\n### Transformation and Reduction\n\n**Example**\n\n**<code>@sec-find</code>` . -type f | `<code>@sec-xargs</code>` du -sh | `<code>@sec-sort</code>` -h`** finds files (`-type f`) in the current directory and subdirectories, calculates their sizes (`du -sh`), and sorts them by size (`sort -h`):\n\n\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nfind data -type f | xargs du -sh | sort -h\n# 4.0K\tdata/roxanne.txt\n# 4.0K\tdata/roxanne_orig.txt\n# 4.0K\tdata/roxanne_rev.txt\n# 4.0K\tdata/who_tb_data.tsv\n# 4.0K\tdata/who_tb_data.txt\n# 4.0K\tdata/wu_tang.csv\n# 4.0K\tdata/wu_tang.dat\n# 4.0K\tdata/wu_tang.tsv\n# 4.0K\tdata/wu_tang.txt\n# 8.0K\tdata/music_vids.tsv\n# 8.0K\tdata/trees.tsv\n# 8.0K\tdata/vg_hof.tsv\n#  12K\tdata/README.md\n#  12K\tdata/pwrds.csv\n#  12K\tdata/pwrds.tsv\n#  16K\tdata/ajperlis_epigrams.txt\n```\n:::\n\n\n\n\n\nThis pipeline not only identifies files but also sorts them by their disk usage, illustrating a complex operation made simple through pipes.\n\n### Real-time Streaming and Monitoring\n\n**Example**\n\n**`cat /var/log/system.log | grep DEAD_PROCESS`** prints the `system.log` file, continuously monitoring for new entries, filters for those containing `DEAD_PROCESS`, then counts the number of lines:[^pipes-1]\n\n[^pipes-1]: `tail -f /var/log/syslog | grep sshd` is useful for real-time monitoring of SSH daemon logs.\n\n\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncat /var/log/system.log | grep \"DEAD_PROCESS\" \n## Apr 10 06:35:23 Users-MacBook-Pro login[3596]: DEAD_PROCESS: 3596 ttys000\n## Apr 10 06:35:25 Users-MacBook-Pro sessionlogoutd[19895]: DEAD_PROCESS: 225 console\n## Apr 10 10:20:25 Users-MacBook-Pro login[715]: DEAD_PROCESS: 715 ttys000\n```\n:::\n\n\n\n\n\n### Data Manipulation\n\n**Example**\n\n**`cut -d':' -f1 data/roxanne.txt | sort | uniq`** extracts the first field from each line in `data/roxanne.txt`, sorts the contents alphabetically, and removes duplicates.\n\n\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncut -d':' -f1 data/roxanne.txt | sort | uniq\n# I have to tell you just how I feel\n# I know my mind is made up\n# I loved you since I knew you\n# I won't share you with another boy\n# I wouldn't talk down to you\n# It's a bad way\n# Ro...\n# Roxanne\n# Roxanne (Put on the red light)\n# Roxanne (You don't have to put on the red light)\n# So put away your make up\n# Those days are over\n# Told you once I won't tell you again\n# Walk the streets for money\n# You don't care if it's wrong or if it's right\n# You don't have to put on the red light\n# You don't have to sell your body to the night\n# You don't have to wear that dress tonight\n```\n:::\n\n\n\n\n\nThis sequence is an example of performing data extraction and deduplication.\n\n## [Pipes with Loops]{style=\"font-size: 1.05em; font-weight: bold;\"} \n\nThe example below demonstrates how to use the `while` loop with pipes with `find`, `echo`, `grep`, and `wc`.\n\n### Filter with `find`\n\n**`find data -name \"*.tsv\"`** starts in the `data` directory, looking for all files that end with the `.tsv` extension. The search is recursive, meaning it includes all subdirectories of `data` as well. Produces a list of paths to `.tsv` files, each path on a new line. This list is piped to the next command.\n\n\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nfind data -name \"*.tsv\" \n# data/pwrds.tsv\n# data/music_vids.tsv\n# data/vg_hof.tsv\n# data/who_tb_data.tsv\n# data/trees.tsv\n# data/wu_tang.tsv\n```\n:::\n\n\n\n\n\n### Iterate with `while` and `do`\n\n**`| while read fname; do`**: The pipe (`|`) feeds the output from the <code>@sec-find</code> command into a `while` loop, which reads each line (file name) into the variable `fname`, one at a time. For each iteration of the loop (i.e., for each file name read into `fname`), the commands within the `do ... done` block are executed.\n\n\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nfind data -name \"*.tsv\" | while read fname; do\n  # do this!\ndone\n```\n:::\n\n\n\n\n\n### Print with `echo` \n\n**`echo -n \"$fname: \"`**: Prints the current file's name being processed. <code>@sec-echo</code>`-n` outputs the value of `fname` (the path to the current `.tsv` file) followed by a colon and a space, without adding a newline at the end. This means the count returned by <code>@sec-wc</code> will be printed on the same line, right after the file name.\n\n\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nfind data -name \"*.tsv\" | while read fname; do\n  echo -n \"$fname: \"\ndone\n# data/pwrds.tsv: data/music_vids.tsv: data/vg_hof.tsv: data/who_tb_data.tsv: data/trees.tsv: data/wu_tang.tsv:\n```\n:::\n\n\n\n\n\n### Search with `grep`\n\n**`grep \"RZA\" \"$fname\"`**: Searches for a specific pattern within the file. <code>@sec-grep</code> looks through the contents of the file (whose path is in `fname`) for lines containing the string \"RZA\". Only the lines that match this pattern are printed to `stdout`, which is then piped to <code>@sec-wc</code>.\n\n\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nfind data -name \"*.tsv\" | while read fname; do\n  echo -n \"$fname: \"\n  grep \"RZA\" \"$fname\"\ndone\n# data/pwrds.tsv: data/music_vids.tsv: data/vg_hof.tsv: data/who_tb_data.tsv: data/trees.tsv: data/wu_tang.tsv: RZA\tRobert Diggs\n```\n:::\n\n\n\n\n\n### Count with `wc`\n\n**`wc`**: For each file processed by the loop, <code>@sec-wc</code> outputs three numbers: the line count, word count, and character/byte count of the lines that <code>@sec-grep</code> found to contain \"RZA\". Since no specific option is given to <code>@sec-wc</code>, it defaults to displaying all three counts.\n\n\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nfind data -name \"*.tsv\" | while read fname; do\n  echo -n \"$fname: \"\n  grep \"RZA\" \"$fname\" | wc \ndone\n# data/pwrds.tsv:        0       0       0\n# data/music_vids.tsv:        0       0       0\n# data/vg_hof.tsv:        0       0       0\n# data/who_tb_data.tsv:        0       0       0\n# data/trees.tsv:        0       0       0\n# data/wu_tang.tsv:        1       3      17\n```\n:::\n\n\n\n\n\nThis Bash command sequence combines <code>@sec-find</code>, a `while` loop, <code>@sec-echo</code>, <code>@sec-grep</code>, and <code>@sec-wc</code> to search through `.tsv` (Tab-Separated Values) files for lines containing a specific pattern (\"RZA\") and reports the count of lines, words, and characters for each occurrence. Combining pipelines with loops is an efficient way to sift through a potentially large set of files within a directory, facilitating a detailed aggregation of specified conditions across multiple files.\n\n\n\n\n\n\n:::: {.callout-tip collapse='true' appearance='default' icon=false}\n\n## [Considerations when using pipes]{style='font-weight: bold; font-size: 1.15em;'}\n\n::: {style='font-size: 1.00em; color: #282b2d;'}\n\n\n\n\n\n**Efficiency and Performance**\n\nWhile pipes are incredibly powerful, their use can impact performance, especially when processing large amounts of data. Each pipe involves creating a new subprocess, and data is copied between processes, which can lead to overhead.\n\n**Error Handling**\n\nError handling in pipes can be non-trivial, as each command in a pipeline executes independently. Users need to consider how each command handles errors and ensure that the pipeline as a whole behaves as expected even when errors occur.\n\n::: \n\n::::\n\n\n\n\n\n\n## [Recap]{style=\"font-size: 1.05em; font-weight: bold;\"} \n\nPipes (`|`) allow the output of one command (`stdout`) to be used as the input (`stdin`) to another, enabling the chaining of commands to perform complex tasks with the output of one serving as the input for the next. Unix pipes embody the concept of composability in Unix, enabling users to build complex workflows out of simple, single-purpose programs. They are a testament to the flexibility and power of the Unix command line, facilitating a wide range of tasks from simple text processing to sophisticated data analysis and system monitoring.\n\nThis framework of commands, arguments, options, and the interplay of input (`stdin`), output (`stdout`) , and pipes enables sophisticated data processing and manipulation directly from the terminal.\n\n\n\n\n\n\n:::: {.callout-note collapse='false' appearance='simple' icon=false}\n\n## [See a typo, error, or something missing?]{style='font-weight: bold; font-size: 1.15em;'}\n\n::: {style='font-size: 1.00em; color: #282b2d;'}\n\n\nPlease open an issue on [GitHub.](https://github.com/mjfrigaard/fm-unix/issues/new)\n\n::: \n\n::::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}