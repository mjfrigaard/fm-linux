[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fm-unix",
    "section": "",
    "text": "Welcome!\nWelcome to my Unix/Linux Field Manual!\nThis book aims to provide a quick reference for basic concepts and common tasks when using Unix/Linux operating systems.1",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "fm-unix",
    "section": "",
    "text": "Most of these commands will work on any Shell (Zsh, Bash, etc.) program.↩︎",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Book outline\nThe first four sections of this manual cover concepts and materials for users who are new to Unix/Linux. If you’re familiar with Unix/Linux command-line tools and regular expressions, feel free to skip these chapters.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#book-outline",
    "href": "preface.html#book-outline",
    "title": "Preface",
    "section": "",
    "text": "Introduction\nThe Introduction acquaints you with some background that formed the landscape of Unix/Linux systems. It explains the Bash shell, a command-line interface (CLI), which is the gateway to leveraging the full potential of Unix/Linux systems.\n\n\nSet-Ups\nBefore diving into the commands and scripts, setting up your Unix/Linux environment is crucial. Set-ups guides you through various options for setting up Shells and Terminals, Virtual Machines, and Quarto documents.\n\n\nSyntax\nUnderstanding the syntax is vital for effectively communicating with the Unix/Linux system. The Syntax section demystifies the structure of commands, including how to differentiate between Commands, Arguments, and Options and manage inputs and outputs. This knowledge is critical to executing tasks efficiently in the command line.\nPipes are a cornerstone of Unix/Linux productivity, enabling the output of one command to serve as the input to another. Syntax also covers how to combine commands using stdout and stdin, allowing for robust command chains that can perform complex tasks with a simple syntax. Understanding pipes unlocks a higher level of command-line efficiency and is a step towards advanced Unix/Linux usage.\n\n\nThe Basics\nNavigating and managing files and folders are daily tasks for Unix/Linux users. Directories covers essential commands such as cd for changing directories, pwd to print the current directory, ls for listing files, and mkdir for making directories.\nFiles chapter covers file manipulation commands like cp (copy), mv (move), rm (remove), and less for viewing file content, ensuring you can organize and manage your file system effectively. These commands are the building blocks for more complex operations in Unix/Linux.\nThe System chapter goes over common system commands like top for monitoring processes in real-time, ps for listing currently running processes, kill to stop a process, df (Disk Free) to display disk space usage on all mounted filesystems, and du (Disk Usage) to estimate file space usage. These commands help you manage the system’s processes and resources efficiently, ensuring you have a clear view of resource allocation and consumption.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#manipulating-text",
    "href": "preface.html#manipulating-text",
    "title": "Preface",
    "section": "Manipulating Text",
    "text": "Manipulating Text\nThe Text section covers using Symbols & Patterns to build regular expressions (regex). Unix/Linux systems are renowned for their powerful text manipulation capabilities. The Manipulating Text chapter introduces commands such as cat for displaying file contents, grep for searching within files, sort for sorting data, uniq for filtering unique lines, and cut, paste, and join for editing files. Mastering these commands will allow you to handle and process text data efficiently.\nText Editors: This section covers nano, vim, and emacs.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#shell-scripts-in-development",
    "href": "preface.html#shell-scripts-in-development",
    "title": "Preface",
    "section": "Shell Scripts (in development)",
    "text": "Shell Scripts (in development)\nShell scripting is a powerful tool for automating repetitive tasks in Unix/Linux. This section introduces you to writing your shell scripts, covering the basics of script creation, execution, and debugging. You`ll learn how to automate simple tasks, making your Unix/Linux experience more productive and enjoyable.\nFormat: Shell scripts can be written in various formats, depending on your shell (e.g., Bash, Zsh). This section delves into the differences between these formats, guiding you on writing compatible scripts that can run across different Unix/Linux systems.\nPermissions: Understanding file permissions is crucial to managing the security of your files and directories in Unix/Linux. This section explains how Unix/Linux permissions work and teaches you how to set and modify permissions to protect your data and system from unauthorized access.\nBy the end of this book, you’ll have a solid understanding of the Unix/Linux operating system. You’ll be equipped to navigate, manage files, write scripts, and set permissions confidently. Whether you’re looking to enhance your career prospects, manage your Unix/Linux systems, or simply satisfy your curiosity, this book will be your companion on a fascinating journey into the world of Unix/Linux.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#data-files",
    "href": "preface.html#data-files",
    "title": "Preface",
    "section": "Data files",
    "text": "Data files\nThe data files used in this book are documented in data/README.md.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "The Tale of Unix\nImagine Unix and Linux as the master and apprentice in the vast workshop of computer operating systems. Our story begins in the late 1960s at AT&T’s Bell Labs. Unix was born out of a desire for a more flexible and portable operating system. It was a time when computers were as big as rooms and operated on specific, often incompatible, systems. Unix was a breath of fresh air because it was designed to be simple, elegant, and, most importantly, portable, meaning it could run on different types of hardware.\nThe Unix philosophy has been distilled into a comprehensive operating system of essential commands and operations, guiding other apprentices in creating their versions of tools and systems.\nUnix is like the master craftsman in this story, having laid the foundational tools and techniques, and crafting a blueprint for how computers could efficiently and securely manage tasks like organizing files or running software.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#recap",
    "href": "intro.html#recap",
    "title": "Introduction",
    "section": "Recap",
    "text": "Recap\nTo summarize, Unix and Linux provide the underlying framework for computer programs. They’re like the behind-the-scenes craftsmen ensuring the workshop runs smoothly, whether crafting a simple piece of furniture (like running a straightforward program on your computer), or constructing an elaborate mansion (like managing the complex operations of a large server).\nThe shell is like the skilled artisan’s primary tool within the grand workshop of Unix and Linux, serving as a bridge between the user and the system’s deeper capabilities. Just as a master carpenter relies on a trusted hammer or saw, users of Unix and Linux turn to the shell for navigating and manipulating the vast landscape of these operating systems.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "sh_term.html",
    "href": "sh_term.html",
    "title": "Shells and Terminals",
    "section": "",
    "text": "Shells\nA Shell in Unix/Linux is a program that interprets commands and acts as an intermediary between the user and the kernel of the operating system. The shell processes user commands, which might involve calling other programs, and returns the output to the user. Shells can be either command-line based or graphical. Some popular examples of shells include Bash, Zsh, and Fish.\nYou can use the following commands to discover which shell you’re currently:\necho $SHELL\n# /bin/zsh\nps -p $$\n#   PID TTY           TIME CMD\n#   683 ttys000    0:00.09 -zsh",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Shells and Terminals</span>"
    ]
  },
  {
    "objectID": "sh_term.html#sec-shells",
    "href": "sh_term.html#sec-shells",
    "title": "Shells and Terminals",
    "section": "",
    "text": "Bash\nBash, or the Bourne Again SHell, is one of the most widely used shells in Unix and Linux environments. Incorporating features from the Korn shell (ksh) and the C shell (csh), Bash supports features like command history, tab completion, aliases and scripting tasks.1\n\n\n\n\n\n\nKey features of  Bash\n\n\n\n\n\n\n\nProgramming Features\nBash includes an array of programming constructs that make it a powerful tool for scripting, which includes:\n- Conditional statements (if, then, else, elif, fi) - Looping statements (for, while, until) - Functions that allow code reusability\nCommand Line Editing\nBash provides an interactive command line editing environment with features:\n\nHistory expansion: Commands can be re-executed by recalling them from the history\nCommand line editing: Users can navigate and edit commands directly on the command line using Emacs or Vi editing modes\nTab Completion: Bash supports tab completion for command names, file names, and even command arguments, speeding up the input process and reducing typos\n\nJob Control\nBash offers comprehensive job control, which includes:\n\nBackgrounding (&), foregrounding (fg), and job management (jobs, bg)\n\nStopping (suspending) processes and continuing them with kill and kill -CONT\n\nAliases and Shell Functions\nUsers can create shorter commands to represent longer sequences of commands using aliases. Bash also supports more powerful functions that can take arguments like small scripts.\nRedirection and Piping\nBash allows for advanced redirection and piping:\n\nRedirecting input and output (&gt;, &lt;, &gt;&gt;, 2&gt;, etc.)\nPiping commands (|) are used to pass the output of one command as input to another\n\nScript Debugging\nBash scripts can be debugged using options like set -x to print commands and their arguments as they are executed, which is invaluable for troubleshooting scripts\nEnvironment Control\nBash allows users to control their shell environment extensively through:\n\nEnvironment variables configuration and management\nVariables are exported to make them available to sub-processes\n\nExpansion Capabilities\nBash supports several types of expansions that enhance its scripting capabilities:\n\nBrace expansion: {a,b,c}\nTilde expansion: ~ translates to the home directory.\nParameter and variable expansion: $name or ${name}\nArithmetic expansion: $(( expression )0\n\nHistory Features\nBash maintains a history of commands that users have executed, which can be navigated, searched, and reused. It also supports configuring the history size and behavior through various environment variables like HISTSIZE and HISTFILESIZE.\n\n\n\n\n\n\n Zsh\nZsh (Z Shell or ‘Oh My ZSH!’) is noted for its interactive features and is often used with customization frameworks. Zsh is a powerful command-line interpreter for Unix systems that serves as both a scriptable shell and an interactive command interpreter.2\n\n\n\n\n\n\nKey features of  Zsh\n\n\n\n\n\n\n\nCommand Line Editing\nZsh provides an advanced and customizable command-line editing environment. Users can configure key bindings and have extensive control over the text editing capabilities directly within the command prompt.\nTab Completion\nZsh has one of the most powerful tab completion systems. It supports:\n\nCompletion for command options and arguments.\nAutomatic listing of options when a tab is hit twice.\nContext-sensitive completion that can recognize patterns in filenames, history, running processes, hostnames, and more.\n\nThemes and Prompts\nZsh allows extensive customization of its prompt, supporting themes that can completely change the look of your command line. The prompt can include colors, content from shell variables, functions, and command outputs.\nScripting\nZsh scripting is robust, with features like arrays, associative arrays, and floating-point arithmetic which are not typically available in all shells. It enhances scripting capabilities and improves on the scripting syntax of the Bourne Shell.\nLoadable Modules: Zsh supports dynamically loadable modules, expanding its capabilities with features like:\n\nFTP client\nTCP and UDP socket operations\nAdvanced math functions\nFull-fledged regular expression matching\n\nImproved Variable Handling\nVariable handling in Zsh includes several enhancements like:\n\nBetter array handling\nAssociative arrays (similar to dictionaries in higher-level programming languages)\nEasier string manipulation and pattern matching\n\nSpell Check and Correction\nZsh can be configured to correct commands automatically if misspelled and to suggest corrections or alternatives. This feature helps in reducing syntax errors and improves user efficiency.\nExtended Globbing\nZsh’s file globbing allows for more complex pattern matching than traditional Unix shells. You can specify patterns in a more expressive and powerful way, which is particularly useful in scripts.\n\n\n\n\n\n\n Fish\nFish, or the Friendly Interactive SHell, is a smart and user-friendly command line shell for Unix-like operating systems. It’s designed to be more interactive and user-friendly than traditional shells like Bash or Zsh.3\n\n\n\n\n\n\nKey features of  Fish\n\n\n\n\n\n\n\nAutosuggestions\nFish suggests commands as you type based on history and completions, just like a web browser. This feature allows users to see and reuse previous commands by simply pressing the right arrow key to complete the suggested command, which can significantly speed up typing and reduce errors.\nSyntax Highlighting\nOne of Fish’s most noticeable features is its real-time syntax highlighting. Commands that are valid change color as you type them. It also helps users catch errors before the command is executed, such as highlighting misspelled commands or incorrect paths in red.\nWeb-Based Configuration\nFish includes a web-based configuration interface (accessible via the fish_config command), which makes customizing the shell settings and prompt easier for users who prefer a graphical interface over editing configuration files manually (or if you’re new to the command line).\nEnhanced Tab Completion\nFish provides intelligent tab completions for commands, file names, variables, and user-defined functions. It not only completes based on the prefix but also considers the whole line context, making the completions more relevant.\nImproved Variables and Scoping\nFish simplifies variable management, including universal variables that are automatically shared between all running shells and persist across restarts without needing explicit saving to a file. Variable scoping is also more straightforward, helping avoid common bugs seen in other shells.\nFunction Autoloads\nFish allows functions to be defined in individual files and automatically loads them only when needed. This lazy-loading of functions helps speed up the start time of the shell.\nExtensible\nFish is designed to be easily extensible through plugins. The Fisherman and Oh My Fish frameworks offer many plugins and themes designed to enhance Fish’s capabilities or customize its appearance.\nMan Page Completions\nFish generates command completions automatically from man pages, which means it often supports completions for all the installed commands without needing special configuration.\nNo Configuration Needed\nFish is designed to work properly out of the box, without needing to configure it extensively. This makes it very accessible for new users or those who want a powerful shell without the need to customize or configure it heavily.\nUser-Friendly Scripts\nFish uses a syntax that is slightly different from the traditional POSIX shell syntax, which is often simpler and easier to understand. For example, loops and conditionals are clearer, and there is no need for explicit subshell management.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Shells and Terminals</span>"
    ]
  },
  {
    "objectID": "sh_term.html#sec-terminals",
    "href": "sh_term.html#sec-terminals",
    "title": "Shells and Terminals",
    "section": "Terminals",
    "text": "Terminals\nA Terminal (or terminal emulator) is a software program that provides a text-based interface to the shell. Terminals interpret keystrokes and commands from users and send these to the shell for execution. When the shell produces output, the terminal displays it to the user. Terminal emulators allow users to interact with the shell and other command-line tools. Below is an expanded look at some commonly used terminal emulators and their key features.\n\n GNOME\nGNOME Terminal is the default terminal emulator for the GNOME desktop environment, widely used in many Linux distributions.\n\n\n\n\n\n\nKey features of GNOME\n\n\n\n\n\n\n\n\nProfiles: Users can create multiple profiles, each with its own set of preferences, including colors, fonts, and keyboard shortcuts.\nTabs and Splitting: Supports opening multiple tabs and can split the terminal window into multiple panes.\nTransparency and Backgrounds: Allows setting background images and adjusting the transparency of the terminal window.\nCompatibility: Supports UTF-8 for a wide range of characters, making it suitable for international use.\n\n\n\n\n\n\n\n Konsole\nKonsole is part of the KDE desktop environment. It is known for its deep integration with KDE and its high degree of customizability.\n\n\n\n\n\n\nKey features of Konsole\n\n\n\n\n\n\n\n\nTabbed Interface: Allows multiple tabs within a single window, facilitating multitasking.\nProfiles: Supports multiple profiles, enabling different settings for each session.\nSplit Views: Users can split Konsole windows horizontally or vertically.\nTransparency and Theming: Supports background transparency and themes, which can be customized easily.\n\n\n\n\n\n\n\n iTerm2\niTerm2 is a replacement for Terminal and the successor to iTerm for macOS. It offers features beyond what traditional terminals provide.\n\n\n\n\n\n\nKey features of iTerm2\n\n\n\n\n\n\n\n\nSplit Panes: Users can divide iTerm2 into multiple panes, each with its own session.\nSearch: iTerm2 allows users to search through text and highlights occurrences.\nProfiles: Supports detailed profiles, each with its custom colors, fonts, window transparency, and key bindings.\nAdvanced Paste Features: Offers a paste history and allows pasting with escape codes to avoid issues with unintended command executions.\nMouseless Copy: iTerm2 lets you use keyboard shortcuts to select and copy text without needing the mouse.\nShell Integration: iTerm2 can integrate with the shell to display badges, track command statuses, and more.\nTrigger Support: Executes user-defined actions based on text output to the terminal.\n\n\n\n\n\nEach of these terminal emulators offers unique features that cater to different needs and preferences, enhancing the user’s command-line experience. Whether you need deep customization, minimal resource usage, or advanced functionalities like search and shell integration, there’s a terminal emulator that fits the requirement.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Shells and Terminals</span>"
    ]
  },
  {
    "objectID": "sh_term.html#recap",
    "href": "sh_term.html#recap",
    "title": "Shells and Terminals",
    "section": "Recap",
    "text": "Recap\nIn summary, the shell is the command interpreter that executes the commands, while the terminal is the application that allows you to interact with the shell.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Shells and Terminals</span>"
    ]
  },
  {
    "objectID": "sh_term.html#footnotes",
    "href": "sh_term.html#footnotes",
    "title": "Shells and Terminals",
    "section": "",
    "text": "Since the release of macOS Catalina (2019), Apple transitioned from using Bash as the default command-line interface to Zsh.↩︎\nZsh is an extended version of Bash (Bourne Again SHell), with many improvements, and is fully compatible with the Bourne Shell.↩︎\nArs Technica has a great summary comparing Fish to other shells.↩︎",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Shells and Terminals</span>"
    ]
  },
  {
    "objectID": "vms.html",
    "href": "vms.html",
    "title": "Virtual Machines",
    "section": "",
    "text": "Virtualization Software\nSetting up Unix/Linux on virtual machines (VMs) is a valuable technique for running different operating systems on a single physical machine, which is helpful for development, testing, server deployment, and more. Several virtualization tools are available, but the most popular ones include:",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Virtual Machines</span>"
    ]
  },
  {
    "objectID": "vms.html#recap",
    "href": "vms.html#recap",
    "title": "Virtual Machines",
    "section": "Recap",
    "text": "Recap\nThis chapter covered virtualization and Linux distributions. Specifically, we discussed:\n\nSetting up Unix/Linux on virtual machines, highlighting the process and popular virtualization software options.\n\nCommercial options: VMware Workstation, VMware Fusion, and Parallels Desktop\nOpen-source option: VirtualBox\n\nThe key features of three major Linux distributions:\n\nUbuntu, known for its user-friendliness\nFedora, recognized for its cutting-edge technology\nCentOS, valued for its enterprise-level stability\n\n\nEach of these distributions brings its strengths and is tailored to different segments of the Linux user base, from desktop users and hobbyists to developers and enterprise clients.\n\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Virtual Machines</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "Quarto",
    "section": "",
    "text": "Install",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#quarto-documents",
    "href": "quarto.html#quarto-documents",
    "title": "Quarto",
    "section": "Quarto Documents",
    "text": "Quarto Documents\n\nYAML header\nYAML is a lightweight markup language that’s easy to write and read. In Quarto, the YAML header is used to configure document properties such as the title, engine, output format, and more. It serves as the foundation for controlling how your Quarto document behaves and appears.\nQuarto documents are written in markdown and can include executable code in various programming languages, including Unix commands. The YAML header is placed between three dashes --- at the top of each Quarto document to specify metadata and global options.\n---\ntitle: \"Using Bash\"\n---\nTo run Bash commands, specify knitr in the engine field of in the YAML header of the Quarto file, and any additional key-value pairs:1\n---\ntitle: \"Using Bash\"\nengine: knitr\nknitr:\n  opts_chunk: \n    collapse: true\n---\n\n\nBash Code Chunks\nIntroduced in 1989, Bash has become the default command-line interface or “shell” for most Linux distributions.2 We’ll use Bash commands and scripts to preproceess data files, render Quarto documents, and automate other operations in the data analysis pipeline, from downloading and ingesting data to creating and deploying analysis outputs.\nOne of the powerful features of Quarto is the ability to integrate executable code chunks into Markdown documents.3 You can create bash code chunks using the following syntax:\n```{bash}\necho \"foo\" \n```\nBash code chunks allow you to include executable commands within your Quarto documents. You can also specify the code chunk options with the hash-pipe (#|):4\n```{bash}\n#| code-fold: show\n#| code-summary: 'show/hide echo'\necho \"foo\" \n```\nWhen the document is rendered, the narrative text is included with the output from the commnads.\n\n\nshow/hide echo\necho \"foo\"\n## foo\n\n\nThis simplicity allows authors to focus on their content rather than formatting.\n\n\nCode Chunk Isolation\nWhen incorporating Bash code chunks into Quarto documents, an essential detail to remember is the behavior of the working directory during file rendering. By default, Quarto sets the working directory to the location of the current document within the project.\nConsider the following scenario in a Quarto project:\n\n# This code chunk displays the current working directory\npwd\n## /Users/mjfrigaard/projects/books/fm-unix\n\nAlthough we can navigate to a different directory within a given code chunk:\n\ncd data # Change the current working directory to 'data' \npwd # confirm the change\n## /Users/mjfrigaard/projects/books/fm-unix/data\n\nIt’s crucial to note that Quarto resets the working directory to the document’s location for each new code chunk:\n\n# Verifying the working directory, which reverts to \npwd # the document's location for each new code chunk\n## /Users/mjfrigaard/projects/books/fm-unix\n\nThis behavior is different than what we’d see in Posit Workbench’s Terminal on my local machine:\n\nAssume the current working directory is ~/projects/ (as indicated by the blue highlighted area in the Terminal pane)\n\n\n\n\n~/projects/\n\n\n\nIf we change my working directory with .., we are in the home directory ~.\n\n\n\n\n~/\n\n\n\nWhen we check the current working directory again with pwd, we see the location has been permanently changed to ~.\n\n\n\n\nStill ~/\n\n\nThis behavior can be frustrating, but it also means we’ll start with a ‘clean slate’ in each new code chunk!",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#posit-workbench",
    "href": "quarto.html#posit-workbench",
    "title": "Quarto",
    "section": "Posit Workbench",
    "text": "Posit Workbench\n\nTerminal Pane (Posit Workbench)\n\n\n\nTerminal Pane in Posit Workbench\n\n\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#footnotes",
    "href": "quarto.html#footnotes",
    "title": "Quarto",
    "section": "",
    "text": "Read more about configuring shell code blocks in Quarto in the documentation.↩︎\nBash was the default command-line interface for Apple’s macOS (which is Unix-based) until the transition to zsh as the default shell in macOS Catalina.↩︎\nIn fact, this entire book was created using Quarto and executable code chunks!↩︎\nConsult the full list of code chunk options in the Quarto documentation.↩︎",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "commands.html",
    "href": "commands.html",
    "title": "Commands",
    "section": "",
    "text": "REPL\nThe REPL in Bash exemplifies a powerful and flexible interface for interacting with the system, running commands, and developing scripts, providing both novice and experienced users with an efficient way to manage their computing environment.\nHere is how it works:",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Commands</span>"
    ]
  },
  {
    "objectID": "commands.html#repl",
    "href": "commands.html#repl",
    "title": "Commands",
    "section": "",
    "text": "1. Read\nIn Bash, the “Read” step occurs when the shell waits for input from the user. This is typically represented by the shell prompt, where we can type commands.\n\nusername@hostname:current_directory$\n\nThe prompt might display useful information, such as the current user (username), hostname (hostname), and working directory (current_directory), depending on its configuration.\n\n\n2. Eval\nOnce a command is entered, Bash “evaluates” it. This step involves parsing the command and its arguments, checking for syntax correctness, and then executing it.\n\nusername@hostname:current_directory$ date\n\nCommands can be simple, such as listing the current date and time, or complex scripts involving loops, conditionals, and functions.\n\n\n3. Print\nAfter evaluating the command, Bash “prints” the output or the result of the command execution to the screen (stdout or standard output) or another specified location.\n\n#&gt; Wed Apr 10 02:55:23 MST 2024\n\nIf the command results in an error, the error message is displayed instead (typically on standard error).\n\n\nLoop\nAfter executing a command and returning the output, Bash immediately returns to the “read” step, displaying the prompt and waiting for new user input.\n\nusername@hostname:current_directory$\nusername@hostname:current_directory$ date\n#&gt; Wed Apr 10 02:55:23 MST 2024\nusername@hostname:current_directory$\n\nThis cycle repeats indefinitely until the user exits the REPL environment, typically with an exit command or by pressing Ctrl + D.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Commands</span>"
    ]
  },
  {
    "objectID": "commands.html#basics",
    "href": "commands.html#basics",
    "title": "Commands",
    "section": "Basics",
    "text": "Basics\nIn Unix, several commands can operate without any options or arguments, performing their basic functions in their simplest form. Below are some of these commands:\n\nwho\nwho shows who is logged on the system.\n\nwho\n# username       console      Apr 14 13:45 \n# username       ttys000      Apr 14 13:45 \n# username       ttys001      Apr 14 13:45 \n# username       ttys003      Apr 16 05:56\n\nwho by itself, without options or arguments, lists the users currently logged into the system.\n\n\nwhoami\nwhoami shows the username of the user currently logged into the system.\n\nwhoami\n# username\n\n\n\nhostname\nhostname displays the system’s network name.\n\nhostname\n# Users-MacBook-Pro-2.local\n\n\n\ncal\ncal displays a calender of the current month.\n\ncal\n#      April 2024       \n# Su Mo Tu We Th Fr Sa  \n#     1  2  3  4  5  6  \n#  7  8  9 10 11 12 13  \n# 14 15 16 17 18 19 20  \n# 21 22 23 24 25 26 27  \n# 28 29 30              \n# \n\n\n\nuptime\nuptime shows how long the system has been running.\n\nuptime\n# 11:39  up 15:05, 4 users, load averages: 3.85 3.08 2.93\n\n\n\nclear\nclear clears the terminal screen and doesn’t print any return values.\n\nclear\n\nclear does its job without the need for additional input.\n\n\nexit\nexit exits the shell or your current session.\n\nexit\n\nexit requires no arguments or options to execute this action, and doesn’t print any return values.\n\n\nyes\nyes repeatedly outputs a string until killed. Without arguments, it defaults to outputting ‘y’.\n\nyes\n\nNOTE: Use Ctrl + C to interrupt the yes command.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Commands</span>"
    ]
  },
  {
    "objectID": "commands.html#recap",
    "href": "commands.html#recap",
    "title": "Commands",
    "section": "Recap",
    "text": "Recap\nEach of these commands performs a specific and often utilized function within the Unix environment, embodying the Unix philosophy of doing one thing well.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Commands</span>"
    ]
  },
  {
    "objectID": "arguments.html",
    "href": "arguments.html",
    "title": "Arguments",
    "section": "",
    "text": "Anatomy\nA Unix command can be broken down into the command name, followed by its options (which we’ll address in the next chapter), and then its arguments:\ncommand argument1 argument2 ... argument",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Arguments</span>"
    ]
  },
  {
    "objectID": "arguments.html#types",
    "href": "arguments.html#types",
    "title": "Arguments",
    "section": "Types",
    "text": "Types\nBelow are a variety of command arguments types. This is not an exhaustive list, but includes many of the commands and arguments you’ll encounter on a regular basis.\n\nDirect and Indirect\nDirect arguments are the most straightforward type of arguments. They are typically the names of files or directories on which commands operate.\nExample\n\ncat myfile.txt\n# This is my file\n\nIn the command echo myfile.txt, myfile.txt is a direct argument to the cat command, telling it which file to display on the standard output.\nIndirect arguments are arguments that might specify additional information that commands need to complete their tasks.\nExample\n\ngrep file myfile.txt\n# This is my file\n\nThe file search pattern for the grep command is an example of an indirect command, and myfile.txt is the direct argument.\nMost commonly, arguments are the names of files and directory names on which the command will operate.\nExample\n\nmv myfile.txt tmp/myfile.txt\n\nmyfile.txt and tmp/myfile.txt are arguments representing the source and destination locations to move with mv, respectively.\nCommands related to user management might take user and group names names as arguments.\nExample\n\nchown user:group myfile.txt\n\nchown changes the ownership of file to user and group.\n\n\nCommand Targets\nSome commands take other commands as arguments. For example, sudo command runs command with superuser privileges.\nExample\n\nsudo vi path/to/file.config\n\n\n\nData Values\nCommands might take data values as arguments for processing.\nExample\n\necho Hello, World!\n\nIn echo Hello, World!, Hello, World! is the argument value that echo prints to the terminal.\n\n\nOrder and Position\nFor many commands, the order of the arguments is significant.\nExample\n\ncp tmp/myfile.txt myfile.txt\n\ntmp/myfile.txt is the first argument (indicating the file to copy from), and myfile.txt is the second argument (indicating where to copy the file to). Reversing these arguments would result in a completely different operation.\nArguments that contain spaces must be quoted or escaped, so the shell understands them as a single argument rather than multiple arguments.\nExample\nTo copy the contents of the new myfile.txt to 'my file 2.txt, you would use:\n\ncp myfile.txt 'my file 2.txt'\n\n\n\nCommand Substitution\nThe output of a command can be used as an argument for another command using backticks (` `) or $( ).\nExample\necho $(grep file 'my file 2.txt') uses the output of the grep command as an argument for echo:\n\necho $(grep file 'my file 2.txt')\n# This is my file\n\n\n\nVariables as Arguments\nEnvironment variables can be used as arguments in commands.\nExample\necho $HOME prints the path to the user’s home directory, where $HOME is an argument that the echo command interprets:\n\necho $HOME\n# /Users/username\n\nUnderstanding the nuances of Unix arguments is crucial for crafting precise and effective commands, allowing users to leverage the full power of the Unix command line for a wide array of tasks.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Arguments</span>"
    ]
  },
  {
    "objectID": "options.html",
    "href": "options.html",
    "title": "Options",
    "section": "",
    "text": "Short Options\nShort options are typically a single dash followed by a single letter (e.g., -l) and they modify the command behavior in a specific, often concise way.\nExample\nls -l data lists files in data in a long format, showing detailed information like permissions, owner, size, and modification date:\nls -l data\n# total 200\n# -rw-r--r--  1 mjfrigaard  staff  12531 Apr 13 20:39 ajperlis_epigrams.txt\n# -rw-r--r--@ 1 mjfrigaard  staff   6122 Apr 10 14:04 music_vids.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff  12057 Apr 22 14:11 pwrds.csv\n# -rw-r--r--@ 1 mjfrigaard  staff  12057 Apr 22 14:11 pwrds.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff   1315 Apr  6 05:38 roxanne.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    113 Apr 18 09:27 roxanne_orig.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    117 Apr 18 09:27 roxanne_rev.txt\n# -rw-r--r--@ 1 mjfrigaard  staff   4417 Apr 10 14:01 trees.tsv\n# -rw-r--r--  1 mjfrigaard  staff   4814 Apr 10 14:07 vg_hof.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff    381 Mar 28  2023 who_tb_data.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff    381 Apr 12 13:14 who_tb_data.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    263 Apr 10 09:34 wu_tang.csv\n# -rw-r--r--@ 1 mjfrigaard  staff    462 Apr 15 14:07 wu_tang.dat\n# -rw-r--r--@ 1 mjfrigaard  staff    263 Apr 10 09:39 wu_tang.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff    281 Apr 10 09:38 wu_tang.txt",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#combining-short-options",
    "href": "options.html#combining-short-options",
    "title": "Options",
    "section": "Combining Short Options",
    "text": "Combining Short Options\nMultiple short options can be combined after a single dash, without spaces (e.g., -lrt) allowing users to use multiple options at once, and reducing the need to type multiple dashes.\nExample\nls -lrt combines three options: -l (long listing format), -r (reverse order), and -t (sort by modification time), providing a detailed, reverse-chronological list of files.\n\nls -lrt data\n# total 200\n# -rw-r--r--@ 1 mjfrigaard  staff    381 Mar 28  2023 who_tb_data.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff   1315 Apr  6 05:38 roxanne.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    263 Apr 10 09:34 wu_tang.csv\n# -rw-r--r--@ 1 mjfrigaard  staff    281 Apr 10 09:38 wu_tang.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    263 Apr 10 09:39 wu_tang.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff   4417 Apr 10 14:01 trees.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff   6122 Apr 10 14:04 music_vids.tsv\n# -rw-r--r--  1 mjfrigaard  staff   4814 Apr 10 14:07 vg_hof.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff    381 Apr 12 13:14 who_tb_data.txt\n# -rw-r--r--  1 mjfrigaard  staff  12531 Apr 13 20:39 ajperlis_epigrams.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    462 Apr 15 14:07 wu_tang.dat\n# -rw-r--r--@ 1 mjfrigaard  staff    113 Apr 18 09:27 roxanne_orig.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    117 Apr 18 09:27 roxanne_rev.txt\n# -rw-r--r--@ 1 mjfrigaard  staff  12057 Apr 22 14:11 pwrds.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff  12057 Apr 22 14:11 pwrds.csv",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#long-options",
    "href": "options.html#long-options",
    "title": "Options",
    "section": "Long Options",
    "text": "Long Options\nLong options usually use two dashes followed by a word or compound words (e.g., --long-listing) and provide a more descriptive way to specify options, making scripts and commands more readable.\nExample\nls --reverse lists files in reverse order.\n\nls --reverse\n#&gt; wu_tang.txt\n#&gt; wu_tang.tsv\n#&gt; wu_tang.dat\n#&gt; wu_tang.csv\n#&gt; who-tb-data.tsv\n#&gt; vg_hof.tsv\n#&gt; trees.tsv\n#&gt; roxanne\n#&gt; music_vids.tsv\n\nNote that not all commands support the long-form option syntax.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#options-and-arguments",
    "href": "options.html#options-and-arguments",
    "title": "Options",
    "section": "Options and Arguments",
    "text": "Options and Arguments\nOptions can be combined with arguments when they are followed by a space and then the argument (e.g., -o filename). Some options require or accept an argument to specify a value related to the option’s action.\nExample\ngrep-i \"FILE\" myfile.txt uses -i to ignore case when searching for \"FILE\" in myfile.txt:\n\ngrep -i \"FILE\" myfile.txt\n# This is my file\n\nThe \"FILE\" here is an argument to the -i option.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#options-affecting-output",
    "href": "options.html#options-affecting-output",
    "title": "Options",
    "section": "Options Affecting Output",
    "text": "Options Affecting Output\nExample 1\n\nls -a data\n# .\n# ..\n# ajperlis_epigrams.txt\n# music_vids.tsv\n# pwrds.csv\n# pwrds.tsv\n# roxanne.txt\n# roxanne_orig.txt\n# roxanne_rev.txt\n# trees.tsv\n# vg_hof.tsv\n# who_tb_data.tsv\n# who_tb_data.txt\n# wu_tang.csv\n# wu_tang.dat\n# wu_tang.tsv\n# wu_tang.txt\n\nls -a lists all files, including hidden ones (those starting with a dot). This option alters the command’s output by showing files that are not listed by default.\nExample 2\ndf -h shows disk space usage in human-readable form (e.g., KB, MB, GB), modifying the default output to be more easily understood.\n\ndf -h\n# Filesystem                          Size    Used   Avail Capacity iused ifree %iused  Mounted on\n# /dev/disk1s1s1                     466Gi   9.5Gi    40Gi    20%    404k  422M    0%   /\n# devfs                              201Ki   201Ki     0Bi   100%     696     0  100%   /dev\n# /dev/disk1s3                       466Gi   2.4Gi    40Gi     6%    5.1k  422M    0%   /System/Volumes/Preboot\n# /dev/disk1s5                       466Gi   1.0Gi    40Gi     3%       1  422M    0%   /System/Volumes/VM\n# /dev/disk1s6                       466Gi    19Mi    40Gi     1%      19  422M    0%   /System/Volumes/Update\n# /dev/disk1s2                       466Gi   411Gi    40Gi    92%    6.3M  422M    1%   /System/Volumes/Data\n# map auto_home                        0Bi     0Bi     0Bi   100%       0     0     -   /System/Volumes/Data/home",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#options-modifying-command-behavior",
    "href": "options.html#options-modifying-command-behavior",
    "title": "Options",
    "section": "Options Modifying Command Behavior",
    "text": "Options Modifying Command Behavior\nExample\ncp-n source.txt dest.txt does not overwrite the destination file if it already exists.\n\ncp -n myfile.txt myfile2.txt\n\nThe -n option modifies the default behavior of the cp command.\n\ncat myfile2.txt\n# This is my 2nd file",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#options-for-help-and-information",
    "href": "options.html#options-for-help-and-information",
    "title": "Options",
    "section": "Options for Help and Information",
    "text": "Options for Help and Information\nExample\ngrep--help displays usage information for the grep command, helping users understand available options and syntax.\n\ngrep --help\n#&gt; usage: grep [-abcdDEFGHhIiJLlMmnOopqRSsUVvwXxZz] [-A num] [-B num] [-C[num]]\n#&gt;  [-e pattern] [-f file] [--binary-files=value] [--color=when]\n#&gt;  [--context[=num]] [--directories=action] [--label] [--line-buffered]\n#&gt;  [--null] [pattern] [file ...]\n\nExample 2\nThe --version option is commonly used to get version information for various commands.\n\ngrep --version  \n# grep (BSD grep, GNU compatible) 2.6.0-FreeBSD\n\nExample 3\nman shows the official manual for the command.\n\n#\nman echo\n#&gt; ECHO(1)                     General Commands Manual                    ECHO(1)\n#&gt; \n#&gt; NAME\n#&gt;      echo – write arguments to the standard output\n#&gt; \n#&gt; SYNOPSIS\n#&gt;      echo [-n] [string ...]\n#&gt; \n#&gt; DESCRIPTION\n#&gt;      The echo utility writes any specified operands, separated by single blank\n#&gt;      (‘\\n’) characters and followed by a newline (‘\\n’) character, to the\n#&gt;      standard output.\n#&gt; \n#&gt;      The following option is available:\n#&gt; \n#&gt;      -n    Do not print the trailing newline character.  This may also be\n#&gt;            achieved by appending ‘\\c’ to the end of the string, as is done by\n#&gt;            iBCS2 compatible systems.  Note that this option as well as the\n#&gt;            effect of ‘\\c’ are implementation-defined in IEEE Std 1003.1-2001\n#&gt;            (“POSIX.1”) as amended by Cor. 1-2002.  Applications aiming for\n#&gt;            maximum portability are strongly encouraged to use printf(1) to\n#&gt;            suppress the newline character.\n#&gt; \n#&gt;      Some shells may provide a builtin eecchhoo command which is similar or\n#&gt;      identical to this utility.  Most notably, the builtin eecchhoo in sh(1) does\n#&gt;      not accept the --nn option.  Consult the builtin(1) manual page.\n#&gt; \n#&gt; EXIT STATUS\n#&gt;      The echo utility exits 0 on success, and &gt;0 if an error occurs.\n#&gt; \n#&gt; SEE ALSO\n#&gt;      builtin(1), csh(1), printf(1), sh(1)\n#&gt; \n#&gt; STANDARDS\n#&gt;      The echo utility conforms to IEEE Std 1003.1-2001 (“POSIX.1”) as amended\n#&gt;      by Cor. 1-2002.\n#&gt; \n#&gt; macOS 14.4                      April 12, 2003                      macOS 14.4",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#environment-specific-options",
    "href": "options.html#environment-specific-options",
    "title": "Options",
    "section": "Environment-Specific Options",
    "text": "Environment-Specific Options\nsort-u file.txt sorts the lines in file.txt, removing duplicate lines. The -u option’s behavior (considering case sensitivity) might vary depending on the locale and environment settings.\nExample\nsort -u data/roxanne.txt sorts the lines in data/roxanne.txt, removing duplicate lines.1\n\nsort -u data/roxanne.txt\n# I have to tell you just how I feel\n# I know my mind is made up\n# I loved you since I knew you\n# I won't share you with another boy\n# I wouldn't talk down to you\n# It's a bad way\n# Ro...\n# Roxanne\n# Roxanne (Put on the red light)\n# Roxanne (You don't have to put on the red light)\n# So put away your make up\n# Those days are over\n# Told you once I won't tell you again\n# Walk the streets for money\n# You don't care if it's wrong or if it's right\n# You don't have to put on the red light\n# You don't have to sell your body to the night\n# You don't have to wear that dress tonight",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#recap",
    "href": "options.html#recap",
    "title": "Options",
    "section": "Recap",
    "text": "Recap\nOptions greatly enhance the power and versatility of Unix commands, allowing users to tailor operations to their specific needs and preferences.\nNot all Unix-like systems or shells may support the same options for a given command, and behavior can vary between implementations. It’s important to refer to a command’s manual page (using man command or command --help) for the most accurate and comprehensive list of options and their effects.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#footnotes",
    "href": "options.html#footnotes",
    "title": "Options",
    "section": "",
    "text": "data/roxanne.txt contains the lyrics to the 1978 song Roxanne by The Police.↩︎",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "pipes.html",
    "href": "pipes.html",
    "title": "Pipes",
    "section": "",
    "text": "Fundamental Concept\nThe pipe is placed between two commands and directs the standard output (stdout) of the command to the left of the pipe to the standard input (stdin) of the command to the right.\nExample\necho \"Hello, World!\" | wc -w sends the output of the echo command to wc, which then counts the words.\necho \"Hello, World!\" | wc -w\n#&gt;        2\nThe output is 2, indicating there are two words in “Hello, World!”.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pipes</span>"
    ]
  },
  {
    "objectID": "pipes.html#fundamental-concept",
    "href": "pipes.html#fundamental-concept",
    "title": "Pipes",
    "section": "",
    "text": "Standard Input and Output\n\n\n\n\n\n\nstdin (standard input) is a text stream from which a command reads its input. By default, it’s the keyboard, but it can be redirected to read from a file or another command’s output.\n\n\n\nThe text stream looks nothing like this\n\n\nstdout (standard output) is a text stream where a command writes its output. Typically, this is the terminal screen, but it can be redirected to a file or another command’s input.\n\n\n\n\n\n\n\n\n\nCombining Pipes\nCommands can be chained together using multiple pipes, allowing for the creation of command pipelines where data is processed in stages.\nExample\npsaux |grep httpd lists all processes, filters those containing “httpd” (HTTPD = web server processes running):\n\nps aux | grep httpd\n#&gt; mjfrigaard       22321   0.0  0.0 33597016    632   ??  S    10:44PM   0:00.00 grep httpd\n#&gt; mjfrigaard       22319   0.0  0.0 33598572    916   ??  S    10:44PM   0:00.01 bash -c ps aux | grep httpd\n#&gt; mjfrigaard       22318   0.0  0.0 33601644    972   ??  S    10:44PM   0:00.01 sh -c 'bash'  -c 'ps aux | grep httpd' 2&gt;&1\n\nExample\nwc-l counts the number of lines:\n\nps aux | grep httpd | wc -l\n#&gt;        3\n\n\n\nFiltering and Processing\nExample 1\ncatdata/roxanne.txt |grep\"night\" displays lines from data/roxanne.txt that contain the number \"2\".\n\ncat data/roxanne.txt | grep \"night\"\n#&gt; You don't have to sell your body to the night\n#&gt; You don't have to wear that dress tonight\n\nHere, cat outputs the file’s contents, which grep filters.\nExample 2\nls-l data |sort-r lists the files in data in a detailed format, then sorts them in reverse order.\n\nls -l data | sort -r\n#&gt; total 184\n#&gt; -rw-r--r--@ 1 mjfrigaard  staff   6122 Apr 10 14:04 music_vids.tsv\n#&gt; -rw-r--r--@ 1 mjfrigaard  staff   4417 Apr 10 14:01 trees.tsv\n#&gt; -rw-r--r--@ 1 mjfrigaard  staff    461 Apr 10 09:37 wu_tang.dat\n#&gt; -rw-r--r--@ 1 mjfrigaard  staff    381 Mar 28  2023 who-tb-data.tsv\n#&gt; -rw-r--r--@ 1 mjfrigaard  staff    381 Apr 12 13:14 who_tb_data.txt\n#&gt; -rw-r--r--@ 1 mjfrigaard  staff    281 Apr 10 09:38 wu_tang.txt\n#&gt; -rw-r--r--@ 1 mjfrigaard  staff    263 Apr 10 09:39 wu_tang.tsv\n#&gt; -rw-r--r--@ 1 mjfrigaard  staff    263 Apr 10 09:34 wu_tang.csv\n#&gt; -rw-r--r--  1 mjfrigaard  staff  13074 Apr 13 22:42 numbered_epigrams.txt\n#&gt; -rw-r--r--  1 mjfrigaard  staff  12531 Apr 13 20:39 ajperlis_epigrams.txt\n#&gt; -rw-r--r--  1 mjfrigaard  staff   4814 Apr 10 14:07 vg_hof.tsv\n#&gt; -rw-r--r--  1 mjfrigaard  staff   1315 Apr  6 05:38 roxanne.txt\n#&gt; -rw-r--r--  1 mjfrigaard  staff    897 Apr 13 21:14 data_epigrams.txt\n#&gt; -rw-r--r--  1 mjfrigaard  staff    542 Apr 13 22:42 numbered_lines.txt\n\nIt showcases how to reverse the listing of directory contents.\n\n\nTransformation and Reduction\nExample\nfind. -type f |xargsdu -sh |sort-h finds files (-type f) in the current directory and subdirectories, calculates their sizes (du -sh), and sorts them by size (sort -h):\n\nfind data -type f | xargs du -sh | sort -h\n#&gt; 4.0K data/data_epigrams.txt\n#&gt; 4.0K data/numbered_lines.txt\n#&gt; 4.0K data/roxanne.txt\n#&gt; 4.0K data/who-tb-data.tsv\n#&gt; 4.0K data/who_tb_data.txt\n#&gt; 4.0K data/wu_tang.csv\n#&gt; 4.0K data/wu_tang.dat\n#&gt; 4.0K data/wu_tang.tsv\n#&gt; 4.0K data/wu_tang.txt\n#&gt; 8.0K data/music_vids.tsv\n#&gt; 8.0K data/trees.tsv\n#&gt; 8.0K data/vg_hof.tsv\n#&gt;  16K data/ajperlis_epigrams.txt\n#&gt;  16K data/numbered_epigrams.txt\n\nThis pipeline not only identifies files but also sorts them by their disk usage, illustrating a complex operation made simple through pipes.\n\n\nReal-time Streaming and Monitoring\nExample\ncat /var/log/system.log | grep DEAD_PROCESS prints the system.log file, continuously monitoring for new entries, filters for those containing DEAD_PROCESS, then counts the number of lines:1\n\ncat /var/log/system.log | grep \"DEAD_PROCESS\" \n## Apr 10 06:35:23 Users-MacBook-Pro login[3596]: DEAD_PROCESS: 3596 ttys000\n## Apr 10 06:35:25 Users-MacBook-Pro sessionlogoutd[19895]: DEAD_PROCESS: 225 console\n## Apr 10 10:20:25 Users-MacBook-Pro login[715]: DEAD_PROCESS: 715 ttys000\n\n\n\nAdvanced Data Manipulation\nExample\ncut -d':' -f1 data/roxanne.txt | sort | uniq extracts the first field from each line in data/roxanne.txt, sorts the contents alphabetically, and removes duplicates.\n\ncut -d':' -f1 data/roxanne.txt | sort | uniq\n#&gt; I have to tell you just how I feel\n#&gt; I know my mind is made up\n#&gt; I loved you since I knew you\n#&gt; I won't share you with another boy\n#&gt; I wouldn't talk down to you\n#&gt; It's a bad way\n#&gt; Ro...\n#&gt; Roxanne\n#&gt; Roxanne (Put on the red light)\n#&gt; Roxanne (You don't have to put on the red light)\n#&gt; So put away your make up\n#&gt; Those days are over\n#&gt; Told you once I won't tell you again\n#&gt; Walk the streets for money\n#&gt; You don't care if it's wrong or if it's right\n#&gt; You don't have to put on the red light\n#&gt; You don't have to sell your body to the night\n#&gt; You don't have to wear that dress tonight\n\nThis sequence is an example of performing data extraction and deduplication.\n\nPipes with Loops\nExample\nfind data -name \"*.tsv\": starts in the data directory, looking for all files that end with the .tsv extension. The search is recursive, meaning it includes all subdirectories of data as well. Produces a list of paths to .tsv files, each path on a new line. This list is piped to the next command.\n\nfind data -name \"*.tsv\" \n#&gt; data/music_vids.tsv\n#&gt; data/vg_hof.tsv\n#&gt; data/trees.tsv\n#&gt; data/who-tb-data.tsv\n#&gt; data/wu_tang.tsv\n\n| while read fname; do: The pipe (|) feeds the output from the find command into a while loop, which reads each line (file name) into the variable fname, one at a time. For each iteration of the loop (i.e., for each file name read into fname), the commands within the do ... done block are executed.\n\nfind data -name \"*.tsv\" | while read fname; do\n  # do this!\ndone\n\necho -n \"$fname: \": Prints the current file’s name being processed. echo-n outputs the value of fname (the path to the current .tsv file) followed by a colon and a space, without adding a newline at the end. This means the count returned by wc will be printed on the same line, right after the file name.\n\nfind data -name \"*.tsv\" | while read fname; do\n  echo -n \"$fname: \"\ndone\n#&gt; data/music_vids.tsv: data/vg_hof.tsv: data/trees.tsv: data/who-tb-data.tsv: data/wu_tang.tsv:\n\ngrep \"RZA\" \"$fname\": Searches for a specific pattern within the file. grep looks through the contents of the file (whose path is in fname) for lines containing the string “RZA”. Only the lines that match this pattern are printed to stdout, which is then piped to wc.\n\nfind data -name \"*.tsv\" | while read fname; do\n  echo -n \"$fname: \"\n  grep \"RZA\" \"$fname\"\ndone\n#&gt; data/music_vids.tsv: data/vg_hof.tsv: data/trees.tsv: data/who-tb-data.tsv: data/wu_tang.tsv: RZA    Robert Diggs\n\nwc: For each file processed by the loop, wc outputs three numbers: the line count, word count, and character/byte count of the lines that grep found to contain “RZA”. Since no specific option is given to wc, it defaults to displaying all three counts.\n\nfind data -name \"*.tsv\" | while read fname; do\n  echo -n \"$fname: \"\n  grep \"RZA\" \"$fname\" | wc \ndone\n#&gt; data/music_vids.tsv:        0       0       0\n#&gt; data/vg_hof.tsv:        0       0       0\n#&gt; data/trees.tsv:        0       0       0\n#&gt; data/who-tb-data.tsv:        0       0       0\n#&gt; data/wu_tang.tsv:        1       3      17\n\nThis Bash command sequence combines find, a while loop, echo, grep, and wc to search through .tsv (Tab-Separated Values) files for lines containing a specific pattern (“RZA”) and reports the count of lines, words, and characters for each occurrence. Combining pipelines with loops is an efficient way to sift through a potentially large set of files within a directory, facilitating a detailed aggregation of specified conditions across multiple files.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pipes</span>"
    ]
  },
  {
    "objectID": "pipes.html#recap",
    "href": "pipes.html#recap",
    "title": "Pipes",
    "section": "Recap",
    "text": "Recap\nPipes (|) allow the output of one command (stdout) to be used as the input (stdin) to another, enabling the chaining of commands to perform complex tasks with the output of one serving as the input for the next.\n\nEfficiency and Performance\nWhile pipes are incredibly powerful, their use can impact performance, especially when processing large amounts of data. Each pipe involves creating a new subprocess, and data is copied between processes, which can lead to overhead.\n\n\nError Handling\nError handling in pipes can be non-trivial, as each command in a pipeline executes independently. Users need to consider how each command handles errors and ensure that the pipeline as a whole behaves as expected even when errors occur.\nUnix pipes embody the concept of composability in Unix, enabling users to build complex workflows out of simple, single-purpose programs. They are a testament to the flexibility and power of the Unix command line, facilitating a wide range of tasks from simple text processing to sophisticated data analysis and system monitoring.\nThis framework of commands, arguments, options, and the interplay of input (stdin), output (stdout) , and pipes enables sophisticated data processing and manipulation directly from the terminal.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pipes</span>"
    ]
  },
  {
    "objectID": "pipes.html#footnotes",
    "href": "pipes.html#footnotes",
    "title": "Pipes",
    "section": "",
    "text": "tail -f /var/log/syslog | grep sshd is useful for real-time monitoring of SSH daemon logs.↩︎",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pipes</span>"
    ]
  },
  {
    "objectID": "dirs.html",
    "href": "dirs.html",
    "title": "Directories",
    "section": "",
    "text": "Navigate\npwd (Print Working Directory) tells you exactly where you are in the filesystem.\npwd # where am I?\n# /Users/mjfrigaard/projects/books/fm-unix\ncd (Change Directory) lets you move to a different folder on your computer. If you want to move from the one place to another, cd can get you there. For example, cd /bin takes you to the /bin folder, the toolshed of software tools.\ncd /bin # change location\npwd # now where am I?\n# /bin\nls (List) is like standing in one location, looking around, and seeing what files and folders are around you. In /bin, ls would show you the software tools available:\ncd /bin # change location\nls # what's in here?\n# [\n# bash\n# cat\n# chmod\n# cp\n# csh\n# dash\n# date\n# dd\n# df\n# echo\n# ed\n# expr\n# hostname\n# kill\n# ksh\n# launchctl\n# link\n# ln\n# ls\n# mkdir\n# mv\n# pax\n# ps\n# pwd\n# realpath\n# rm\n# rmdir\n# sh\n# sleep\n# stty\n# sync\n# tcsh\n# test\n# unlink\n# wait4path\n# zsh\nlocate",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Directories</span>"
    ]
  },
  {
    "objectID": "dirs.html#sec-nav-dirs",
    "href": "dirs.html#sec-nav-dirs",
    "title": "Directories",
    "section": "",
    "text": "Warning\n\n\n\n\n\n\nThis topic is under development. Thank you for your patience.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Directories</span>"
    ]
  },
  {
    "objectID": "dirs.html#sec-manage-dirs",
    "href": "dirs.html#sec-manage-dirs",
    "title": "Directories",
    "section": "Manage",
    "text": "Manage\nIn the Unix/Linux world, file and directory management is a fundamental skill. This chapter dives deep into the commands that allow users to create, copy, move, remove, and link files and directories. Each section below introduces a different command, detailing its purpose and providing examples of its use.\nmkdir (Make Directory) builds a new folder wherever you tell it to, like making a new folder in our project for outputs (out/) or documents (doc/).\nmkdir out\nmkdir doc\ncp (Copy) duplicates files or folders. The cp command is used to Copy files or directories from one location to another. Imagine having a file (myfile.txt) on your root (.) directory that you want to copy to the /data folder; you could use cp to make a duplicate.\n\ncp myfile.txt data/myfile.txt\n# confirm copy\nls data\n# ajperlis_epigrams.txt\n# music_vids.tsv\n# myfile.txt\n# roxanne.txt\n# roxanne_orig.txt\n# roxanne_rev.txt\n# trees.tsv\n# vg_hof.tsv\n# who-tb-data.tsv\n# who_tb_data.txt\n# wu_tang.csv\n# wu_tang.dat\n# wu_tang.tsv\n# wu_tang.txt\n\nmv (Move): mv, short for Move, functions similarly to picking up a book from your desk and placing it on a shelf. It moves files or directories from one location to another. It can also be used for renaming files. This command is especially useful for organizing files and directories that are in the wrong place.\n\n# create folder\nmkdir doc\n# move file\nmv data/myfile.txt doc/myfile.txt \n\n\n# confirm move\nls doc\n# myfile.txt\n\nrm (Remove): The rm command stands for remove and is used to delete files or directories.\n\n# remove doc folder\nrm doc\n# rm: doc: is a directory\n\nBy default, it won’t remove a directory without the -R or -r option.\n\n\n\n\n\n\nWarning\n\n\n\n\n\n\nIt’s important to note here that the command-line is not very forgiving. Using rm is a powerful action with significant consequences, as it permanently deletes files, akin to shredding documents. There’s usually no easy way to recover deleted files unless you have a backup.\n\n‘Unix is like a chainsaw. Chainsaws are powerful tools, and make many difficult tasks like cutting through thick logs quite easy. Unfortunately, this power comes with danger: chainsaws can cut just as easily through your leg.’ - Gary Bernhardt1\n\n\n\n\n\n\n# add option \nrm -R doc\n\nln (Link): ln creates Links to files or directories, making them accessible from multiple locations without duplicating the actual content. It’s like creating a shortcut on your desktop to a program you frequently use. There are two types of links: hard links and symbolic (soft) links. Symbolic links are more commonly used because they can link to directories and provide more flexibility.\nln -s /path/to/original /path/to/link",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Directories</span>"
    ]
  },
  {
    "objectID": "dirs.html#recap",
    "href": "dirs.html#recap",
    "title": "Directories",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Directories</span>"
    ]
  },
  {
    "objectID": "dirs.html#footnotes",
    "href": "dirs.html#footnotes",
    "title": "Directories",
    "section": "",
    "text": "As quoted in Bioinformatics Data Skills: Reproducible and Robust Research with Open Source Tools (2015) by Vince Buffalo.↩︎",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Directories</span>"
    ]
  },
  {
    "objectID": "files.html",
    "href": "files.html",
    "title": "Files",
    "section": "",
    "text": "Create\nTo create a new file (data/who_tb_data.txt), use the touch command.\ntouch data/who_tb_data.txt\nWe can add some contents to the data/who_tb_data.txt file using echo and the &gt; operator.\necho \"country   year    type    count\nAfghanistan 1999    cases   745\nAfghanistan 1999    population  19987071\nAfghanistan 2000    cases   2666\nAfghanistan 2000    population  20595360\nBrazil  1999    cases   37737\nBrazil  1999    population  172006362\nBrazil  2000    cases   80488\nBrazil  2000    population  174504898\nChina   1999    cases   212258\nChina   1999    population  1272915272\nChina   2000    cases   213766\nChina   2000    population  1280428583\" &gt; data/who_tb_data.txt",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#view",
    "href": "files.html#view",
    "title": "Files",
    "section": "View",
    "text": "View\nfile gives us a summary of what a file is or what it contains, like telling us what’s in data/who_tb_data.txt.\n\nfile data/who_tb_data.txt\n#&gt; data/who_tb_data.txt: ASCII text\n\nWe can use head and tail to view the top and bottom of data/who_tb_data.txt.\n\nhead data/vg_hof.tsv\n#&gt; year game    developer   year_released\n#&gt; 2015 DOOM    id Software 1993\n#&gt; 2015 Pac-Man Namco   1980\n#&gt; 2015 Pong    Atari   1972\n#&gt; 2015 Super Mario Bros.   Nintendo    1985\n#&gt; 2015 Tetris  Alexey Pajitnov 1985\n#&gt; 2015 World of Warcraft   Blizzard Entertainment  2004\n#&gt; 2015 Angry Birds Rovio Entertainment 2009\n#&gt; 2015 FIFA International Soccer   EA Canada   1993\n#&gt; 2015 The Legend of Zelda Nintendo    1986\n\n\ntail data/vg_hof.tsv\n#&gt; 2024 Guitar Hero Harmonix    2005\n#&gt; 2024 Metroid Nintendo    1986\n#&gt; 2024 Myst    Cyan    1993\n#&gt; 2024 Neopets Adam Powell, Donna Powell   1999\n#&gt; 2024 Resident Evil   Capcom  1996\n#&gt; 2024 SimCity Maxis   1989\n#&gt; 2024 Tokimeki Memorial   Konami  1994\n#&gt; 2024 Tony Hawk's Pro Skater  Neversoft   1999\n#&gt; 2024 Ultima  Richard Garriott, Origin Systems    1981\n#&gt; 2024 You Don't Know Jack Jellyvision 1995\n\nless lets you skim through a file on your computer, moving forwards and backwards as you please. We’ll use less on the data/vg_hof.tsv file.\n\nless data/vg_hof.tsv\n\n\n\n\nEnter ‘q’ to exit the less scroll",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#search",
    "href": "files.html#search",
    "title": "Files",
    "section": "Search",
    "text": "Search\nfind is used to search for files and directories in a directory hierarchy based on various criteria such as name, size, file type, and modification time. For example, the commands below look in the data directory for files with a .txt extension (-name \"*.txt\") and finds files modified in the last 2 days (-mtime -2).\n\nfind data -name \"*.txt\" -mtime -1\n#&gt; data/who_tb_data.txt",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#count",
    "href": "files.html#count",
    "title": "Files",
    "section": "Count",
    "text": "Count\nwc (word count) counts the number of lines, words, and characters in the given input. If a file name is provided, it performs the count on the file; otherwise, it reads from the standard input.\n\nfind data -name \"*.txt\" -mtime -1 | wc\n#&gt;        1       1      21\n\n\ncounts=$(wc data/who_tb_data.txt | awk '{print $1, $2, $3}')\nprintf \"   lines   words characters\\n\"\nprintf \"%8s %7s %10s\\n\" $counts\n#&gt;    lines   words characters\n#&gt;       13      52        381",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#permissions-priviledges",
    "href": "files.html#permissions-priviledges",
    "title": "Files",
    "section": "Permissions & Priviledges",
    "text": "Permissions & Priviledges\n\nchown\n\n\nsudo",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#documentation",
    "href": "files.html#documentation",
    "title": "Files",
    "section": "Documentation",
    "text": "Documentation\n\nman\nman (Manual) displays the user manual of any command that we can run on the terminal. It’s the go-to resource for learning about the options, arguments, and examples of how to use commands.\n\n\nhelp\nhelp or --help option provides information about built-in commands and usage guidelines. It’s a quick way to get help for shell built-ins or to understand the basic usage of a command.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "sys.html",
    "href": "sys.html",
    "title": "System",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.\n\n\n\n\nSystem Monitoring: top, htop, free, df, du",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>System</span>"
    ]
  },
  {
    "objectID": "symbols_patterns.html",
    "href": "symbols_patterns.html",
    "title": "Symbols & Patterns",
    "section": "",
    "text": "Wildcards\nWildcards (also known as glob patterns) are mostly used in commands to match filenames, paths, or filter text (ls, cp, mv, rm, etc.). Arguments can include wildcards, which the shell expands into a list of files or directories that match the pattern.",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Symbols & Patterns</span>"
    ]
  },
  {
    "objectID": "symbols_patterns.html#sec-wildcards",
    "href": "symbols_patterns.html#sec-wildcards",
    "title": "Symbols & Patterns",
    "section": "",
    "text": "Asterisk: *\n* is a wildcard for matching zero or more characters.\nExample\nls *.md lists all files in the data/ directory that end with .md:\n\nls data/*.md\n# data/README.md\n\n\n\nQuestion Mark: ?\n? is the wildcard for matching exactly one character.\nExample\nls myfile?.txt lists files like myfile2.txt, but not myfile.txt and my file 3.txt:\n\nls myfile?.txt\n# myfile2.txt\n\n\n\nSquare brackets: []\n[abc]: Matches any one character listed (a, b, or c).\nExample\n[a-z]: Matches any one character (n, e, or w).\n\nls [new]*.txt\n# newfile.txt\n\nExample\nNatch any one character in range (a to p).\n\nls data/[a-p]*\n# data/ajperlis_epigrams.txt\n# data/music_vids.tsv\n# data/pwrds.csv\n# data/pwrds.tsv",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Symbols & Patterns</span>"
    ]
  },
  {
    "objectID": "symbols_patterns.html#sec-regexp",
    "href": "symbols_patterns.html#sec-regexp",
    "title": "Symbols & Patterns",
    "section": "Regular Expressions",
    "text": "Regular Expressions\nRegular expressions (or the singular ‘regex’) are powerful tools for searching and manipulating text data. A regex is made up of special symbols that define specific patterns to be identified or transformed.\nRegular expressions operate on text–the sequence of characters that can include letters, digits, punctuation, and other character types. Text serves as the ‘data’ or ‘medium’ for which the patterns the regex describes are searched.\nRegular expressions are more complex than wildcards, and are typically used with tools like grep (global regular expression print), sed (stream editor), and awk.\n\nDot: .\n. matches any single character except a newline.\nExamples\nMatches lines containing “password” or similar patterns where any character stands between ‘p’ and ‘ssword’.\n\ngrep \"p.ssword\" data/pwrds.csv\n# password,rank,strength,online_crack\n# password,1,8,6.91 years\n\nReplaces “password” where any character is between ‘p’ and ‘ssword’ with “p@ssword”.\n\nsed 's/p.ssword/p@ssword/' data/pwrds.csv | head -n2\n# p@ssword,rank,strength,online_crack\n# p@ssword,1,8,6.91 years\n\nSelect records where “password” or similar patterns appear with any character between ‘p’, ‘ssw and rd’.\n\nawk '/p.ssw.rd/' data/pwrds.csv\n# password,rank,strength,online_crack\n# password,1,8,6.91 years\n# passw0rd,500,28,92.27 years\n\n\n\nAsterisk: *\n* matches zero or more of the preceding element.\nExamples\nWe can use grep to find lines where “i” is followed by zero or more “l”s (including none):\n\ngrep 'il*' data/wu_tang.txt\n# RZA   Robert Diggs\n# GZA   Gary Grice\n# Method Man    Clifford Smith\n# Ghostface Killah  Dennis Coles\n# U-God     Lamont Hawkins\n# Masta Killa   Jamel Irief\n# Cappadonna    Darryl Hill\n# Ol Dirty Bastard  Russell Tyrone Jones\n\nWe can use sed to replace two or more \"l\"s with 11:\n\nsed 's/lll*/11/g' data/wu_tang.txt\n# Member    Name\n# RZA   Robert Diggs\n# GZA   Gary Grice\n# Method Man    Clifford Smith\n# Raekwon the Chef  Corey Woods\n# Ghostface Ki11ah  Dennis Coles\n# Inspectah Deck    Jason Hunter\n# U-God     Lamont Hawkins\n# Masta Ki11a   Jamel Irief\n# Cappadonna    Darryl Hi11\n# Ol Dirty Bastard  Russe11 Tyrone Jones\n\nPrint lines that start with one or more \"R\"s\n\nawk '/^ *R/' data/wu_tang.txt\n# RZA   Robert Diggs\n# Raekwon the Chef  Corey Woods\n\n\n\nPlus: +\n+ matches one or more occurrences of the preceding element.\nExamples\nUse grep with extended regular expressions to find ‘i’ followed by one or more ’l’s:\n\ngrep -E 'il+' data/wu_tang.txt\n# Ghostface Killah  Dennis Coles\n# Masta Killa   Jamel Irief\n# Cappadonna    Darryl Hill\n\nReplace one or more \"a\"s with the @:\n\nsed -E 's/a+/@/g' data/wu_tang.txt\n# Member    N@me\n# RZA   Robert Diggs\n# GZA   G@ry Grice\n# Method M@n    Clifford Smith\n# R@ekwon the Chef  Corey Woods\n# Ghostf@ce Kill@h  Dennis Coles\n# Inspect@h Deck    J@son Hunter\n# U-God     L@mont H@wkins\n# M@st@ Kill@   J@mel Irief\n# C@pp@donn@    D@rryl Hill\n# Ol Dirty B@st@rd  Russell Tyrone Jones\n\nThe + operator needs the -E option to enable extended regular expressions.\nPrint lines with text containing one or more \"Z\"s:\n\nawk '/Z+/' data/wu_tang.txt\n# RZA   Robert Diggs\n# GZA   Gary Grice\n\n\n\nQuestion Mark: ?\n? makes the preceding element optional (matches zero or one occurrence).\nExamples\nUse grep with extended regular expressions to find lines with ‘Killah’ or ‘Killah’:\n\ngrep -E 'Kill?' data/wu_tang.txt\n# Ghostface Killah  Dennis Coles\n# Masta Killa   Jamel Irief\n\nsed: Replace Ghostface with Ghost Face:\n\nsed -E 's/Ghostface?/Ghost Face/' data/wu_tang.txt\n# Member    Name\n# RZA   Robert Diggs\n# GZA   Gary Grice\n# Method Man    Clifford Smith\n# Raekwon the Chef  Corey Woods\n# Ghost Face Killah Dennis Coles\n# Inspectah Deck    Jason Hunter\n# U-God     Lamont Hawkins\n# Masta Killa   Jamel Irief\n# Cappadonna    Darryl Hill\n# Ol Dirty Bastard  Russell Tyrone Jones\n\nawk: Print lines with one or more digits.\n\nawk '/[0-9]+/' data/wu_tang.txt\n\n\n\nCharacter Set: [abc]\n[abc] matches any single character listed in the set.\nExample\nUse grep to find lines containing ‘a’, ‘b’, or ‘c’:\ngrep '[abc]' filename.txt\n\n\nCaret: ^\n^ matches the start of a line.\nExample\nUse grep to find lines that start with ‘start’:\ngrep '^start' filename.txt\n\n\nDollar: $\n$ matches the end of a line.\nExample\nUse grep to find lines that end with ‘end’:\ngrep 'end$' filename.txt\nThese patterns are extremely powerful in scripting and command-line operations for filtering and manipulating text data efficiently. Here’s how you might use them in combination across different tools:\n\nsed for substitution: Replace ‘foo’ with ‘bar’ only if ‘foo’ appears at the beginning of a line:\n\nsed 's/^foo/bar/' filename.txt\n\nawk for selection: Print lines where the first field matches ‘start’:\n\nawk '/^start/ {print $0}' filename.txt\n\nperl for advanced manipulation: Increment numbers found at the end of each line:\n\nperl -pe 's/(\\d+)$/ $1+1 /e' filename.txt",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Symbols & Patterns</span>"
    ]
  },
  {
    "objectID": "symbols_patterns.html#special-characters",
    "href": "symbols_patterns.html#special-characters",
    "title": "Symbols & Patterns",
    "section": "Special Characters",
    "text": "Special Characters\nSpecial Characters: Characters such as spaces, quotes, and others have special meanings in the shell. They need to be treated carefully when used within arguments.\n\nBraces: {}\nBrace Expansion: Similar to wildcards, brace expansion ({}) allows the creation of multiple text strings from a pattern containing braces.\nExample\ncat wu_tang.{txt,csv}\n\ncat data/wu_tang.{tsv,dat}\n# Member    Name\n# RZA   Robert Diggs\n# GZA   Gary Grice\n# Method Man    Clifford Smith\n# Raekwon the Chef  Corey Woods\n# Ghostface Killah  Dennis Coles\n# Inspectah Deck    Jason Hunter\n# U-God Lamont Hawkins\n# Masta Killa   Jamel Irief\n# Cappadonna    Darryl Hill\n# Ol Dirty Bastard  Russell Tyrone Jones\n# |Member           |Name                 |\n# |RZA              |Robert Diggs         |\n# |GZA              |Gary Grice           |\n# |Method Man       |Clifford Smith       |\n# |Raekwon the Chef |Corey Woods          |\n# |Ghostface Killah |Dennis Coles         |\n# |Inspectah Deck   |Jason Hunter         |\n# |U-God            |Lamont Hawkins       |\n# |Masta Killa      |Jamel Irief          |\n# |Cappadonna       |Darryl Hill          |\n# |Ol Dirty Bastard |Russell Tyrone Jones |\n\nExpands into:\n\ncat data/wu_tang.tsv \ncat data/wu_tang.dat\n# Member    Name\n# RZA   Robert Diggs\n# GZA   Gary Grice\n# Method Man    Clifford Smith\n# Raekwon the Chef  Corey Woods\n# Ghostface Killah  Dennis Coles\n# Inspectah Deck    Jason Hunter\n# U-God Lamont Hawkins\n# Masta Killa   Jamel Irief\n# Cappadonna    Darryl Hill\n# Ol Dirty Bastard  Russell Tyrone Jones\n# |Member           |Name                 |\n# |RZA              |Robert Diggs         |\n# |GZA              |Gary Grice           |\n# |Method Man       |Clifford Smith       |\n# |Raekwon the Chef |Corey Woods          |\n# |Ghostface Killah |Dennis Coles         |\n# |Inspectah Deck   |Jason Hunter         |\n# |U-God            |Lamont Hawkins       |\n# |Masta Killa      |Jamel Irief          |\n# |Cappadonna       |Darryl Hill          |\n# |Ol Dirty Bastard |Russell Tyrone Jones |\n\n\n\nBackslash: \\\n\\ escapes the following character, nullifying its special meaning\nExample\necho \"File name with spaces \\& special characters\" prints the text with spaces and the ampersand:\n\necho \"File name with spaces \\& special characters\"\n# File name with spaces & special characters\n\n\n\nSingle quotes: ''\nSingle quotes (' ') treat every character literally, ignoring the special meaning of all characters.\nExample\necho '$HOME' prints $HOME, not the path to the home directory:\n\necho '$HOME'\n# $HOME\n\n\n\nDouble quotes: \"\"\nDouble quotes (\" \") allow for the inclusion of special characters in an argument, except for the dollar sign ($), backticks (` `), and backslash (\\).\nExample\necho \"$HOME\" prints the path to the home directory:\n\necho \"$HOME\"\n#&gt; /Users/username\n\n\n\nTilde: ~\n~ represents the home directory of the current user.\nExample\nList the items in the user’s home directory:\n\nls ~\n#&gt; Applications\n#&gt; Creative Cloud Files\n#&gt; Desktop\n#&gt; Documents\n#&gt; Downloads\n#&gt; Dropbox\n#&gt; Fonts\n#&gt; Library\n#&gt; Movies\n#&gt; Music\n#&gt; Pictures\n#&gt; Public\n#&gt; R\n#&gt; Themes\n\n\n\nDollar Sign: $\n$ indicates a variable.\nExample\necho $PATH prints the value of the PATH environment variable:\n\necho $PATH\n\n\n\nAmpersand: &\n& runs a command in the background.\nExample\nfirefox & opens Firefox in the background, allowing the terminal to be used for other commands.\n\nfirefox &\n\n\n\nSemicolon: ;\n; separates multiple commands to be run in sequence.\nExample\ncd data; ls changes the directory to data and then lists its contents:\n\ncd data; ls\n# README.md\n# ajperlis_epigrams.txt\n# music_vids.tsv\n# pwrds.csv\n# pwrds.tsv\n# roxanne.txt\n# roxanne_orig.txt\n# roxanne_rev.txt\n# trees.tsv\n# vg_hof.tsv\n# who_tb_data.tsv\n# who_tb_data.txt\n# wu_tang.csv\n# wu_tang.dat\n# wu_tang.tsv\n# wu_tang.txt\n\n\n\nGreater Than: &gt;\nRedirection operators: &gt; directs output to a file or a device.\nExample\necho \"This is my 2nd file\" &gt; myfile2.txt writes \"This is my 2nd file\" into myfile2.txt:\n\necho \"This is my 2nd file\" &gt; myfile2.txt\n\n\n\nLess Than: &lt;\nRedirection operators: &lt; takes input from a file or a device.\nExample\nThen wc &lt; myfile2.txt counts the words in myfile2.txt:\n\nwc &lt; myfile2.txt\n#        1       5      20\n\n\n\nParentheses: ()\nParentheses can be used to group commands or for command substitution with $( ).\nExample\n(cd /data; ls) runs ls in /data without changing the current directory:\n\n(cd data; ls)\n# README.md\n# ajperlis_epigrams.txt\n# music_vids.tsv\n# pwrds.csv\n# pwrds.tsv\n# roxanne.txt\n# roxanne_orig.txt\n# roxanne_rev.txt\n# trees.tsv\n# vg_hof.tsv\n# who_tb_data.tsv\n# who_tb_data.txt\n# wu_tang.csv\n# wu_tang.dat\n# wu_tang.tsv\n# wu_tang.txt\n\n$(command) uses the output of command.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Symbols & Patterns</span>"
    ]
  },
  {
    "objectID": "text_commands.html",
    "href": "text_commands.html",
    "title": "Manipulating Text",
    "section": "",
    "text": "The Text Stream\nUnix/Linux conceptualizes text as a stream, a continuous sequence of characters that can be manipulated in real-time. Streams are crucial for understanding how Unix/Linux commands process text. A text stream can originate from files, input devices, or even the output of other commands. Treating text as a steady stream of inputs offers a versatile and powerful method for text manipulation.",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Manipulating Text</span>"
    ]
  },
  {
    "objectID": "text_commands.html#the-text-stream",
    "href": "text_commands.html#the-text-stream",
    "title": "Manipulating Text",
    "section": "",
    "text": "Refresher: Standard Input and Standard Output\n\n\n\n\n\n\nTwo key concepts in Unix text processing are standard input (stdin) and standard output (stdout). stdin is the default input stream, which often comes from the keyboard or the output of another command. stdout is the default output stream, typically the terminal screen. Many Unix commands read from stdin when no file is specified and write to stdout, allowing the output of one command to become the input of another. This design facilitates the chaining of commands (piping) to perform complex operations in a streamlined manner.\n\nInput generally refers to the data fed into a command, which can come from stdin or be specified as arguments.\nOutput is the data produced by a command, displayed on stdout unless redirected.\n\nThis interconnectivity of stdin and stdout, all communicating through text streams, exemplifies the efficiency and flexibility of Unix-like systems.",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Manipulating Text</span>"
    ]
  },
  {
    "objectID": "text_commands.html#text-manipulation",
    "href": "text_commands.html#text-manipulation",
    "title": "Manipulating Text",
    "section": "Text Manipulation",
    "text": "Text Manipulation\nText manipulation commands embody the Unix philosophy of ‘do one thing and do it well’ and demonstrate the system’s power in processing text streams. This section will explore these fundamental commands, illustrating how they easily interact with text streams, standard input (stdin), and standard output (stdout) to perform complex text manipulations.\n\nWu-Tang\nThe data/ folder contains three different file formats of the members of the Wu-Tang American hip hop collective. We’ll use the tree command to view the contents of the data/ directory:1\n\ntree -P 'wu*' data\n\nThe -P 'wu*' option tells tree to include only files and directories that match the pattern 'wu*'. The pattern here uses a wildcard (*), meaning it will match any file or directory name that starts with \"wu\". The pattern is case-sensitive by default.\n\n# data\n# ├── wu_tang.csv\n# ├── wu_tang.dat\n# ├── wu_tang.tsv\n# └── wu_tang.txt\n# \n# 1 directory, 4 files\n\nWe can add the -f option to instruct tree to display the full path for each file and directory relative to the root of the tree, instead of just showing the names.\n\ntree -Pf 'wu*' data\n\nThe command summarizes the content by showing “1 directory, 4 files”.2\n\n# data\n# ├── data/wu_tang.csv\n# ├── data/wu_tang.dat\n# ├── data/wu_tang.tsv\n# └── data/wu_tang.txt\n# \n# 1 directory, 4 files\n\nThe wu_tang.dat file contains the members in pipe-delimited format:\n\ncat data/wu_tang.dat\n# |Member           |Name                 |\n# |RZA              |Robert Diggs         |\n# |GZA              |Gary Grice           |\n# |Method Man       |Clifford Smith       |\n# |Raekwon the Chef |Corey Woods          |\n# |Ghostface Killah |Dennis Coles         |\n# |Inspectah Deck   |Jason Hunter         |\n# |U-God            |Lamont Hawkins       |\n# |Masta Killa      |Jamel Irief          |\n# |Cappadonna       |Darryl Hill          |\n# |Ol Dirty Bastard |Russell Tyrone Jones |\n\nWe can use head and tail to view specific ‘rows’ of the data:\n\nhead -n8 data/wu_tang.dat | tail -n4\n# |Raekwon the Chef |Corey Woods          |\n# |Ghostface Killah |Dennis Coles         |\n# |Inspectah Deck   |Jason Hunter         |\n# |U-God            |Lamont Hawkins       |\n\n\n\nEpigrams\necho prints its arguments to the standard output (stdout). It can be used in scripts and on the command line to display messages or variables.\n\necho \"Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\"\n# Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\n\necho can also be used to write text to a file created with touch. The quote below comes from Alan Perlis’s 1982 article, “Epigrams on Programming.”3\n\ntouch data/turing_tarpit.txt\necho \"Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\" &gt; data/turing_tarpit.txt\n\ncat displays the content of files straight to the screen, useful for checking what’s in a file quickly.\n\ncat data/turing_tarpit.txt\n# Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\n\nAll 130 epigrams are stored and numbered in the data/perlis_epigrams.txt file.\nhead and tail allow us to view the top and bottom of any text file:\n\nhead data/ajperlis_epigrams.txt\n# One man’s constant is another man’s variable.\n# Functions delay binding; data structures induce binding. Moral: Structure data late in the programming process.\n# Syntactic sugar causes cancer of the semicolon.\n# Every program is a part of some other program and rarely fits.\n# If a program manipulates a large amount of data, it does so in a small number of ways.\n# Symmetry is a complexity-reducing concept (co-routines include subroutines); seek it everywhere.\n# It is easier to write an incorrect program than understand a correct one.\n# A programming language is low level when its programs require attention to the irrelevant.\n# It is better to have 100 functions operate on one data structure than 10 functions on 10 data structures.\n# Get into a rut early: Do the same process the same way. Accumulate idioms. Standardize. The only difference(!) between Shakespeare and you was the size of his idiom list - not the size of his vocabulary.\n\n\ntail data/ajperlis_epigrams.txt\n# In seeking the unattainable, simplicity only gets in the way. If there are epigrams, there must be meta-epigrams.\n# Epigrams are interfaces across which appreciation and insight flow.\n# Epigrams parametrize auras.\n# Epigrams are macros, since they are executed at read time.\n# Epigrams crystallize incongruities.\n# Epigrams retrieve deep semantics from a data base that is all procedure.\n# Epigrams scorn detail and make a point: They are a superb high-level documentation.\n# Epigrams are more like vitamins than protein.\n# Epigrams have extremely low entropy.\n# The last epigram? Neither eat nor drink them, snuff epigrams.\n\nApplying a ‘trust, but verify’ to the previous claim about the Turing tar-pit quote involves using grep (“global regular expression print”) to confirm the text in turing_tarpit.txt is also in dataperlis_epigrams.txt.\ngrep reads from stdin (or a list of files) and outputs the lines that match a specified pattern. Lets see how many epigrams in data/perlis_epigrams.txt include the word “Turing”:\n\ngrep Turing data/ajperlis_epigrams.txt\n# Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\n# What is the difference between a Turing machine and the modern computer? It’s the same as that between Hillary’s ascent of Everest and the establishment of a Hilton hotel on its peak.\n\n\nNumber sequences\nWe’ll add numbers to each of the 130 epigrams in data/ajperlis_epigrams.txt to make them easier to reference. We can use the seq command piped into a formatting command like awk:\n\nseq 130 | awk '{print $1\".\"}' | head\n# 1.\n# 2.\n# 3.\n# 4.\n# 5.\n# 6.\n# 7.\n# 8.\n# 9.\n# 10.\n\nseq 130 generates the sequence of numbers from 1 to 130, then awk '{print $1\") \"}' takes uses the numbers from seq as the input to awk ($1) and appends a period . to each number. We can add the &gt; operator redirects the output to data/numbered_lines.txt.\n\nseq 130 | awk '{print $1\".\"}' &gt; data/numbered_lines.txt\n\nWe can now use paste to combine data/numbered_lines.txt and data/ajperlis_epigrams.txt. The -d option stands is the delimiter to be placed between the pasted lines (which we’ll use to specify a space \" \").\nWe’ll preview the head and tail of our paste before writing to a file:\n\npaste -d \" \" data/numbered_lines.txt data/ajperlis_epigrams.txt | head -n5\n# 1. One man’s constant is another man’s variable.\n# 2. Functions delay binding; data structures induce binding. Moral: Structure data late in the programming process.\n# 3. Syntactic sugar causes cancer of the semicolon.\n# 4. Every program is a part of some other program and rarely fits.\n# 5. If a program manipulates a large amount of data, it does so in a small number of ways.\n\n\npaste -d \" \" data/numbered_lines.txt data/ajperlis_epigrams.txt | tail -n5\n# 126. Epigrams retrieve deep semantics from a data base that is all procedure.\n# 127. Epigrams scorn detail and make a point: They are a superb high-level documentation.\n# 128. Epigrams are more like vitamins than protein.\n# 129. Epigrams have extremely low entropy.\n# 130. The last epigram? Neither eat nor drink them, snuff epigrams.\n\nAll 130 epigrams line up, so we’ll assign the output to the data/numbered_epigrams.txt file.\n\npaste -d \" \" data/numbered_lines.txt data/ajperlis_epigrams.txt &gt; \\\n  data/numbered_epigrams.txt\n\nNow we can re-check our Turing pattern in data/numbered_epigrams.txt:\n\ngrep Turing data/numbered_epigrams.txt\n# 54. Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\n# 83. What is the difference between a Turing machine and the modern computer? It’s the same as that between Hillary’s ascent of Everest and the establishment of a Hilton hotel on its peak.\n\n\n\n\nRoxanne\nThe data/roxanne.txt file contains the lyrics to the 1979 song Roxanne by The Police. We’ll use this file to explore several powerful Unix/Linux command-line utilities that are invaluable for searching, editing, and manipulating text data in files.\n\nGlobal substitutions\nawk is a powerful text processing tool. Here’s an example where awk uses gsub (global substitution) to replace the phrase “red light” with “green light” in the lyrics:\n\nawk '{gsub(/red light/, \"green light\"); print}' \\\n  data/roxanne.txt | head -4\n# Roxanne\n# You don't have to put on the green light\n# Those days are over\n# You don't have to sell your body to the night\n\ngsub(/red light/, \"green light\") tells awk to substitute \"red light\" with \"green light\" globally within each line. print outputs the modified line. Without this, awk would not display anything. The entire command is enclosed in single quotes to prevent the shell from interpreting any special characters.\nIf we wanted to replace \"Roxanne\" with \"Dianne\" throughout the song, we’d use:\n\nsed 's/Roxanne/Dianne/g' data/roxanne.txt\n\n\n\nshow/hide output\n# Dianne\n# You don't have to put on the red light\n# Those days are over\n# You don't have to sell your body to the night\n# Dianne\n# You don't have to wear that dress tonight\n# Walk the streets for money\n# You don't care if it's wrong or if it's right\n# Dianne\n# You don't have to put on the red light\n# Dianne\n# You don't have to put on the red light\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Ro...\n# I loved you since I knew you\n# I wouldn't talk down to you\n# I have to tell you just how I feel\n# I won't share you with another boy\n# I know my mind is made up\n# So put away your make up\n# Told you once I won't tell you again\n# It's a bad way\n# Dianne\n# You don't have to put on the red light\n# Dianne\n# You don't have to put on the red light\n# Dianne (You don't have to put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (You don't have to put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n\n\nThe s stands for substitute, the pattern to be replaced (\"Roxanne\") is followed by the new text (\"Dianne\"), and the g at the end of the command tells sed to perform the substitution globally on each line, rather than stopping after the first occurrence.\nsort arranges lines of text alphabetically or numerically and uniq filters out adjacent repeated lines in a file (often used in conjunction with sort).\n\nsort data/roxanne.txt | uniq\n\n\n\nshow/hide output\n# I have to tell you just how I feel\n# I know my mind is made up\n# I loved you since I knew you\n# I won't share you with another boy\n# I wouldn't talk down to you\n# It's a bad way\n# Ro...\n# Roxanne\n# Roxanne (Put on the red light)\n# Roxanne (You don't have to put on the red light)\n# So put away your make up\n# Those days are over\n# Told you once I won't tell you again\n# Walk the streets for money\n# You don't care if it's wrong or if it's right\n# You don't have to put on the red light\n# You don't have to sell your body to the night\n# You don't have to wear that dress tonight\n\n\nThe commands above sort the lines in the file first, then filter repeated lines.\nWe’ll use awk to add line numbers to data/roxanne.txt. NR is the record number variable in awk, which counts the lines. $0 represents the entire current line, and combining them with print will print the line number followed by the original line.\n\nawk '{print NR, $0}' data/roxanne.txt &gt; data/roxanne_lined.txt\nhead -n5 data/roxanne_lined.txt \n# 1 Roxanne\n# 2 You don't have to put on the red light\n# 3 Those days are over\n# 4 You don't have to sell your body to the night\n# 5 Roxanne\n\n\n\nTwo-file commands\nWe’ll create a ‘metadata’ file for Roxanne in data/roxanne_meta.txt and add some content:\n\ntouch data/roxanne_meta.txt\necho \"1 &lt;Song Title&gt;\n2 &lt;Chorus&gt;\n3 &lt;Verse 1&gt;\n4 &lt;Verse 2&gt;\n5 &lt;Song Title&gt;\" &gt; data/roxanne_meta.txt\ncat data/roxanne_meta.txt\n# 1 &lt;Song Title&gt;\n# 2 &lt;Chorus&gt;\n# 3 &lt;Verse 1&gt;\n# 4 &lt;Verse 2&gt;\n# 5 &lt;Song Title&gt;\n\njoin is used to combine two files based on a common field. Assuming there’s another file with additional details for some lines, you would use:\n\njoin -1 1 -2 1 data/roxanne_lined.txt data/roxanne_meta.txt\n# 1 Roxanne &lt;Song Title&gt;\n# 2 You don't have to put on the red light &lt;Chorus&gt;\n# 3 Those days are over &lt;Verse 1&gt;\n# 4 You don't have to sell your body to the night &lt;Verse 2&gt;\n# 5 Roxanne &lt;Song Title&gt;\n\n-1 1 specifies that the join field for the first file (data/roxanne_lined.txt) is the first column, and -2 1 means that the join field for the second file (data/roxanne_meta.txt) is also the first column.\ncomm is used to compare two sorted files line by line and outputs three columns by default:\n1. Lines unique to the first file. 2. Lines unique to the second file. 3. Lines common to both files.\nLet’s assume we have two versions of the song “Roxanne”. The original version is stored in roxanne_orig.txt, and a revised version with some lines changed, added, or removed is stored in roxanne_rev.txt.\nroxanne_orig.txt\n\ntouch data/roxanne_orig.txt\necho \"Roxanne\nYou don't have to put on the red light\nThose days are over\nYou don't have to sell your body to the night\" &gt; data/roxanne_orig.txt\n\nroxanne_rev.txt\n\ntouch data/roxanne_rev.txt\necho \"Roxanne\nYou don't have to put on the green light\nThose days are over\nYou don't need to sell your dreams to the night\" &gt; data/roxanne_rev.txt\n\nThese files are structured to have similar content with minor differences.\n\n\n\n\n\n\n# Roxanne\n# You don't have to put on the red light\n# Those days are over\n# You don't have to sell your body to the night\n\n\n# Roxanne\n# You don't have to put on the green light\n# Those days are over\n# You don't need to sell your dreams to the night\n\n\n\nFirst, ensure both files are sorted (if not already). For simplicity, let’s assume these are sorted or have matching line orders. Then, use comm:\n\ncomm data/roxanne_orig.txt data/roxanne_rev.txt\n#       Roxanne\n#   You don't have to put on the green light\n#   Those days are over\n# You don't have to put on the red light\n# Those days are over\n# You don't have to sell your body to the night\n#   You don't need to sell your dreams to the night\n\nOutput:\n\nThe first column shows lines that are only in the original file (roxanne_orig.txt).\n\n\n\n\n\n\n\nThe second column shows lines that are only in the revised file (roxanne_rev.txt).\n\n\n\n\n\n\n\nThe third column shows lines that are common in both files.\n\n\n\n\n\n\nWe can suppress any of these columns using the -1, -2, or -3 options. For example, comm -12 data/roxanne_orig.txt data/roxanne_rev.txt will show only the lines that are common in both files:\n\ncomm -12 data/roxanne_orig.txt data/roxanne_rev.txt\n# Roxanne\n\nOr\n\ncomm -1 -2 data/roxanne_orig.txt data/roxanne_rev.txt\n# Roxanne\n\nNOTE: ensure the files are sorted on the lines you are comparing; otherwise, comm will not function correctly.\nTo see the line by line differences between data/roxanne_orig.txt and data/roxanne_rev.txt, pass both files to diff:\n\ndiff data/roxanne_orig.txt data/roxanne_rev.txt\n# 2c2\n# &lt; You don't have to put on the red light\n# ---\n# &gt; You don't have to put on the green light\n# 4c4\n# &lt; You don't have to sell your body to the night\n# ---\n# &gt; You don't need to sell your dreams to the night\n\nThe output of differences from diff can be interpreted as follow:\n\n2c2 indicates that a change has been made at line 2 of both files. The c stands for “change”.\n\n&lt; You don't have to put on the red light shows what line 2 looked like in the original file (roxanne_orig.txt)\n--- is a separator used by diff to distinguish between the old version and the new version of the line\n&gt; You don't have to put on the green light: shows what line 2 now looks like in the revised file (roxanne_rev.txt)\n\n4c4 indicates a change at line 4 in both documents.\n\n&lt; You don't have to sell your body to the night shows the original text at line 4 in roxanne_orig.txt\n---: Again, a separator\n&gt; You don't need to sell your dreams to the night shows the revised text at line 4 in roxanne_rev.txt\n\n\nThe -y option will display the changes side-by-side\n\ndiff -y data/roxanne_orig.txt data/roxanne_rev.txt\n# Roxanne                                                                 Roxanne\n# You don't have to put on the red light                          |       You don't have to put on the green light\n# Those days are over                                                     Those days are over\n# You don't have to sell your body to the night                   |       You don't need to sell your dreams to the night\n\nOr, if all we care about is if the files differ, we can use the -q option:\n\ndiff -q data/roxanne_orig.txt data/roxanne_rev.txt\n# Files data/roxanne_orig.txt and data/roxanne_rev.txt differ",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Manipulating Text</span>"
    ]
  },
  {
    "objectID": "text_commands.html#recap",
    "href": "text_commands.html#recap",
    "title": "Manipulating Text",
    "section": "Recap",
    "text": "Recap\nUsing these commands can dramatically enhance productivity and efficiency when working with text files in Unix/Linux environments.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Manipulating Text</span>"
    ]
  },
  {
    "objectID": "text_commands.html#footnotes",
    "href": "text_commands.html#footnotes",
    "title": "Manipulating Text",
    "section": "",
    "text": "The tree command is not available in every Shell, but you can install it using Homebrew.↩︎\nNote on Directory and File Structure: The output structure from tree -Pf 'wu*' data visually shows that the data directory contains only the files matching the pattern and no subdirectories under it that match the pattern (or any subdirectories at all in this context). The -P option does not cause tree to exclude other directories from the inspection; it only filters what is displayed based on the pattern. If there were non-matching files or subdirectories, they would not appear in the output due to the filter.↩︎\nRead the Wikipedia or download the original PDF.↩︎",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Manipulating Text</span>"
    ]
  },
  {
    "objectID": "text_editors.html",
    "href": "text_editors.html",
    "title": "Text Editors",
    "section": "",
    "text": "nano",
    "crumbs": [
      "Text",
      "Text Editors"
    ]
  },
  {
    "objectID": "text_editors.html#vi-vim",
    "href": "text_editors.html#vi-vim",
    "title": "Text Editors",
    "section": "vi (Vim)",
    "text": "vi (Vim)\nVim, short for Vi IMproved, is an advanced text editor that is an enhanced version of the vi editor common to UNIX systems. Vim designed for both casual text editing and complex code development, making it a popular choice for developers and system administrators alike. Vim is known for its power, flexibility, and efficiency, but it can be intimidating for beginners due to its modal nature and extensive commands.\n\nGetting Started\nTo start using Vim, you can simply type vim followed by the name of the file you wish to edit or create. For example:\n\nvim example.txt\n\nThis command will open example.txt in Vim. If the file doesn’t exist, Vim will create it once you attempt to save.\n\n\nVim Modes\nVim operates in several modes, primarily:\n\nNormal Mode: The default mode where you can use vim commands. No text insertion happens in this mode.\nInsert Mode: Allows you to insert text. Enter this mode by pressing i in Normal Mode.\nCommand Mode: Accessed from Normal Mode by pressing :. Here, you can execute Vim commands and scripts.\n\n\n\nBasic Commands\nHere are some fundamental commands to get you started:\n\ni - Enter insert mode to start typing/editing the text.\nEsc - Return to normal mode from any other mode.\n:w - Save the changes made to the file.\n:q - Quit Vim.\n:wq - Save the changes and quit Vim.\ndd - Delete (cut) a line in normal mode.\nyy - Copy (yank) a line in normal mode.\np - Paste the copied or deleted line below the current line.\n\n\n\nExample Session\nLet’s consider a simple session where you edit a new file:\n\nOpen or create a file:\nvim example.txt\nEnter Insert Mode to start typing:\ni\nType your text, for instance:\nHello, this is a test file with Vim.\nPress Esc to go back to Normal Mode.\nSave and exit:\n:wq",
    "crumbs": [
      "Text",
      "Text Editors"
    ]
  },
  {
    "objectID": "text_editors.html#advanced-features",
    "href": "text_editors.html#advanced-features",
    "title": "Text Editors",
    "section": "Advanced Features",
    "text": "Advanced Features\nAs you grow more comfortable with the basics of Vim, you may explore its advanced features:\n\nMultiple Windows: Open multiple files or views using :split, :vsplit\nMacros: Automate repetitive tasks by recording them.\nCustomizable Settings: Tweak Vim’s behavior through a .vimrc file\nPlugins: Extend Vim’s functionality with plugins like NERDTree for file system navigation or YouCompleteMe for code completion.\n\nWhile Vim has a steep learning curve, mastering it can significantly enhance your productivity and efficiency in text editing tasks. Start with basic commands, gradually exploring more complex features as you become more comfortable with the editor.",
    "crumbs": [
      "Text",
      "Text Editors"
    ]
  },
  {
    "objectID": "text_editors.html#emacs",
    "href": "text_editors.html#emacs",
    "title": "Text Editors",
    "section": "emacs",
    "text": "emacs\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Text",
      "Text Editors"
    ]
  },
  {
    "objectID": "format.html",
    "href": "format.html",
    "title": "Format",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.\n\n\n\n\n\n# Create and populate the file with your data\ntouch who_tb_data.txt\necho \"country year  type  count\nAfghanistan 1999  cases 745\nAfghanistan 1999  population  19987071\nAfghanistan 2000  cases 2666\nAfghanistan 2000  population  20595360\nBrazil  1999  cases 37737\nBrazil  1999  population  172006362\nBrazil  2000  cases 80488\nBrazil  2000  population  174504898\nChina 1999  cases 212258\nChina 1999  population  1272915272\nChina 2000  cases 213766\nChina 2000  population  1280428583\" &gt; who_tb_data.txt\n\n# Get the word count values\ncounts=$(wc who_tb_data.txt | awk '{print $1, $2, $3}')\n\n# Use printf to format the output\nprintf \"   lines   words characters\\n\"\nprintf \"%8s %7s %10s\\n\" $counts\n\nTo make the commands above more generalizable so that any file can be passed as input rather than being restricted to a specific file (who_tb_data.txt), we can modify the script to take a filename as a command-line argument.\nThis way, you can use the script with any file by specifying the filename when you run the script.\n\nStep 1: Modify the Script to Take Command-Line Arguments\nHere’s how the revised script could look:\n#!/bin/bash\n\n# Check if a file name was provided as an argument\nif [ \"$#\" -ne 1 ]; then\n    echo \"Usage: $0 &lt;filename&gt;\"\n    exit 1\nfi\n\n# Check if the file exists\nif [ ! -f \"$1\" ]; then\n    echo \"File not found: $1\"\n    exit 1\nfi\n\n# Get the word count values\ncounts=$(wc \"$1\" | awk '{print $1, $2, $3}')\n\n# Use printf to format the output\nprintf \"   lines   words characters\\n\"\nprintf \"%8s %7s %10s\\n\" $counts\n\n\nStep 2: Save and Make the Script Executable\n\nSave the script in a file, for example, format_wc_output.sh.\nMake sure the script is executable:\nchmod +x format_wc_output.sh\n\n\n\nStep 3: Run the Script with a File as an Argument\n\nNow you can run the script with any file as an argument. For example:\n./format_wc_output.sh somefile.txt\n\n\n\nExplanation\n\nArgument Checking: The script now starts by checking if exactly one argument (the filename) is provided. If not, it prints a usage message and exits. This ensures the user knows how to run the script correctly.\nFile Existence Checking: It checks if the file exists before attempting to process it. If the file doesn’t exist, it prints an error message and exits. This prevents errors related to non-existent files.\nUsing Command-Line Argument: The wc command now uses $1, which is a placeholder for the first command-line argument provided to the script (i.e., the filename you want to process).\n\nThis version of the script is more flexible and useful, as it can handle any file input, making it a handy tool for quickly formatting word count output for various files across your system.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Shell Scripts",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Format</span>"
    ]
  },
  {
    "objectID": "permissions.html",
    "href": "permissions.html",
    "title": "Permissions",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.\n\n\n\n\nFile permissions with chmod\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Shell Scripts",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Permissions</span>"
    ]
  },
  {
    "objectID": "file_size.html",
    "href": "file_size.html",
    "title": "Find Large Files",
    "section": "",
    "text": "find",
    "crumbs": [
      "Use Cases",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Find Large Files</span>"
    ]
  },
  {
    "objectID": "file_size.html#du-sort",
    "href": "file_size.html#du-sort",
    "title": "Find Large Files",
    "section": "du & sort",
    "text": "du & sort",
    "crumbs": [
      "Use Cases",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Find Large Files</span>"
    ]
  },
  {
    "objectID": "file_size.html#awk-ls",
    "href": "file_size.html#awk-ls",
    "title": "Find Large Files",
    "section": "awk & ls",
    "text": "awk & ls",
    "crumbs": [
      "Use Cases",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Find Large Files</span>"
    ]
  },
  {
    "objectID": "file_size.html#ncdu",
    "href": "file_size.html#ncdu",
    "title": "Find Large Files",
    "section": "ncdu",
    "text": "ncdu",
    "crumbs": [
      "Use Cases",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Find Large Files</span>"
    ]
  },
  {
    "objectID": "file_size.html#grep-find-or-du",
    "href": "file_size.html#grep-find-or-du",
    "title": "Find Large Files",
    "section": "grep & find or du",
    "text": "grep & find or du",
    "crumbs": [
      "Use Cases",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Find Large Files</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html",
    "href": "syntax_ref.html",
    "title": "Appendix A — Syntax Reference",
    "section": "",
    "text": "Basics",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#basics",
    "href": "syntax_ref.html#basics",
    "title": "Appendix A — Syntax Reference",
    "section": "",
    "text": "who\nwho shows who is logged on the system.\n\nwho\n#&gt;  username       console      Apr  9 03:20 \n#&gt;  username       ttys000      Apr  9 03:42\n\nwho by itself, without options or arguments, lists the users currently logged into the system.\n\n\nwhoami\nwhoami shows the username of the user currently logged into the system.\n\nwhoami\n#&gt; username\n\n\n\nhostname\nhostname displays the system’s network name.\n\nhostname\n#&gt;  Users-MacBook-Pro-2.local\n\n\n\ndate\ndate displays the current date and time.\n\ndate\n#&gt;  Wed Apr 10 03:39:52 MST 2024\n\n\n\ncal\ncal displays a calender of the current month.\n\ncal\n#&gt;      April 2024       \n#&gt; Su Mo Tu We Th Fr Sa  \n#&gt;     1  2  3  4  5  6  \n#&gt;  7  8  9 10 11 12 13  \n#&gt; 14 15 16 17 18 19 20  \n#&gt; 21 22 23 24 25 26 27  \n#&gt; 28 29 30    \n\n\n\nuptime\nuptime shows how long the system has been running.\n\nuptime\n#&gt;  3:39  up 11:23, 2 users, load averages: 3.82 3.21 3.00\n\n\n\nclear\nclear clears the terminal screen and doesn’t print any return values.\n\nclear\n\nclear does its job without the need for additional input.\n\n\nexit\nexit exits the shell or your current session.\n\nexit\n\nexit requires no arguments or options to execute this action, and doesn’t print any return values.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#navigate",
    "href": "syntax_ref.html#navigate",
    "title": "Appendix A — Syntax Reference",
    "section": "Navigate",
    "text": "Navigate\n\npwd\npwd (Print Working Directory) tells you exactly where you are in the filesystem.\n\npwd # where am I?\n# /Users/mjfrigaard/projects/books/fm-unix\n\n\n\ncd\ncd (Change Directory) lets you move to a different folder on your computer.\nIf you want to move from the one place to another, cd can get you there. For example, cd /bin takes you to the /bin folder, the toolshed of software tools.\n\ncd /bin # change location\npwd # now where am I?\n# /bin\n\n\n\nls\nls (List) is like standing in one location, looking around, and seeing what files and folders are around you. In /bin, ls would show you the software tools available:\n\ncd /bin # change location\nls # what's in here?\n# [\n# bash\n# cat\n# chmod\n# cp\n# csh\n# dash\n# date\n# dd\n# df\n# echo\n# ed\n# expr\n# hostname\n# kill\n# ksh\n# launchctl\n# link\n# ln\n# ls\n# mkdir\n# mv\n# pax\n# ps\n# pwd\n# realpath\n# rm\n# rmdir\n# sh\n# sleep\n# stty\n# sync\n# tcsh\n# test\n# unlink\n# wait4path\n# zsh\n\n\n\nlocate\n\nlocate bin",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#manage",
    "href": "syntax_ref.html#manage",
    "title": "Appendix A — Syntax Reference",
    "section": "Manage",
    "text": "Manage\nIn the Unix/Linux world, file and directory management is a fundamental skill. This chapter dives deep into the commands that allow users to create, copy, move, remove, and link files and directories. Each section below introduces a different command, detailing its purpose and providing examples of its use.\n\nmkdir\nmkdir (Make Directory) builds a new folder wherever you tell it to, like making a new folder in our project for outputs (out/) or documents (doc/).\nmkdir out\nmkdir doc\n\n\ncp\ncp (Copy) duplicates files or folders. The cp command is used to Copy files or directories from one location to another. Imagine having a file (myfile.txt) on your root (.) directory that you want to copy to the /data folder; you could use cp to make a duplicate.\n\ncp myfile.txt data/myfile.txt\n# confirm copy\nls data\n# ajperlis_epigrams.txt\n# music_vids.tsv\n# myfile.txt\n# roxanne.txt\n# roxanne_orig.txt\n# roxanne_rev.txt\n# trees.tsv\n# vg_hof.tsv\n# who-tb-data.tsv\n# who_tb_data.txt\n# wu_tang.csv\n# wu_tang.dat\n# wu_tang.tsv\n# wu_tang.txt\n\n\n\nmv\nmv (Move): mv, short for Move, functions similarly to picking up a book from your desk and placing it on a shelf. It moves files or directories from one location to another. It can also be used for renaming files. This command is especially useful for organizing files and directories that are in the wrong place.\n\n# create folder\nmkdir doc\n# move file\nmv data/myfile.txt doc/myfile.txt \n\n\n# confirm move\nls doc\n# myfile.txt\n\n\n\nrm\nrm (Remove): The rm command stands for remove and is used to delete files or directories.\n\n# remove doc folder\nrm doc\n# rm: doc: is a directory\n\nBy default, it won’t remove a directory without the -R or -r option.\n\n\n\n\n\n\nWarning\n\n\n\n\n\n\nIt’s important to note here that the command-line is not very forgiving. Using rm is a powerful action with significant consequences, as it permanently deletes files, akin to shredding documents. There’s usually no easy way to recover deleted files unless you have a backup.\n\n‘Unix is like a chainsaw. Chainsaws are powerful tools, and make many difficult tasks like cutting through thick logs quite easy. Unfortunately, this power comes with danger: chainsaws can cut just as easily through your leg.’ - Gary Bernhardt1\n\n\n\n\n\n\n# add option \nrm -R doc\n\n\n\nln\nln (Link): ln creates Links to files or directories, making them accessible from multiple locations without duplicating the actual content. It’s like creating a shortcut on your desktop to a program you frequently use. There are two types of links: hard links and symbolic (soft) links. Symbolic links are more commonly used because they can link to directories and provide more flexibility.\nln -s /path/to/original /path/to/link",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#system",
    "href": "syntax_ref.html#system",
    "title": "Appendix A — Syntax Reference",
    "section": "System",
    "text": "System\n\nps\nps (Process Status) reports a snapshot of information about all running processes, regardless of the owner, including the user, CPU and memory usage, process ID, and the command that started each process.. Command options can expand the selection to include other users’ processes, full command lines, etc.\n\n\ndf\ndf (Disk Free) shows disk usage in a human-readable format, including the size, used space, available space, and the mount point of each filesystem. By default, it displays sizes in 1K blocks but can show them in a more readable format (like MB or GB) with the -h option (human-readable).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#files",
    "href": "syntax_ref.html#files",
    "title": "Appendix A — Syntax Reference",
    "section": "Files",
    "text": "Files\n\ntouch\ntouch creates a new empty file if the specified file does not exist. If the file does exist, touch updates its access and modification timestamps.\n\ntouch newfile.txt",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#view",
    "href": "syntax_ref.html#view",
    "title": "Appendix A — Syntax Reference",
    "section": "View",
    "text": "View\n\nfile\nfile gives you a summary of what a computer file is or what it contains, like telling you if a tool in /bin/pax is a program you can run or a text file.\n\nfile /bin/pax\n# /bin/pax: Mach-O universal binary with 2 architectures: [x86_64:Mach-O 64-bit executable x86_64] [arm64e:Mach-O 64-bit executable arm64e]\n# /bin/pax (for architecture x86_64):   Mach-O 64-bit executable x86_64\n# /bin/pax (for architecture arm64e):   Mach-O 64-bit executable arm64e\n\n\n\nless\nless lets you skim through a file on your computer, moving forwards and backwards as you please.\n\nless data/vg_hof.tsv\n\n\n\n\nEnter less in the Terminal to scroll",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#search",
    "href": "syntax_ref.html#search",
    "title": "Appendix A — Syntax Reference",
    "section": "Search",
    "text": "Search\n\nfind\nfind is used to search for files and directories in a directory hierarchy based on various criteria such as name, size, file type, and modification time.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#permissions-priviledges",
    "href": "syntax_ref.html#permissions-priviledges",
    "title": "Appendix A — Syntax Reference",
    "section": "Permissions & Priviledges",
    "text": "Permissions & Priviledges\n\nchown\n\n\nsudo",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#documentation",
    "href": "syntax_ref.html#documentation",
    "title": "Appendix A — Syntax Reference",
    "section": "Documentation",
    "text": "Documentation\n\nman\nman (Manual) displays the user manual of any command that we can run on the terminal. It’s the go-to resource for learning about the options, arguments, and examples of how to use commands.\n\n\nhelp\nhelp or --help option provides information about built-in commands and usage guidelines. It’s a quick way to get help for shell built-ins or to understand the basic usage of a command.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#text",
    "href": "syntax_ref.html#text",
    "title": "Appendix A — Syntax Reference",
    "section": "Text",
    "text": "Text\n\necho\nThe echo command prints its arguments to the standard output. It’s commonly used in scripts and on the command line to display messages or variables.\n\n\ncat\ncat (Concatenate): cat displays the content of files straight to your screen, useful for checking what’s in a text file quickly.\nThis is similar to printing a file and laying out the pages on the floor to see them all at once.\n\n\nhead\nhead displays the first part of files, allowing you to see the beginning of a file. By default, it shows the first 10 lines.\n\n\ntail\ntail displays the last part of files, useful for viewing the end of a file or for monitoring changes to a file in real time with -f.\n\n\nseq\nseq is a simple utility used to generate a sequence of numbers, which can be very handy for scripting and looping operations. seq is often used to provide numeric input for loops or to create numbered lists in files.\n\n\ngrep\ngrep stands for “global regular expression print” and it reads from stdin or a list of files and outputs the lines that contain matches for a specified pattern.\n\n\nsed\nsed (Stream Editor) is a powerful utility in Unix and Linux systems designed for processing text in a stream, meaning it can filter and transform text in a pipeline. It’s typically used to automate editing of large numbers of files and to modify files too large to open in a traditional text editor.\n\n\nawk\nawk reads from a file or a stream, breaks up each line into fields, checks each line against patterns specified in the program, and then performs specified actions on matching lines.\n\n\ngsub\ngsub is not a standalone command but a function used within awk, gsub stands for “global substitution” and it is used to replace all occurrences of a specified pattern within each input record. It takes three arguments: the pattern to match, the replacement string, and the target string or field where the substitutions should be made. It operates on the entire line (or specified field) and replaces every occurrence of the pattern with the replacement.\n\n\nsort\nsort: sort arranges the lines in a text file into order (similar to organizing our stack of papers alphabetically).\n\n\nuniq\nuniq: uniq helps by removing duplicate lines from a file, making sure every line is unique (i.e., you’ve accidentally printed duplicates of a document, and you remove the extra copies).\n\n\ncut\ncut: cut extracts specific parts of lines in a file, like cutting out columns of text. The example below shows the names and log in times of the currently logged in users:\n\nwho | cut -c 1-16,26-42\n# mjfrigaard           Apr 14 13:45\n# mjfrigaard           Apr 14 13:45\n# mjfrigaard           Apr 14 13:45\n# mjfrigaard           Apr 16 05:56\n\n\n\npaste\npaste: paste combines lines from multiple files side by side (like taking snippets of text from different documents and sticking them together into one).\n\n\njoin\njoin: join merges lines from two files based on a common field.\nIf you have two lists with common information, you might merge them based on what matches. ### comm {#sec-comm}\ncomm (Compare): comm compares two sorted files line by line to see what items appear on both, only on one, or the other.\n\n\ndiff\ndiff (Difference): Highlighting what changes have been made between your draft and the final copy of a letter, diff shows the differences between two files.\n\n\nwc\nwc (word count) counts the number of lines, words, and characters in the given input. If a file name is provided, it performs the count on the file; otherwise, it reads from the standard input.\n\n\nxargs\nxargs builds and executes command lines from standard input (stdin). It’s most often used in combination with other commands through pipes. xargs takes input from a pipe and passes it as arguments to another command, allowing for powerful command-line operations that process a list of inputs iteratively.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#footnotes",
    "href": "syntax_ref.html#footnotes",
    "title": "Appendix A — Syntax Reference",
    "section": "",
    "text": "As quoted in Bioinformatics Data Skills: Reproducible and Robust Research with Open Source Tools (2015) by Vince Buffalo.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Bash\nBash, short for Bourne Again SHell, is a command line interface and scripting language for operating systems, enabling direct command input and task automation. Originally created for the GNU project and known for its flexibility and powerful features, Bash is the standard shell on many Linux distributions and was the default shell in the Terminal on macOS until the Catalina release.",
    "crumbs": [
      "Appendices",
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#sec-nfs",
    "href": "glossary.html#sec-nfs",
    "title": "Glossary",
    "section": "Network File System (NFS)",
    "text": "Network File System (NFS)\nNFS is a file system protocol that enables a user to access files over a network. It provides a central location for storing and sharing files across multiple computers and allows users to work with files on remote servers as if they were on their local machine.",
    "crumbs": [
      "Appendices",
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#sec-onprem",
    "href": "glossary.html#sec-onprem",
    "title": "Glossary",
    "section": "On-prem",
    "text": "On-prem\nOn-prem or on-premises refers to software and technology installed and running on computers on the premises of the user instead of a remote facility. On-prem can include data centers, servers, and other hardware within a company’s property. It is chosen for greater control over the computing environment and compliance reasons, as the organization is responsible for managing the security, maintenance, and updating of the systems.",
    "crumbs": [
      "Appendices",
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#sec-quarto",
    "href": "glossary.html#sec-quarto",
    "title": "Glossary",
    "section": "Quarto",
    "text": "Quarto\nQuarto is an open-source scientific and technical publishing framework designed to work with R, Python, Julia, Observable JavaScript, and more, making it a versatile tool for data scientists, researchers, and anyone involved in data analysis.",
    "crumbs": [
      "Appendices",
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#sec-yaml",
    "href": "glossary.html#sec-yaml",
    "title": "Glossary",
    "section": "YAML",
    "text": "YAML\nYAML: YAML is a human-friendly data format for configuration files and data exchange, using key-value pairs, lists, and indentation to organize data.\nkey: value\n  key: value\nIt’s readable and easily parsed by machines, making it popular for application configuration and data sharing.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Appendices",
      "Glossary"
    ]
  },
  {
    "objectID": "macos.html",
    "href": "macos.html",
    "title": "Appendix B — macOS",
    "section": "",
    "text": "macOS Terminal\nThe default Terminal app on macOS provides a solid interface to access the Unix command line, but there are other terminal emulators available (like iTerm2) that offer additional features such as split panes, search, and customization options.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>macOS</span>"
    ]
  },
  {
    "objectID": "macos.html#recap",
    "href": "macos.html#recap",
    "title": "Appendix B — macOS",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>macOS</span>"
    ]
  },
  {
    "objectID": "use_cases.html",
    "href": "use_cases.html",
    "title": "Use Cases",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.",
    "crumbs": [
      "Use Cases"
    ]
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "Basics",
    "section": "",
    "text": "Basic Unix/Linux Commands",
    "crumbs": [
      "Basics"
    ]
  },
  {
    "objectID": "basics.html#recap",
    "href": "basics.html#recap",
    "title": "Basics",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Basics"
    ]
  },
  {
    "objectID": "text.html",
    "href": "text.html",
    "title": "Text",
    "section": "",
    "text": "Plain Text Files\nPlain text files are vital to Unix/Linux systems, embodying their philosophy of ‘simple and beautiful.’1 These files only contain text, making them versatile and powerful. Unix/Linux believes that ‘everything is a file,’ including devices, configurations, and processes.2 Plain text files are the universal interface between systems, programs, and users. Standard Unix/Linux tools can easily create, manipulate, and read plain text, making it an essential interface for system administration, programming, and process management.",
    "crumbs": [
      "Text"
    ]
  },
  {
    "objectID": "text.html#plain-text-files",
    "href": "text.html#plain-text-files",
    "title": "Text",
    "section": "",
    "text": "Simple and Efficient\nPlain text files are simple, versatile, and easy to work with. They can be edited with any text editor and don’t require specialized software, making actions transparent and learning accelerated. Plain text can also be easily manipulated using standard Unix/Linux text-processing tools such as grep, sed, and awk. With simple one-liners from the command line, users can search for a specific line, replace text across multiple files, or transform data formats.\n\n\nCommunication Between Programs\nUnix/Linux philosophy values specialized programs that work together efficiently. Plain text files are used as inputs or outputs in pipelines of simple, single-purpose programs to perform complex operations.",
    "crumbs": [
      "Text"
    ]
  },
  {
    "objectID": "text.html#text-streams",
    "href": "text.html#text-streams",
    "title": "Text",
    "section": "Text Streams",
    "text": "Text Streams\nA text stream in Unix and Linux is a simple, sequential flow of characters. Text streams can be inputs from keyboards, outputs to a display screen, or the data within a file. The concept of text streams is fundamental to the Unix philosophy; it allows for the chaining together of commands, where the output of one command can be seamlessly passed as input to another through a mechanism known as piping.",
    "crumbs": [
      "Text"
    ]
  },
  {
    "objectID": "text.html#text-editors-and-the-unix-philosophy",
    "href": "text.html#text-editors-and-the-unix-philosophy",
    "title": "Text",
    "section": "Text Editors and The Unix Philosophy",
    "text": "Text Editors and The Unix Philosophy\nThe Unix philosophy emphasizes simplicity, clarity, and the principle of “doing one thing well.” Plain text embodies this philosophy, serving as a simple, straightforward, and versatile means of interaction between the user, the system, and the programs running on it. This philosophy also underpins the design of Unix text editors, which range from the simple (like nano) to the powerful and extensible (like vi and emacs).\nText commands and editors are not just tools but the medium through which users communicate with the system and manipulate it to their will. Mastering these commands and editors opens up a world of possibilities for efficient system management, programming, and beyond.\nThis section will explore the core text commands that every Unix and Linux user should know, from file manipulation to text processing and searching. We will also introduce the most popular text editors, guiding you through their primary usage and highlighting their unique features.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Text"
    ]
  },
  {
    "objectID": "text.html#footnotes",
    "href": "text.html#footnotes",
    "title": "Text",
    "section": "",
    "text": "Doug McIlroy on Unix programming: “Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface.” - Wikipedia↩︎\nLinus Torvalds (creator and lead developer of the Linux kernel) has clarified this to, “The UNIX philosophy is often quoted as ”everything is a file”, but that really means ”everything is a stream of bytes.”↩︎",
    "crumbs": [
      "Text"
    ]
  },
  {
    "objectID": "scripts.html",
    "href": "scripts.html",
    "title": "Shell Scripts",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.\n\n\n\n\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Shell Scripts"
    ]
  },
  {
    "objectID": "setups.html",
    "href": "setups.html",
    "title": "Set-Ups",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.\n\n\n\n\nThis book section details the practical aspects of preparing your environment to work with Unix/Linux systems. It is designed to guide readers through various setup processes, catering to different preferences and requirements. We’ll cover working directly with shells and terminals, setting up a virtual machine, and using Quarto documents to execute Bash commands.\nShells and Terminals covers the differences between shells and terminals, and some common options for both.\nVirtual Machines covers how to run Unix on a virtual machine.\nQuarto covers how to setup up the Quarto publishing system.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Set-Ups"
    ]
  },
  {
    "objectID": "syntax.html",
    "href": "syntax.html",
    "title": "Syntax",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.\n\n\n\n\nIn Unix-like operating systems, the terms commands, arguments, and options refer to the components of the syntax you type into the terminal.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Syntax"
    ]
  }
]