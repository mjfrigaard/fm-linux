[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fm-unix",
    "section": "",
    "text": "Welcome!\nWelcome to my Unix/Linux Field Manual!\nThis book aims to provide a quick reference for basic concepts and common tasks when using Unix/Linux operating systems.1",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "fm-unix",
    "section": "",
    "text": "Most of these commands will work on any Shell (Zsh, Bash, etc.) program.↩︎\nBased on the U.S. Army Field Manuals (FMs).↩︎",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Book outline\nThe first four sections of this manual cover concepts and materials for users who are new to Unix/Linux. If you’re familiar with Unix/Linux command-line tools and regular expressions, feel free to skip these chapters.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#book-outline",
    "href": "preface.html#book-outline",
    "title": "Preface",
    "section": "",
    "text": "Introduction\nThe Introduction acquaints you with some background that formed the landscape of Unix/Linux systems. It explains the Bash shell, a command-line interface (CLI), which is the gateway to leveraging the full potential of Unix/Linux systems.\n\n\nSet-Ups\nBefore diving into the commands and scripts, setting up your Unix/Linux environment is crucial. Set-ups guides you through various options for setting up Shells and Terminals, Virtual Machines, and Quarto documents.\n\n\nThe Basics\nNavigating and managing files and folders are daily tasks for Unix/Linux users. Directories covers essential commands such as cd for changing directories, pwd to print the current directory, ls for listing files, and mkdir for making directories.\nFiles chapter covers file manipulation commands like cp (copy), mv (move), rm (remove), and less for viewing file content, ensuring you can organize and manage your file system effectively. These commands are the building blocks for more complex operations in Unix/Linux.\nThe System chapter goes over common system commands like top for monitoring processes in real-time, ps for listing currently running processes, kill to stop a process, df (Disk Free) to display disk space usage on all mounted filesystems, and du (Disk Usage) to estimate file space usage. These commands help you manage the system’s processes and resources efficiently, ensuring you have a clear view of resource allocation and consumption.\n\n\nSyntax\nUnderstanding the syntax is vital for effectively communicating with the Unix/Linux system. The Syntax section demystifies the structure of commands, including how to differentiate between Commands, Arguments, and Options and manage inputs and outputs. This knowledge is critical to executing tasks efficiently in the command line.\nPipes are a cornerstone of Unix/Linux productivity, enabling the output of one command to serve as the input to another. Syntax also covers how to combine commands using stdout and stdin, allowing for robust command chains that can perform complex tasks with a simple syntax. Understanding pipes unlocks a higher level of command-line efficiency and is a step towards advanced Unix/Linux usage.\n\n\nManipulating Text (in development)\nThe Text section covers using Symbols & Patterns to build regular expressions (regex). Unix/Linux systems are renowned for their powerful text manipulation capabilities. The Manipulating Text chapter introduces commands such as cat for displaying file contents, grep for searching within files, sort for sorting data, uniq for filtering unique lines, and cut, paste, and join for editing files. Mastering these commands will allow you to handle and process text data efficiently. Text Editors covers nano, vim, and emacs.\n\n\nShell Scripts (in development)\nShell scripting is a powerful tool for automating repetitive tasks in Unix/Linux. This section introduces you to writing your shell scripts, covering the basics of script creation, execution, and debugging. You’ll learn how to automate simple tasks, making your Unix/Linux experience more productive and enjoyable.\nThe Format chapter delves into the differences between these formats, guiding you on writing compatible scripts that can run across different Unix/Linux systems. Permissions explains how Unix/Linux permissions work and teaches you how to set and modify permissions to protect your data and system from unauthorized access.\n\n\nUse cases (in development)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#data-files",
    "href": "preface.html#data-files",
    "title": "Preface",
    "section": "Data files",
    "text": "Data files\nThe data files used in this book are documented in data/README.md.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#recap",
    "href": "preface.html#recap",
    "title": "Preface",
    "section": "Recap",
    "text": "Recap\nBy the end of this book, you’ll have a solid understanding of the Unix/Linux operating system. You’ll be equipped to navigate, manage files, write scripts, and set permissions confidently. Whether you’re looking to enhance your career prospects, manage your Unix/Linux systems, or simply satisfy your curiosity, this book will be your companion on a fascinating journey into the world of Unix/Linux.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "The Tale of Unix\nImagine Unix and Linux as the master and apprentice in the vast workshop of computer operating systems. Our story begins in the late 1960s at AT&T’s Bell Labs. Unix was born out of a desire for a more flexible and portable operating system. It was a time when computers were as big as rooms and operated on specific, often incompatible, systems. Unix was a breath of fresh air because it was designed to be simple, elegant, and, most importantly, portable, meaning it could run on different types of hardware.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#the-tale-of-unix",
    "href": "intro.html#the-tale-of-unix",
    "title": "Introduction",
    "section": "",
    "text": "Unix Philosophy\nThe Unix philosophy has been distilled into a comprehensive operating system of essential commands and operations, guiding other apprentices in creating their versions of tools and systems.\nUnix is like the master craftsman in this story, having laid the foundational tools and techniques, and crafting a blueprint for how computers could efficiently and securely manage tasks like organizing files or running software.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#the-emergence-of-linux",
    "href": "intro.html#the-emergence-of-linux",
    "title": "Introduction",
    "section": "The Emergence of Linux",
    "text": "The Emergence of Linux\nFast forward a couple of decades to 1991, when a Finnish student named Linus Torvalds decided to create his own free operating system kernel, inspired by Unix. This kernel, which is at the heart of the Linux operating system, was made freely available to anyone who wanted to use or improve it.\n\nOpen-source\nLinux flourished with the help of developers around the world. It’s a testament to what collaboration and shared goals can achieve. This global effort resulted in an operating system that is not only free but also incredibly powerful and reliable.\nLinux, inspired by the teachings of Unix, was the eager apprentice who learned from the master’s manual and decided to share its own set of tools with the world for free. Making it free meant anyone could contribute their skills, refine the tools, and share the fruits of their labor. This open-source community of developers eventually cultivated various specialties, known as “distributions,” each with ts unique set of tools and embellishments, yet all rooted in the same foundational teachings.\n\n\n\n\n\n\nModern Uses of Unix/Linux\n\n\n\n\n\n\nToday, Unix and Linux are everywhere. They’re the invisible forces behind much of the Internet and are responsible for running servers, desktops, smartphones, and even household appliances.\n\nServers and Supercomputers: The majority of the web servers powering the internet, as well as the world’s most powerful supercomputers, run on Linux due to its stability, security, and efficiency.\nDesktops: Although not as common on desktops as Windows or macOS, Linux distributions offer a free, customizable alternative for users.\nSmartphones: Android, the most popular mobile operating system, is powered by a Linux kernel, making Linux the silent workhorse behind billions of smartphones.\nEmbedded Systems: From smartwatches to smart home devices, Linux is often the go-to choice for running embedded systems thanks to its scalability and low cost.\n\nUnix and Linux have grown from niche systems used by academics and researchers to foundational elements that power much of the digital world. They exemplify the power of open collaboration and innovation, showing how a small project or idea can grow and change the world.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#the-shell",
    "href": "intro.html#the-shell",
    "title": "Introduction",
    "section": "The Shell",
    "text": "The Shell\nIn the story of Unix and Linux, Shells are the interpreters that translate commands into actions a computer can understand and execute. If Unix and Linux are the environments where the heavy lifting of computing happens—managing files, running programs, and controlling hardware—then the shell commands are the language and syntax that spells out how the work happens.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#recap",
    "href": "intro.html#recap",
    "title": "Introduction",
    "section": "Recap",
    "text": "Recap\nTo summarize, Unix and Linux provide the underlying framework for computer programs. They’re like the behind-the-scenes craftsmen ensuring the workshop runs smoothly, whether crafting a simple piece of furniture (like running a straightforward program on your computer), or constructing an elaborate mansion (like managing the complex operations of a large server).\nThe shell is like the skilled artisan’s primary tool within the grand workshop of Unix and Linux, serving as a bridge between the user and the system’s deeper capabilities. Just as a master carpenter relies on a trusted hammer or saw, users of Unix and Linux turn to the shell for navigating and manipulating the vast landscape of these operating systems.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "sh_term.html",
    "href": "sh_term.html",
    "title": "Shells and Terminals",
    "section": "",
    "text": "Shells\nThe shell processes user commands, which might involve calling other programs, and returns the output to the user. Shells can be either command-line based or graphical. Some popular examples of shells include Bash, Zsh, and Fish.\nYou can use the echo and ps commands to discover which shell you’re currently using:\necho $SHELL\n# /bin/bash\nps -p $$\n#   PID TTY           TIME CMD\n# 16082 ttys003    0:00.02 -bash",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Shells and Terminals</span>"
    ]
  },
  {
    "objectID": "sh_term.html#sec-shells",
    "href": "sh_term.html#sec-shells",
    "title": "Shells and Terminals",
    "section": "",
    "text": "Tip\n\n\n\n\n\n\nAs we can see from the outupt above, I’m currently using the Bash shell. I’ll use a green callout box to indicate the programs or tools I’m using.\n\n\n\n\n\n Bash\nIntroduced in 1989, Bash, or the Bourne Again SHell has become the default command-line interface or shell for most Linux distributions. Incorporating features from the Korn shell (ksh) and the C shell (csh), Bash supports features like command history, tab completion, aliases and scripting tasks.1\n\n\n\n\n\n\nKey features of  Bash\n\n\n\n\n\n\n\nProgramming\nBash includes an array of programming constructs for scripting:\n\nConditional statements (if, then, else, elif, fi)\n\nLooping statements (for, while, until)\n\nInteractive Command Line Editing\nBash provides an interactive command line editing environment where users can navigate and edit commands directly on the command line using Emacs or Vi editing modes.\nHistory expansion\nCommands can be re-executed by recalling them from the history\nTab Completion\nBash supports tab completion for command names, file names, and even command arguments, speeding up the input process and reducing typos\nComprehensive Job Control\n\nBackgrounding (&), foregrounding (fg), and job management (jobs, bg)\n\nStopping (suspending) processes and continuing them with kill and kill -CONT\n\nAliases\nUsers can create shorter commands to represent longer sequences of commands using aliases\nShell Functions\nBash also supports more powerful functions that can take arguments like small scripts.\nScript Debugging\nBash scripts can be debugged using options like set -x to print commands and their arguments as they are executed, which is invaluable for troubleshooting scripts\nEnvironment Control\n\nEnvironment variables configuration and management\nVariables are exported to make them available to sub-processes\n\nExpansion Capabilities\nBash supports several types of expansions that enhance its scripting capabilities:\n\nBrace expansion: {a,b,c}\nTilde expansion: ~ translates to the home directory.\nParameter and variable expansion: $name or ${name}\nArithmetic expansion: $(( expression )0\n\nHistory Features\nBash maintains a history of commands that users have executed, which can be navigated, searched, and reused. It also supports configuring the history size and behavior through various environment variables like HISTSIZE and HISTFILESIZE.\n\n\n\n\n\n\n Zsh\nZsh (Z Shell or ‘Oh My ZSH!’) is noted for its interactive features and is often used with customization frameworks. Zsh is a powerful command-line interpreter for Unix systems that serves as both a scriptable shell and an interactive command interpreter.2\n\n\n\n\n\n\nKey features of  Zsh\n\n\n\n\n\n\n\nCommand Line Editing\nZsh provides an advanced and customizable command-line editing environment. Users can configure key bindings and have extensive control over the text editing capabilities directly within the command prompt.\nTab Completion\nZsh has one of the most powerful tab completion systems. It supports:\n\nCompletion for command options and arguments.\nAutomatic listing of options when a tab is hit twice.\nContext-sensitive completion that can recognize patterns in filenames, history, running processes, hostnames, and more.\n\nThemes and Prompts\nZsh allows extensive customization of its prompt, supporting themes that can completely change the look of your command line. The prompt can include colors, content from shell variables, functions, and command outputs.\nScripting\nZsh scripting is robust, with features like arrays, associative arrays, and floating-point arithmetic which are not typically available in all shells. It enhances scripting capabilities and improves on the scripting syntax of the Bourne Shell.\nLoadable Modules: Zsh supports dynamically loadable modules, expanding its capabilities with features like:\n\nFTP client\nTCP and UDP socket operations\nAdvanced math functions\nFull-fledged regular expression matching\n\nImproved Variable Handling\nVariable handling in Zsh includes several enhancements like:\n\nBetter array handling\nAssociative arrays (similar to dictionaries in higher-level programming languages)\nEasier string manipulation and pattern matching\n\nSpell Check and Correction\nZsh can be configured to correct commands automatically if misspelled and to suggest corrections or alternatives. This feature helps in reducing syntax errors and improves user efficiency.\nExtended Globbing\nZsh’s file globbing allows for more complex pattern matching than traditional Unix shells. You can specify patterns in a more expressive and powerful way, which is particularly useful in scripts.\n\n\n\n\n\n\n Fish\nFish, or the Friendly Interactive SHell, is a smart and user-friendly command line shell for Unix-like operating systems. It’s designed to be more interactive and user-friendly than traditional shells like Bash or Zsh.3\n\n\n\n\n\n\nKey features of  Fish\n\n\n\n\n\n\n\nAutosuggestions\nFish suggests commands as you type based on history and completions, just like a web browser. This feature allows users to see and reuse previous commands by simply pressing the right arrow key to complete the suggested command, which can significantly speed up typing and reduce errors.\nSyntax Highlighting\nOne of Fish’s most noticeable features is its real-time syntax highlighting. Commands that are valid change color as you type them. It also helps users catch errors before the command is executed, such as highlighting misspelled commands or incorrect paths in red.\nWeb-Based Configuration\nFish includes a web-based configuration interface (accessible via the fish_config command), which makes customizing the shell settings and prompt easier for users who prefer a graphical interface over editing configuration files manually (or if you’re new to the command line).\nEnhanced Tab Completion\nFish provides intelligent tab completions for commands, file names, variables, and user-defined functions. It not only completes based on the prefix but also considers the whole line context, making the completions more relevant.\nImproved Variables and Scoping\nFish simplifies variable management, including universal variables that are automatically shared between all running shells and persist across restarts without needing explicit saving to a file. Variable scoping is also more straightforward, helping avoid common bugs seen in other shells.\nFunction Autoloads\nFish allows functions to be defined in individual files and automatically loads them only when needed. This lazy-loading of functions helps speed up the start time of the shell.\nExtensible\nFish is designed to be easily extensible through plugins. The Fisherman and Oh My Fish frameworks offer many plugins and themes designed to enhance Fish’s capabilities or customize its appearance.\nMan Page Completions\nFish generates command completions automatically from man pages, which means it often supports completions for all the installed commands without needing special configuration.\nNo Configuration Needed\nFish is designed to work properly out of the box, without needing to configure it extensively. This makes it very accessible for new users or those who want a powerful shell without the need to customize or configure it heavily.\nUser-Friendly Scripts\nFish uses a syntax that is slightly different from the traditional POSIX shell syntax, which is often simpler and easier to understand. For example, loops and conditionals are clearer, and there is no need for explicit subshell management.\n\n\n\n\n\nChanging the Shell\nOn macOS Sonoma (or any other recent version of macOS), you can change your default shell using the Terminal.\nList all the available shells with the commands below:\n\ncat /etc/shells\n## # List of acceptable shells for chpass(1).\n## # Ftpd will not allow users to connect who are not using\n## # one of these shells.\n## \n## /bin/bash\n## /bin/csh\n## /bin/dash\n## /bin/ksh\n## /bin/sh\n## /bin/tcsh\n## /bin/zsh\n\nTo change the default shell, use the chsh command (change shell). For example, if you want to switch to bash, use this command:\n\nchsh -s /bin/bash\n\nYou can verify your new default shell by using echo with the $SHELL variable:\n\necho $SHELL\n# /bin/bash\n\nTo change the Shell prompt, add the following to the ~/.bash_profile file:\n\n\n\n\n\n\nUse the export PS1= to customize the prompt:4\n\n\nThis will change Terminal prompt to:\n\n\n\n\n\n\n\n\n\nexport PS1=\"username@\\W:\\$ \"\n\n\nusername@~:$",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Shells and Terminals</span>"
    ]
  },
  {
    "objectID": "sh_term.html#sec-terminals",
    "href": "sh_term.html#sec-terminals",
    "title": "Shells and Terminals",
    "section": "Terminals",
    "text": "Terminals\nTerminals interpret keystrokes and commands from users and send these to the shell for execution. When the shell produces output, the terminal displays it to the user. Terminal emulators allow users to interact with the shell and other command-line tools. Below is an expanded look at some commonly used terminal emulators and their key features.\n\n GNOME\nGNOME Terminal is the default terminal emulator for the GNOME desktop environment, widely used in many Linux distributions.\n\n\n\n\n\n\nKey features of GNOME\n\n\n\n\n\n\n\nProfiles\nUsers can create multiple profiles, each with its own set of preferences, including colors, fonts, and keyboard shortcuts.\nTabs and Splitting\nSupports opening multiple tabs and can split the terminal window into multiple panes.\nTransparency and Backgrounds\nAllows setting background images and adjusting the transparency of the terminal window.\nCompatibility\nSupports UTF-8 for a wide range of characters, making it suitable for international use.\n\n\n\n\n\n\n Konsole\nKonsole is part of the KDE desktop environment. It is known for its deep integration with KDE and its high degree of customizability.\n\n\n\n\n\n\nKey features of Konsole\n\n\n\n\n\n\n\nTabbed Interface\nAllows multiple tabs within a single window, facilitating multitasking.\nProfiles\nSupports multiple profiles, enabling different settings for each session.\nSplit Views\nUsers can split Konsole windows horizontally or vertically.\nTransparency and Theming\nSupports background transparency and themes, which can be customized easily.\n\n\n\n\n\n\n iTerm2\niTerm2 is a replacement for Terminal and the successor to iTerm for macOS. It offers features beyond what traditional terminals provide.\n\n\n\n\n\n\nKey features of iTerm2\n\n\n\n\n\n\n\nSplit Panes\nUsers can divide iTerm2 into multiple panes, each with its own session.\nSearch\niTerm2 allows users to search through text and highlights occurrences.\nProfiles\nSupports detailed profiles, each with its custom colors, fonts, window transparency, and key bindings.\nAdvanced Paste Features\nOffers a paste history and allows pasting with escape codes to avoid issues with unintended command executions.\nMouseless Copy\niTerm2 lets you use keyboard shortcuts to select and copy text without needing the mouse.\nShell Integration\niTerm2 can integrate with the shell to display badges, track command statuses, and more.\nTrigger Support\nExecutes user-defined actions based on text output to the terminal.\n\n\n\n\nEach of these terminal emulators offers unique features that cater to different needs and preferences, enhancing the user’s command-line experience. Whether you need deep customization, minimal resource usage, or advanced functionalities like search and shell integration, there’s a terminal emulator that fits the requirement.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Shells and Terminals</span>"
    ]
  },
  {
    "objectID": "sh_term.html#recap",
    "href": "sh_term.html#recap",
    "title": "Shells and Terminals",
    "section": "Recap",
    "text": "Recap\nIn summary, the shell is the command interpreter that executes the commands, while the terminal is the application that allows you to interact with the shell.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Shells and Terminals</span>"
    ]
  },
  {
    "objectID": "sh_term.html#footnotes",
    "href": "sh_term.html#footnotes",
    "title": "Shells and Terminals",
    "section": "",
    "text": "Bash was the default command-line interface for Apple’s macOS (which is Unix-based) until the transition to zsh as the default shell in macOS Catalina.↩︎\nZsh is an extended version of Bash (Bourne Again SHell), with many improvements, and is fully compatible with the Bourne Shell.↩︎\nArs Technica has a great summary comparing Fish to other shells.↩︎\n\\W prints the basename of the current working directory. You can also specify the username (\\u) and the host name up to the first period (\\h).↩︎",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Shells and Terminals</span>"
    ]
  },
  {
    "objectID": "vms.html",
    "href": "vms.html",
    "title": "Virtual Machines",
    "section": "",
    "text": "Virtualization Software\nSetting up Unix/Linux on virtual machines (VMs) is a valuable technique for running different operating systems on a single physical machine, which is helpful for development, testing, server deployment, and more. Several virtualization tools are available, but the most popular ones include:",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Virtual Machines</span>"
    ]
  },
  {
    "objectID": "vms.html#recap",
    "href": "vms.html#recap",
    "title": "Virtual Machines",
    "section": "Recap",
    "text": "Recap\nThis chapter covered virtualization and Linux distributions. Specifically, we discussed:\n\nSetting up Unix/Linux on virtual machines, highlighting the process and popular virtualization software options.\n\nCommercial options: VMware Workstation, VMware Fusion, and Parallels Desktop\nOpen-source option: VirtualBox\n\nThe key features of three major Linux distributions:\n\nUbuntu, known for its user-friendliness\nFedora, recognized for its cutting-edge technology\nCentOS, valued for its enterprise-level stability\n\n\nEach of these distributions brings its strengths and is tailored to different segments of the Linux user base, from desktop users and hobbyists to developers and enterprise clients.\n\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Virtual Machines</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "Quarto",
    "section": "",
    "text": "Install\nQuarto can be installed from its official website. Follow the platform-specific instructions to install it on your system. Make sure Quarto has been installed correctly and is available in your system’s PATH.\nCheck if Quarto is in PATH:\nquarto --version\n## 1.5.28\nIf the Quarto version isn’t on your PATH, you’ll need to add the location of your quarto installation to PATH. You can do that with the commands below (depending on your shell).\nYou can also use which to locate quarto path:\nwhich quarto\n## /usr/local/bin/quarto\nOn macOS, you can use find in the Terminal:",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#install",
    "href": "quarto.html#install",
    "title": "Quarto",
    "section": "",
    "text": "Bash:1\n\n\necho 'export PATH=\"$PATH:/path/to/quarto\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n\n\n\nZsh:2\n\n\necho 'export PATH=\"$PATH:/path/to/quarto\"' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n\n\n\n\n\n\nfind / -name quarto 2&gt;/dev/null",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#quarto-documents",
    "href": "quarto.html#quarto-documents",
    "title": "Quarto",
    "section": "Quarto Documents",
    "text": "Quarto Documents\nQuarto documents are a flexible and powerful tool for creating dynamic and reproducible reports, presentations, and publications. Quarto documents can support multiple programming languages, including R, Python, Julia, and Bash. Users can combine narrative text with code in a single document, rendering it into various formats like HTML, PDF, and Word.\nThis feature proves not only invaluable for data scientists, but for anyone looking to learn a new programming language. Quarto documents are a perfect ‘sandbox’ to experiment with code, take notes, and review the outputs. Moreover, Quarto has the supports cross-referencing, advanced layout options, and beautiful web and book publishing workflows.3\nQuarto documents consist of the following elements:\n\nYAML Header: This is used to specify the document’s title, author, and output format.\nNarrative Text: Plain language explanations and context.\nCode Chunks: Executable code blocks from languages like R, Python, Bash, or Julia.\n\nAdditional features include:\n\nResults: the output is displayed directly from the executable code chunks (tables, graphs, etc.).\nCross-references and Citations: Link to figures, tables, sections, or reference bibliographic sources.\nFigures and Tables: Markdown syntax can be used to add images and tables.\nAppendices and Bibliography: Footnotes and references.\n\nIf you decide to use Quarto documents, you’ll need to decide on the tool (or development environment) you want to use. At the time of this writing, Quarto has tutorials for getting started in RStudio, VS Code, Jupyter, Neovim, or a text editor (like Sublime Text).4\n\n\n\n\n\n\nLiterate Programming\n\n\n\n\n\n\nLiterate programming is a programming paradigm that intertwines code with human-readable narrative, allowing programmers to write programs in the order best suited for human understanding. Donald Knuth developed this approach to make code more understandable and maintainable.\n\n‘Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.’ - Donald Knuth. Literate Programming (1984) in Literate Programming. CSLI, 1992, pg. 99.\n\nLiterate programming can foster clearer communication of complex programming concepts and enhances collaboration among developers.\n\n\n\n\n\nYAML header\nYAML is a lightweight markup language that’s easy to write and read. In Quarto, the YAML header is used to configure document properties such as the title, engine, output format, and more. It serves as the foundation for controlling how your Quarto document behaves and appears.\nQuarto documents are written in markdown and can include executable code in various programming languages, including Unix commands. The YAML header is placed between three dashes --- at the top of each Quarto document to specify metadata and global options.\n---\ntitle: \"Using Bash\"\n---\nTo run Bash commands, specify knitr in the engine field of in the YAML header of the Quarto file, and any additional key-value pairs:5\n---\ntitle: \"Using Bash\"\nengine: knitr\nknitr:\n  opts_chunk: \n    collapse: true\n---\n\n\nCode Chunks\nOne of the powerful features of Quarto is the ability to integrate executable code chunks into Markdown documents. You can create bash code chunks using the following syntax:\n```{bash}\necho \"foo\" \n```\nBash code chunks allow you to include executable commands within your Quarto documents. You can also specify the code chunk options with the hash-pipe (#|):6\n```{bash}\n#| code-fold: show\n#| code-summary: 'show/hide echo'\necho \"foo\" \n```\nWhen the document is rendered, the narrative text is included with the output from the commnads.\n\n\nshow/hide echo\necho \"foo\"\n## foo\n\n\nThis simplicity allows authors to focus on their content rather than formatting.\n\n\nCode Chunk Isolation\nWhen incorporating Bash code chunks into Quarto documents, an essential detail to remember is the behavior of the working directory during file rendering. By default, Quarto sets the working directory to the location of the current document within the project.\nConsider the following scenario in a Quarto project:\n\n# This code chunk displays the current working directory\npwd\n## /Users/mjfrigaard/projects/books/fm-unix\n\nAlthough we can navigate to a different directory within a given code chunk:\n\ncd data # Change the current working directory to 'data' \npwd # confirm the change\n## /Users/mjfrigaard/projects/books/fm-unix/data\n\nIt’s crucial to note that Quarto resets the working directory to the document’s location for each new code chunk:\n\n# Verifying the working directory, which reverts to \npwd # the document's location for each new code chunk\n## /Users/mjfrigaard/projects/books/fm-unix\n\nThis behavior is different than what we’d see in Posit Workbench’s Terminal on my local machine:\n\nAssume the current working directory is ~/projects/ (as indicated by the blue highlighted area in the Terminal pane)\n\n\n\n\n~/projects/\n\n\n\nIf we change my working directory with .., we are in the home directory ~.\n\n\n\n\n~/\n\n\n\nWhen we check the current working directory again with pwd, we see the location has been permanently changed to ~.\n\n\n\n\nStill ~/\n\n\nThis behavior can be frustrating, but it also means we’ll start with a ‘clean slate’ in each new code chunk!\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#footnotes",
    "href": "quarto.html#footnotes",
    "title": "Quarto",
    "section": "",
    "text": "Bash is common in Linux and older macOS versions↩︎\nZsh is now the default in shell in macOS↩︎\nIn fact, this entire book was written using Quarto documents.↩︎\nFor what its worth, I’ve been using RStudio for years and love it. However, I’ve also used Quarto in VS Code and love many of it’s features.↩︎\nRead more about configuring shell code blocks in Quarto in the documentation.↩︎\nConsult the full list of code chunk options in the Quarto documentation.↩︎",
    "crumbs": [
      "Set-Ups",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "dirs.html",
    "href": "dirs.html",
    "title": "Directories",
    "section": "",
    "text": "Navigate",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Directories</span>"
    ]
  },
  {
    "objectID": "dirs.html#sec-nav-dirs",
    "href": "dirs.html#sec-nav-dirs",
    "title": "Directories",
    "section": "",
    "text": "pwd\npwd (print working directory) tells you exactly where you are in the filesystem.\n\npwd \n# /Users/username/projects/books/fm-unix\n\n\nPaths\n\n\n\n\n\n\nAbsolute file paths do not change regardless of the current working directory. pwd returns an absolute file path:\n\n\n/Users/username/projects/books/fm-unix\n\n\n\nAbsolute paths always start with / and identifies a specific location from the root directory.\n\n\n\n\n\n\nRelative file paths specify the location of a file relative to the current working directory. We can use a relative file path to view the report.txt file in the data folder with cat:\n\n\ncat data/report.txt\n# my important information\n\n\n\ndata/report.txt is the relative file path (i.e., relative to the current working directory), and it’s meaning is based on the directory from which it is referenced.\n\n\n\ncd\ncd lets us move from one directory to a different directory. For example, cd /bin takes you to the /bin folder, the toolshed of software tools.\n\ncd /bin # change location\npwd # now where am I?\n# /bin\n\n\n\nls\nls (list) lists the files and folders in a given location. In /bin, ls would show you the software tools available:\n\ncd /bin # change location\nls # what's in here?\n# [\n# bash\n# cat\n# chmod\n# cp\n# csh\n# dash\n# date\n# dd\n# df\n# echo\n# ed\n# expr\n# hostname\n# kill\n# ksh\n# launchctl\n# link\n# ln\n# ls\n# mkdir\n# mv\n# pax\n# ps\n# pwd\n# realpath\n# rm\n# rmdir\n# sh\n# sleep\n# stty\n# sync\n# tcsh\n# test\n# unlink\n# wait4path\n# zsh\n\n\n\nfind\nfind can be used to locate files or directories using the -type and -name options. The example below looks in the current working directory (.) for a folder named data:\n\nfind . -type d -name data\n# ./data",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Directories</span>"
    ]
  },
  {
    "objectID": "dirs.html#sec-manage-dirs",
    "href": "dirs.html#sec-manage-dirs",
    "title": "Directories",
    "section": "Manage",
    "text": "Manage\nIn the Unix/Linux world, file and directory management is a fundamental skill. This chapter introduces some common commands that will allow you to create, copy, move, remove, and link files and directories.\n\nmkdir\nmkdir (Make Directory) builds a new folder wherever you tell it to, like making a new folder in data for inputs (data/in) or outputs (data/out)\n\nmkdir data/in\nmkdir data/out\n\nConfirm with tree -d (the -d is for directories):\n\ntree data -d\n# data\n# ├── in\n# └── out\n# \n# 3 directories\n\n\n\ncp\ncp duplicates files or folders. The cp command is used to copy files or directories from one location to another. Imagine having a file (binary_data.tsv) on your root (.) directory that you want to copy to the /data/in folder; you could use cp to make a duplicate.\n\ncp binary_data.tsv data/in/binary_data.tsv\n\nConfirm with tree\n\ntree data/in\n# data/in\n# └── binary_data.tsv\n# \n# 1 directory, 1 file\n\n\n\nmv\nmv, short for move, moves files or directories from one location to another. We’ll use it to move data/binary_data.tsv to data/out/binary_data.tsv:\n\n# move file\nmv data/in/binary_data.tsv data/out/binary_data.tsv \n\nConfirm move with tree:1\n\ntree data -P *.tsv\n# data\n# ├── in\n# └── out\n#     └── binary_data.tsv\n# \n# 3 directories, 1 file\n\nIt can also be used for renaming files.\n\n# rename file\nmv data/out/binary_data.tsv  data/out/bin_dat.tsv \n\n\n# confirm rename \ntree data/out\n# data/out\n# └── bin_dat.tsv\n# \n# 1 directory, 1 file\n# \u001b[01;34mdata/out\u001b[0m\n# └── \u001b[00mbin_dat.tsv\u001b[0m\n# \n# 1 directory, 1 file\n\nmv is especially useful for organizing files and directories that are in the wrong place.\n\n\nrm\nThe rm command stands for remove and is used to delete files or directories.\n\n# remove doc folder\nrm data/out\n# rm: data/out: is a directory\n\nBy default, it won’t remove a directory without the -R or -r option.\n\n\n\n\n\n\nWarning\n\n\n\n\n\n\nIt’s important to note here that the command-line is not very forgiving. Using rm is a powerful action with significant consequences, as it permanently deletes files, akin to shredding documents. There’s usually no easy way to recover deleted files unless you have a backup.\n\n‘Unix is like a chainsaw. Chainsaws are powerful tools, and make many difficult tasks like cutting through thick logs quite easy. Unfortunately, this power comes with danger: chainsaws can cut just as easily through your leg.’ - Gary Bernhardt2\n\n\n\n\n\n\n# add option \nrm -R data/out",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Directories</span>"
    ]
  },
  {
    "objectID": "dirs.html#recap",
    "href": "dirs.html#recap",
    "title": "Directories",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Directories</span>"
    ]
  },
  {
    "objectID": "dirs.html#footnotes",
    "href": "dirs.html#footnotes",
    "title": "Directories",
    "section": "",
    "text": "The -P *.tsv option for tree tells it to look in data for files or folders with a .tsv extension. We’ll cover wildcards and patterns in the Symbols & Patterns chapter.↩︎\nAs quoted in Bioinformatics Data Skills: Reproducible and Robust Research with Open Source Tools (2015) by Vince Buffalo.↩︎",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Directories</span>"
    ]
  },
  {
    "objectID": "files.html",
    "href": "files.html",
    "title": "Files",
    "section": "",
    "text": "Create\nThe commands below can be used to create new files or update the time stamp of an existing file.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#create",
    "href": "files.html#create",
    "title": "Files",
    "section": "",
    "text": "touch\nWe’ll start by creating a new empty file (data/who_tb_data.tsv) with touch:\n\ntouch data/who_tb_data.tsv\n\nWe can confirm the new who_tb_data.tsv file was created, we’ll use the tree command to check the data folder:\ntree -P who_tb_data.tsv data\n# data\n# └── who_tb_data.tsv\n# \n# 1 directory, 1 file\nThe -P option lets us specify a pattern to search for in the data folder, which we’ll cover more in Symbols & Patterns.\n\n\necho\nWe can add some contents to the data/who_tb_data.tsv file using echo and the &gt; operator.1\n\necho \"country   year    type    count\nAfghanistan 1999    cases   745\nAfghanistan 1999    population  19987071\nAfghanistan 2000    cases   2666\nAfghanistan 2000    population  20595360\nBrazil  1999    cases   37737\nBrazil  1999    population  172006362\nBrazil  2000    cases   80488\nBrazil  2000    population  174504898\nChina   1999    cases   212258\nChina   1999    population  1272915272\nChina   2000    cases   213766\nChina   2000    population  1280428583\" &gt; data/who_tb_data.tsv",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#view",
    "href": "files.html#view",
    "title": "Files",
    "section": "View",
    "text": "View\ncat concatenates and displays file contents. We can use this to view the entire data/who_tb_data.tsv file we just created:\n\ncat\n\ncat data/who_tb_data.tsv\n# country   year    type    count\n# Afghanistan   1999    cases   745\n# Afghanistan   1999    population  19987071\n# Afghanistan   2000    cases   2666\n# Afghanistan   2000    population  20595360\n# Brazil    1999    cases   37737\n# Brazil    1999    population  172006362\n# Brazil    2000    cases   80488\n# Brazil    2000    population  174504898\n# China 1999    cases   212258\n# China 1999    population  1272915272\n# China 2000    cases   213766\n# China 2000    population  1280428583\n\n\n\nmore & less\nless and more lets you skim through a file on your computer, moving forwards and backwards as you please. These commands are helpful for larger files, like the Video Game Hall of Fame data stored in the data/vg_hof.tsv file:2\n\nmore data/vg_hof.tsv\n\n\n\n\nEnter ‘q’ to exit the more scroll\n\n\n\nless data/vg_hof.tsv\n\n\n\n\nEnter ‘q’ to exit the less scroll\n\n\n\n\nhead & tail\nThe head and tail commands let us view the tops and bottoms of files (the -n3 specifies three rows from data/vg_hof.tsv).\n\nhead -n3 data/vg_hof.tsv\n# year  game    developer   year_released\n# 2015  DOOM    id Software 1993\n# 2015  Pac-Man Namco   1980\n\n\ntail -n3 data/vg_hof.tsv\n# 2024  Tony Hawk's Pro Skater  Neversoft   1999\n# 2024  Ultima  Richard Garriott, Origin Systems    1981\n# 2024  You Don't Know Jack Jellyvision 1995",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#look",
    "href": "files.html#look",
    "title": "Files",
    "section": "Look",
    "text": "Look\nThese commands can help you search for files (and directories).\n\ngrep\ngrep searches files for lines matching a pattern. We’ll use it to search for a specfic video game title in data/vg_hof.tsv:\n\ngrep \"The Oregon Trail\" data/vg_hof.tsv\n# 2015  The Oregon Trail    Don Rawitsch, Bill Heinemann, and Paul Dillenberger 1971\n# 2016  The Oregon Trail    Don Rawitsch, Bill Heinemann, and Paul Dillenberger 1971\n\nfind is used to search for files and directories in a directory hierarchy based on various criteria such as name, size, file type, and modification time.\n\n\nfind\nThe .psv extension is used for pipe-separated files (|). We’ll use find to locate any .psv files in data/:\n\nfind data -name \"*.psv\"\n# data/wu_tang.psv\n\nfind can be very specific, too. For example, the commands below look in the data directory for tab-delimited files (i.e., with a .tsv extension modified in the last day.\n\nfind data -name \"*.tsv\" -mtime -1\n# data/who_tb_data.tsv\n\n\n\nlocate\nlocate finds files by name quickly using a database.3\n\nlocate who_tb_data data | head -n3\n# /Users/username/projects/books/fm-unix/data/who_tb_data.csv\n# /Users/username/projects/books/fm-unix/data/who_tb_data.psv\n# /Users/username/projects/books/fm-unix/data/who_tb_data.tsv\n\n\n\n\n\n\n\nUpdating locate database\n\n\n\n\n\n\nMake sure your locate database is up-to-date using one of the commands below if you’ve recently added or moved files to get accurate results:\n# on linux\nsudo updatedb\n# on macos\nsudo /usr/libexec/locate.updatedb",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#manage",
    "href": "files.html#manage",
    "title": "Files",
    "section": "Manage",
    "text": "Manage\nThe commands below can be used for copying, moving, renaming, and creating links to files. Assume we want to create backups of the delimiter-separated data files in the data/ folder. We’ll store these backups in folders according the file extension.\nFirst we need to create folders for each type of delimiter (.csv, .tsv, and .psv):\n\nmkdir data/csv\nmkdir data/tsv\nmkdir data/psv\n\nConfirm these new folders with tree -d\n\ntree data -d\n# data\n# ├── csv\n# ├── psv\n# └── tsv\n# \n# 4 directories\n\n\ncp\nWe’ll copy the files into their respective folder based their extension using cp and *. For reference, here is a look of the data/ folder before and after copying the .csv files:\n\ncp data/*.csv data/csv/\n\n\n\n\n\n\n\nBefore copying .csv files into data/csv/:\n\n\nAfter copying .csv files into data/csv/:4\n\n\n\n\n\n\n\n\n\ntree data -L 2 -P '*.csv'\n# data\n# ├── csv\n# ├── psv\n# ├── pwrds.csv\n# ├── tsv\n# └── wu_tang.csv\n# \n# 4 directories, 2 files\n\n\ntree data -L 2 -P '*.csv'\n# data\n# ├── csv\n# │   ├── pwrds.csv\n# │   └── wu_tang.csv\n# ├── psv\n# ├── pwrds.csv\n# ├── tsv\n# └── wu_tang.csv\n\n4 directories, 4 files\n\n\n\nNote that the number of .csv files doubled (from two files to four files). We’ll do the same for the .tsv and .psv files.\n\ncp data/*.tsv data/tsv/\n\nConfirm the .tsv files were copied:\n\ntree data/tsv/\n# data/tsv/\n# ├── music_vids.tsv\n# ├── pwrds.tsv\n# ├── trees.tsv\n# ├── vg_hof.tsv\n# ├── who_tb_data.tsv\n# └── wu_tang.tsv\n# \n# 1 directory, 6 files\n\n\ncp data/*.psv data/psv/\n\nConfirm the .psv files were copied:\n\ntree data/psv/\n# data/psv/\n# └── wu_tang.psv\n# \n# 1 directory, 1 file\n\n\n\nmv\nThe commands below create .csv and .psv versions of the who_tb_data.tsv we created above:\n\ntouch data/csv/who_tb_data.csv\necho \"country,year,type,count\nAfghanistan,1999,cases,745\nAfghanistan,1999,population,19987071\nAfghanistan,2000,cases,2666\nAfghanistan,2000,population,20595360\nBrazil,1999,cases,37737\nBrazil,1999,population,172006362\nBrazil,2000,cases,80488\nBrazil,2000,population,174504898\nChina,1999,cases,212258\nChina,1999,population,1272915272\nChina,2000,cases,213766\nChina,2000,population,1280428583\" &gt; data/csv/who_tb_data.csv\n\ntouch data/who_tb_data.psv\necho \"| country     | year | type       | count      |\n|-------------|------|------------|------------|\n| Afghanistan | 1999 | cases      | 745        |\n| Afghanistan | 1999 | population | 19987071   |\n| Afghanistan | 2000 | cases      | 2666       |\n| Afghanistan | 2000 | population | 20595360   |\n| Brazil      | 1999 | cases      | 37737      |\n| Brazil      | 1999 | population | 172006362  |\n| Brazil      | 2000 | cases      | 80488      |\n| Brazil      | 2000 | population | 174504898  |\n| China       | 1999 | cases      | 212258     |\n| China       | 1999 | population | 1272915272 |\n| China       | 2000 | cases      | 213766     |\n| China       | 2000 | population | 1280428583 |\" &gt; data/who_tb_data.psv\n\nOops–it looks like the who_tb_data.psv file was created in the data folder (and not the data/psv folder): 5\n\ntree data -L 2 -P '*who_tb_data.psv|*who_tb_data.csv'\n# data\n# ├── csv\n# │   └── who_tb_data.csv\n# ├── psv\n# ├── tsv\n# └── who_tb_data.psv\n# \n# 4 directories, 2 files\n\nWe’ll use mv to move the who_tb_data.psv and data/who_tb_data.psv files into data/psv/ and confirm with tree:6\n\nmv data/who_tb_data.psv data/psv/who_tb_data.psv\n\n\ntree data/psv\n# data/psv\n# ├── who_tb_data.psv\n# └── wu_tang.psv\n# \n# 1 directory, 2 files\n\n\n\nln\nln is used to create links between files. It can create two types of links: hard links and symbolic (or soft) links.\n\n\n\n\n\n\nA hard link is an additional name for an existing file on the same file system, and is effectively an additional directory entry for the file. In Unix/Linux file systems, all file names are technically hard links.\n\n\nA symbolic link (often called a symlink) is a file that points to another file or directory, and it contains a path to another entry somewhere in the file system.\n\n\n\nWith the ln command, you need to specify the target file first (the original file) and then the name of the new link:\n\nln original_file.txt new_link.txt\n\n\n\n\n\n\n\nln\n\n\n\n\n\n\nln doesn’t produce any output and returns zero when it’s successful.\n\n\n\n\nWe’ll use ln to create data/who_tb_data.psv, a hard link for the data file in data/psv/who_tb_data.psv:\n\nln data/psv/who_tb_data.psv data/who_tb_hardlink.psv\n\nIf we check the data folder with tree, we see the new who_tb_data.psv file looks identical to the other files:\n\ntree data -P '*.psv'\n# data\n# ├── csv\n# ├── psv\n# │   ├── who_tb_data.psv\n# │   └── wu_tang.psv\n# ├── tsv\n# ├── who_tb_hardlink.psv\n# └── wu_tang.psv\n# \n# 4 directories, 4 files\n\nHard links are basically copies–changes made to one will reflect in the other since they both refer to the same data.\nNow we’ll use ln -s to create data/who_tb_symlink.csv, a symlink for the data file in data/csv/who_tb_data.csv.\n\nln -s csv/who_tb_data.csv data/who_tb_symlink.csv\n\nWhen we look at the folder with tree now, we see the symlink is listed with a special pointer (-&gt;) to the original file:\n\ntree data -P '*.csv'\n# data\n# ├── csv\n# │   ├── pwrds.csv\n# │   ├── who_tb_data.csv\n# │   └── wu_tang.csv\n# ├── psv\n# ├── pwrds.csv\n# ├── tsv\n# ├── who_tb_symlink.csv -&gt; csv/who_tb_data.csv\n# └── wu_tang.csv\n# \n# 4 directories, 6 files\n\nThe symbolic link only references the actual file, but doesn’t store the data itself.\n\n\n\n\n\n\nTip: ln & tree\n\n\n\n\n\n\nThe tree command displays the target file first (the original file) and the link using color:\n\n\n\nColor for symlinks with tree",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#info",
    "href": "files.html#info",
    "title": "Files",
    "section": "Info",
    "text": "Info\nThe commands below return different types of information from a files (or files).\n\nls\nThe ls command lists the contents of a directory.\n\nls data/who_tb_hardlink.psv\n# data/who_tb_hardlink.psv\n\nAdding -l returns the contents in a detailed long format. The information below is from the hardlink for the .psv file.\n\nls -l data/psv/who_tb_data.psv\n# -rw-r--r--@ 2 username  staff  686 May 13 13:09 data/psv/who_tb_data.psv\n\n-l will add information like file permissions, number of links, owner, group, file size, and last modification date for each item.\nIf we check the hardlink for the .psv file:\n\nls -l data/who_tb_hardlink.psv\n# -rw-r--r--@ 2 username  staff  686 May 13 13:09 data/who_tb_hardlink.psv\n\nBoth ls -l commands return similar information for the who_tb_data.psv files, indicating they’re not treated as separate files, but rather two paths to the same file.\nThe preceding @ 2 indicates both files have two hard links, meaning who_tb_data.psv is physically located in one place but can be accessed from two locations: data/who_tb_data.psv and data/psv/who_tb_data.psv.\nCompare this to the symlink we created with it’s original file:\n\nls -l data/who_tb_symlink.csv\n# lrwxr-xr-x@ 1 username  staff  19 May 13 13:10 data/who_tb_symlink.csv -&gt; csv/who_tb_data.csv\n\nThe ls -l output for data/who_tb_data.csv (the symlink) returns lrwxr-xr-x (file permissions for a symbolic link) and includes @ 1, indicating a single hard link (symbolic links always have one link).\n\nls -l data/csv/who_tb_data.csv\n# -rw-r--r--@ 1 username  staff  381 May 13 13:09 data/csv/who_tb_data.csv\n\nThe who_tb_data.csv file in data/csv returns -rw-r--r-- (standard file permissions),\nWe’ll cover this output more in the Permissions section below.\n\n\ndiff\ndiff compare the contents of two files line-by-line. We’ll use diff to compare the pipe-separated values file (data/wu_tang.psv) to the comma-separated separated values file (data/wu_tang.csv)\n\ndiff data/wu_tang.psv data/wu_tang.csv\n\n\n\n\n\n\n\n# 1,11c1,11\n\n\nThe first line of the output indicates that lines 1 through 11 in the first file (data/wu_tang.psv) have been changed compared to lines 1 through 11 in the second file (data/wu_tang.csv).\n\n\n\n\n\n\n\n\n\nLines starting with &lt; indicate the content from the first file (data/wu_tang.psv). These entries are separated by pipes or spaces (as commonly used in PSV files).\n\n\n# &lt; |Member           |Name                 |\n# &lt; |RZA              |Robert Diggs         |\n# &lt; |GZA              |Gary Grice           |\n# &lt; |Method Man       |Clifford Smith       |\n# &lt; |Raekwon the Chef |Corey Woods          |\n# &lt; |Ghostface Killah |Dennis Coles         |\n# &lt; |Inspectah Deck   |Jason Hunter         |\n# &lt; |U-God            |Lamont Hawkins       |\n# &lt; |Masta Killa      |Jamel Irief          |\n# &lt; |Cappadonna       |Darryl Hill          |\n# &lt; |Ol Dirty Bastard |Russell Tyrone Jones |\n\n\n\n\n\n\n\n\n\n# &gt; Member,Name\n# &gt; RZA,Robert Diggs\n# &gt; GZA,Gary Grice\n# &gt; Method Man,Clifford Smith\n# &gt; Raekwon the Chef,Corey Woods\n# &gt; Ghostface Killah,Dennis Coles\n# &gt; Inspectah Deck,Jason Hunter\n# &gt; U-God,Lamont Hawkins\n# &gt; Masta Killa,Jamel Irief\n# &gt; Cappadonna,Darryl Hill\n# &gt; Ol Dirty Bastard,Russell Tyrone Jones\n\n\nLines starting with &gt; show the content from the second file (data/wu_tang.csv). These entries are separated by commas, as is typical for CSV files.\n\n\n\nThere is no difference in the actual data (Member or Name)–both files contain the same information, so the primary difference is purely in the formatting of the data: the PSV (pipe-separated Values) file uses vertical bars (|) and spaces to separate data fields, whereas the CSV (comma-separated values) file uses commas (,).7\nWhat happens when we compare the symlink (data/who_tb_data.csv) and it’s original file (data/csv/who_tb_data.csv) with diff?\n\ndiff data/who_tb_symlink.csv data/csv/who_tb_data.csv\n\ndiff returns nothing and doesn’t produce any output if there are no differences between the two files.\nDeleting, moving, or renaming the original file does not affect the integrity of a hardlink:\n\n# remove original file\nrm data/psv/who_tb_data.psv\n# check hard link\ncat data/who_tb_hardlink.psv\n# | country     | year | type       | count      |\n# |-------------|------|------------|------------|\n# | Afghanistan | 1999 | cases      | 745        |\n# | Afghanistan | 1999 | population | 19987071   |\n# | Afghanistan | 2000 | cases      | 2666       |\n# | Afghanistan | 2000 | population | 20595360   |\n# | Brazil      | 1999 | cases      | 37737      |\n# | Brazil      | 1999 | population | 172006362  |\n# | Brazil      | 2000 | cases      | 80488      |\n# | Brazil      | 2000 | population | 174504898  |\n# | China       | 1999 | cases      | 212258     |\n# | China       | 1999 | population | 1272915272 |\n# | China       | 2000 | cases      | 213766     |\n# | China       | 2000 | population | 1280428583 |\n\nHowever, if the original file for a symlink is deleted, moved, or renamed, the symbolic link breaks and typically becomes a ‘dangling’ link that points to a non-existent path.\n\n\nfile\nfile gives us a summary of what a file is or what it contains, like telling us what’s in data/who_tb_data.csv.\n\nfile data/who_tb_symlink.csv\n# data/who_tb_symlink.csv: CSV text\n\nThe -i option will tell us if this is a regular file:\n\nfile -i data/who_tb_symlink.csv\n# data/who_tb_symlink.csv: regular file\n\n\n\nreadlink\nreadlink displays the target of a symbolic link.\n\nreadlink data/who_tb_symlink.csv\n# csv/who_tb_data.csv\n\nThe -f option provides the target’s absolute path.\n\nreadlink -f data/who_tb_symlink.csv\n# path/to/data/csv/who_tb_data.csv\n\n\n\nwc\nwc (word count) counts the number of lines, words, and characters in the given input. If a file name is provided, it performs the count on the file; otherwise, it reads from the standard input.\n\nwc data/who_tb_symlink.csv\n#       13      13     381 data/who_tb_symlink.csv\n\n\n\nstat\nstat displays detailed information about files.\n\nstat data/who_tb_symlink.csv\n# 16777221 317774438 lrwxr-xr-x 1 username staff 0 19 \"May 13 13:10:34 2024\" \\\n#     \"May 13 13:10:34 2024\" \"May 13 13:10:34 2024\" \"May 13 13:10:34 2024\"     \\ \n#     4096 0 0 data/who_tb_symlink.csv\n\nAdding the -l includes the symbolic link to the original file.\n\nstat -l data/who_tb_symlink.csv\n# lrwxr-xr-x 1 username staff 19 May 13 13:10:34 2024 \\\n#     data/who_tb_symlink.csv -&gt; csv/who_tb_data.csv\n\n\n\ndu\ndu estimates file space usage.\n\ndu data/pwrds.csv\n# 24    data/pwrds.csv\n\nThe -h makes the output human readable.\n\ndu -h data/pwrds.csv\n#  12K  data/pwrds.csv\n\nIf we pass the original and symlink of who_tb_data.csv to du, we see the symlink doesn’t contain any actual data:\n\ndu -h data/csv/who_tb_data.csv\n# 4.0K  data/csv/who_tb_data.csv\n\n\ndu -h data/who_tb_symlink.csv\n#   0B  data/who_tb_symlink.csv",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#sec-file-permissions",
    "href": "files.html#sec-file-permissions",
    "title": "Files",
    "section": "Permissions",
    "text": "Permissions\nWe’ll go over file permissions in-depth in the Permissions chapter, but I’ll quickly summarize two common uses of chmod and chown below.\nFirst we’ll check the permissions with ls -l:\nls -l data/README.md\n# -rw-r--r--@ 1 username  staff  8834 May 14 09:33 data/README.md\nThe file permissions for data/README.md are stored in the combination of characters and symbols returned from ls -l:\nBreakdown of -rw-r--r--@:\n\n\n\n\n\n\n# -\n\n\nThe - indicates that the item is a regular file.\n\n\n\n\n\n\n\n\n\n# -rw-\n\n\nThe first three characters after the dash represent the user (owner) permissions. The user has read (r) and write (w) permissions but no execute (x) permission.\n\n\n\n\n\n\n\n\n\n# -rw-r--\n\n\nThe next three characters represent the group permissions. The group has read (r) permission only, with no write (w) or execute (x) permissions.\n\n\n\n\n\n\n\n\n\n# -rw-r--r--\n\n\nThe final three characters represent the permissions for others (everyone else). Like the group, others have only read (r) permission, with no write (w) or execute (x) permissions.\n\n\n\n\n\n\n\n\n\n# -rw-r--r--@\n\n\nThe @ symbol indicates that the file has extended attributes, which are additional metadata stored by the operating system.\n\n\n\n\n\n\n\n\n\n# -rw-r--r--@ 1\n\n\nThe 1 indicates the count of hard links pointing to the file, which in this case is the file itself. This is typical for regular files that haven’t been explicitly linked elsewhere.\n\n\n\n\n\nchmod\nchmod changes file permissions. To change the file permissions using chmod, using either symbolic notation or numeric (octal) notation. Below are some simple examples using both methods.\nSymbolic Notation\nTo grant the permissions above (i.e., -rw-r--r--) with symbolic notation with chmod, we could use:\nchmod u=rw,g=r,o=r data/README.md\nu=rw: sets the user (owner) permissions to read and write.\ng=r: sets the group permissions to read.\no=r: set the permissions for others to read.\nWe could also group the permissions and settings using the follow chmod command:\nchmod u=rw,go=r filename\nIn this case,\nu=rw: specifies that the user (owner) can read and write\ngo=r: specifies that both the group and others can read, but not write or execute.\nWe can also use chmod to add permissions with +. The code below grants execute permissions to the user (owner) of the file named data/README.md:\nchmod u+x data/README.md\nThis command adds (+) execute (x) permission to the user (u) of the file.\nNumeric Notation\nTo set the permissions of the file data/README.md so that the user can read and execute it, the group can read it, and others have no permissions, you would use:\nchmod 750 data/README.md\nHere, 7 stands for read (4), write (2), and execute (1) permissions for the user. 5 represents read and execute permissions for the group (4+1), and 0 means no permissions for others.\nBoth of these examples modify the permissions of data/README.md to enhance or restrict access as specified.\n\n\nchown\nchown changes file ownership\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "files.html#footnotes",
    "href": "files.html#footnotes",
    "title": "Files",
    "section": "",
    "text": "data/who_tb_data.tsv comes from the WHO global tuberculosis programme.↩︎\ndata/vg_hof.tsv is the Video Game Hall of Fame data↩︎\nlocate sometimes requires the search database is generated/updated. Read more here↩︎\nThe -L 2 option tells tree to only look in the data folder (no subfolders) and -P '*.csv' matches the .csv files.↩︎\nThe -L 2 option tells tree to only look in the data folder (no subfolders) and -P '*who_tb_data.psv|*who_tb_data.csv' matches the who_tb_data.psv or the who_tb_data.csv file.↩︎\ncp and mv also work with directories.↩︎\nThis type of difference is significant if the format impacts how data is parsed or used. For example, a software program expecting data in CSV format might not correctly parse a PSV file, and vice versa.↩︎",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Files</span>"
    ]
  },
  {
    "objectID": "sys.html",
    "href": "sys.html",
    "title": "System",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.\n\n\n\n\n\n\ntop\ntop displays the\n\ntop\n\n\n\n\ntop in iTerm2\n\n\n\n\nhtop\nhtop\n\n\nfree\nfree\n\n\ndf\ndf (disk free) shows disk usage in a human-readable format, including the size, used space, available space, and the mount point of each filesystem. By default, it displays sizes in 1K blocks but can show them in a more readable format (like MB or GB) with the -h option (human-readable).\n\ndf\n#&gt; Filesystem                       512-blocks      Used Available Capacity iused     ifree %iused  Mounted on\n#&gt; /dev/disk1s1s1                    976490576  20002808  84798224    20%  403755 423991120    0%   /\n#&gt; devfs                                   395       395         0   100%     684         0  100%   /dev\n#&gt; /dev/disk1s3                      976490576   5059568  84798224     6%    5077 423991120    0%   /System/Volumes/Preboot\n#&gt; /dev/disk1s5                      976490576   2097312  84798224     3%       1 423991120    0%   /System/Volumes/VM\n#&gt; /dev/disk1s6                      976490576     39200  84798224     1%      19 423991120    0%   /System/Volumes/Update\n#&gt; /dev/disk1s2                      976490576 861746736  84798224    92% 5662798 423991120    1%   /System/Volumes/Data\n#&gt; map auto_home                             0         0         0   100%       0         0     -   /System/Volumes/Data/home\n\n\n\ndu\ndu\n\n\nps\nps (process status) reports a snapshot of information about all running processes, regardless of the owner, including the user, CPU and memory usage, process ID, and the command that started each process. Command options can expand the selection to include other users’ processes, full command lines, etc.\n\nps\n#&gt;   PID TTY           TIME CMD\n#&gt;  2637 ttys000    0:00.04 -zsh\n#&gt;  2646 ttys001    0:00.04 -zsh\n#&gt;  2791 ttys002    0:00.03 -zsh",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>System</span>"
    ]
  },
  {
    "objectID": "vars.html",
    "href": "vars.html",
    "title": "Variables",
    "section": "",
    "text": "Unix/Linux Variables\nDefinition and significance of variables in Unix/Linux.\nOverview of environment variables vs. shell variables.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "vars.html#types-of-variables",
    "href": "vars.html#types-of-variables",
    "title": "Variables",
    "section": "Types of Variables",
    "text": "Types of Variables\nVariables come in two flavors–Shell and Environment.\n\nShell Variables\nLocal to the shell instance.\n\n\nEnvironment Variables\nAvailable system-wide and inherited by child processes.\nDifferences and use cases.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "vars.html#setting-variables",
    "href": "vars.html#setting-variables",
    "title": "Variables",
    "section": "Setting Variables",
    "text": "Setting Variables\n\nSetting Shell Variables\nBasic syntax for setting variables (varname=value).\n\n\nSetting Environment Variables\nBasic syntax for exporting environment variables (export VARNAME=value).",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "vars.html#using-variables",
    "href": "vars.html#using-variables",
    "title": "Variables",
    "section": "Using Variables",
    "text": "Using Variables\n\nUsing Shell Variables\nAccessing variable values (echo $varname).\nExamples of common shell variables.\n\n\nUsing Environment Variables\nHow to make environment variables persistent (.bashrc, .profile, etc.).\nExamples of critical environment variables (PATH, HOME, USER).",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "vars.html#variables-commands-and-utilities",
    "href": "vars.html#variables-commands-and-utilities",
    "title": "Variables",
    "section": "Variables, Commands and Utilities",
    "text": "Variables, Commands and Utilities\n\nprintenv: Displaying environment variables.\nset: Viewing shell and environment variables.\nunset: Deleting variables.\nexport: Marking variables to be exported to child processes.\nUsing env for running programs in a modified environment.\n\n\nVariables in Scripts\n\nHow to use variables to customize scripts.\nPassing variables to scripts via command line.\nReading and modifying variables within scripts.\nExample script demonstrating variable use.\n\n\n\nAdvanced Variable Features\n\nArrays and associative arrays in Bash.\nInteger operations and manipulating string variables.\nSpecial variables ($0, $1, $?, $$, $@).\n\n\n\nSecurity and Best Practices\n\nSecurity implications of improperly handling variables.\nBest practices for naming and using variables.\nAvoiding common pitfalls (e.g., overwriting critical environment variables).",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "vars.html#recp",
    "href": "vars.html#recp",
    "title": "Variables",
    "section": "Recp",
    "text": "Recp\n\nRecap of key points covered in the chapter.\nAdditional resources for deeper exploration (books, online tutorials, communities).",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "commands.html",
    "href": "commands.html",
    "title": "Commands",
    "section": "",
    "text": "REPL\nThe REPL in Bash exemplifies a powerful and flexible interface for interacting with the system, running commands, and developing scripts, providing both novice and experienced users with an efficient way to manage their computing environment.\nHere is how it works:",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Commands</span>"
    ]
  },
  {
    "objectID": "commands.html#repl",
    "href": "commands.html#repl",
    "title": "Commands",
    "section": "",
    "text": "1. Read\nIn the Bash, the “Read” step occurs when the shell waits for input from the user. This is typically represented by the shell prompt, where we can type commands.\nThe prompt might display useful information, such as the current user (username), hostname (hostname), and working directory (current_directory), depending on its configuration.\n\n  username@hostname:current_directory$\n\n\n\n\n2. Eval\nOnce a command is entered, Bash “evaluates” it. This step involves parsing the command and its arguments, checking for syntax correctness, and then executing it.\n\n  username@hostname:current_directory$ date\n\n\nCommands can be simple, such as listing the current date and time, or complex scripts involving loops, conditionals, and functions.\n\n\n3. Print\nAfter evaluating the command, Bash “prints” the output or the result of the command execution to the screen (stdout or standard output) or another specified location.\n\n  # Wed Apr 10 02:55:23 MST 2024\n\n\n\n\nLoop\nAfter executing a command and returning the output, Bash immediately returns to the “read” step, displaying the prompt and waiting for new user input.\n\nusername@hostname:current_directory$\nusername@hostname:current_directory$ date\n#&gt; Wed Apr 10 02:55:23 MST 2024\nusername@hostname:current_directory$\n\nThis cycle repeats indefinitely until the user exits the REPL environment, typically with an exit command or by pressing Ctrl + D.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Commands</span>"
    ]
  },
  {
    "objectID": "commands.html#basic-commands",
    "href": "commands.html#basic-commands",
    "title": "Commands",
    "section": "Basic commands",
    "text": "Basic commands\nIn Unix, several commands can operate without any options or arguments, performing their basic functions in their simplest form. Below are some of these commands:\n\nwho\nwho shows who is logged on the system.\n\nwho\n# username       console      Apr 14 13:45 \n# username       ttys000      Apr 14 13:45 \n# username       ttys001      Apr 14 13:45 \n# username       ttys003      Apr 16 05:56\n\nwho by itself, without options or arguments, lists the users currently logged into the system.\n\n\nwhoami\nwhoami shows the username of the user currently logged into the system.\n\nwhoami\n# username\n\n\n\nhostname\nhostname displays the system’s network name.\n\nhostname\n# Users-MacBook-Pro-2.local\n\n\n\ncal\ncal displays a calender of the current month.\n\ncal\n#      April 2024       \n# Su Mo Tu We Th Fr Sa  \n#     1  2  3  4  5  6  \n#  7  8  9 10 11 12 13  \n# 14 15 16 17 18 19 20  \n# 21 22 23 24 25 26 27  \n# 28 29 30              \n# \n\n\n\nuptime\nuptime shows how long the system has been running.\n\nuptime\n# 11:39  up 15:05, 4 users, load averages: 3.85 3.08 2.93\n\n\n\nclear\nclear clears the terminal screen and doesn’t print any return values.\n\nclear\n\nclear does its job without the need for additional input.\n\n\nexit\nexit exits the shell or your current session.\n\nexit\n\nexit requires no arguments or options to execute this action, and doesn’t print any return values.\n\n\nyes\nyes repeatedly outputs a string until killed. Without arguments, it defaults to outputting ‘y’.\n\nyes\n\nNOTE: Use Ctrl + C to interrupt the yes command.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Commands</span>"
    ]
  },
  {
    "objectID": "commands.html#recap",
    "href": "commands.html#recap",
    "title": "Commands",
    "section": "Recap",
    "text": "Recap\nEach of these commands performs a specific and often utilized function within the Unix environment, embodying the Unix philosophy of doing one thing well.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Commands</span>"
    ]
  },
  {
    "objectID": "arguments.html",
    "href": "arguments.html",
    "title": "Arguments",
    "section": "",
    "text": "Anatomy\nA Unix command can be broken down into the command name, followed by its options (which we’ll address in the next chapter), and then its arguments:\ncommand argument1 argument2 ... argument",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Arguments</span>"
    ]
  },
  {
    "objectID": "arguments.html#argument-types",
    "href": "arguments.html#argument-types",
    "title": "Arguments",
    "section": "Argument Types",
    "text": "Argument Types\nBelow are a variety of command arguments types. This is not an exhaustive list, but includes many of the commands and arguments you’ll encounter on a regular basis.\n\nDirect arguments\nDirect arguments are the most straightforward type of arguments. They are typically the names of files or directories on which commands operate.\nExample\nIn the command cat my_file.txt, my_file.txt is a direct argument to the cat command, telling it which file to display on the standard output.\n\ncat my_file.txt\n# This is my file\n\n\n\nIndirect arguments\nIndirect arguments are arguments that might specify additional information that commands need to complete their tasks.\nExample\nThe file search pattern for the grep command is an example of an indirect command, and myfile.txt is the direct argument.\n\ngrep file my_file.txt\n# This is my file\n\nMost commonly, arguments are the names of files and directory names on which the command will operate.\nExample\nmy_file.txt and tmp/tmp_file.txt are arguments representing the source and destination locations to move with mv, respectively.\n\nmv my_file.txt tmp/tmp_file.txt\n\nCommands related to user management might take user and group names names as arguments.\nExample\nchown changes the ownership of file to user and group.\n\nchown user:group myfile.txt\n\n\n\nCommand Targets\nSome commands take other commands as arguments.\nExample\nFor example, sudo command runs command with superuser privileges.\n\nsudo vi path/to/file.config\n\n\n\nData Values\nCommands might take data values as arguments for processing.\nExample\nIn echo Hello, World!, Hello, World! is the argument value that echo prints to the terminal.\n\necho Hello, World!\n# Hello, World!\n\n\n\nOrder and Position\nFor many commands, the order of the arguments is significant.\nExample\ntmp/tmp_file.txt is the first argument (indicating the file to copy from), and new_file.txt is the second argument (indicating where to copy the file to).\n\ncp tmp/tmp_file.txt new_file.txt\n\nReversing these arguments would result in a completely different operation.\nArguments that contain spaces must be quoted or escaped, so the shell understands them as a single argument rather than multiple arguments.\nExample\nTo copy the contents of the new new_file.txt to 'my new file.txt', you would use:\n\ncp new_file.txt 'my new file.txt'\n\n\n\nCommand Substitution\nThe output of a command can be used as an argument for another command using backticks (` `) or $( ).\nExample\necho $(grep file 'my file 2.txt') uses the output of the grep command as an argument for echo:\n\necho $(grep file 'my new file.txt')\n# This is my file\n\n\n\nVariables as Arguments\nEnvironment variables can be used as arguments in commands.\nExample\necho $HOME prints the path to the user’s home directory, where $HOME is an argument that the echo command interprets:\n\necho $HOME\n# /Users/username",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Arguments</span>"
    ]
  },
  {
    "objectID": "arguments.html#recap",
    "href": "arguments.html#recap",
    "title": "Arguments",
    "section": "Recap",
    "text": "Recap\nUnderstanding the nuances of Unix arguments is crucial for crafting precise and effective commands, allowing users to leverage the full power of the Unix command line for a wide array of tasks.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Arguments</span>"
    ]
  },
  {
    "objectID": "options.html",
    "href": "options.html",
    "title": "Options",
    "section": "",
    "text": "Option types\nArguments can be short or long:",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#option-types",
    "href": "options.html#option-types",
    "title": "Options",
    "section": "",
    "text": "Short options are typically a single dash followed by a single letter (e.g., -l) and they modify the command behavior in a specific, often concise way.\n\n\nLong options usually use two dashes followed by a word or compound words (e.g., --long-listing) and provide a more descriptive way to specify options, making scripts and commands more readable.\n\n\n\n\nShort options\nExample\nls -l data lists files in data in a long format, showing detailed information like permissions, owner, size, and modification date:\n\nls -l data\n# total 224\n# -rw-r--r--@ 1 mjfrigaard  staff   9067 Apr 26 22:13 README.md\n# -rw-r--r--  1 mjfrigaard  staff  12531 Apr 13 20:39 ajperlis_epigrams.txt\n# -rw-r--r--@ 1 mjfrigaard  staff   6122 Apr 10 14:04 music_vids.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff  12057 Apr 22 14:11 pwrds.csv\n# -rw-r--r--@ 1 mjfrigaard  staff  12057 Apr 22 14:11 pwrds.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff   1315 Apr  6 05:38 roxanne.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    113 Apr 18 09:27 roxanne_orig.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    117 Apr 18 09:27 roxanne_rev.txt\n# -rw-r--r--@ 1 mjfrigaard  staff   4417 Apr 10 14:01 trees.tsv\n# -rw-r--r--  1 mjfrigaard  staff   4814 Apr 10 14:07 vg_hof.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff    381 Mar 28  2023 who_tb_data.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff    381 May  1 10:17 who_tb_data.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    263 Apr 10 09:34 wu_tang.csv\n# -rw-r--r--@ 1 mjfrigaard  staff    462 Apr 15 14:07 wu_tang.dat\n# -rw-r--r--@ 1 mjfrigaard  staff    263 Apr 10 09:39 wu_tang.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff    281 Apr 10 09:38 wu_tang.txt\n\n\n\nLong options\nExample 1\ntree data --sort size lists files in order according to size:\n\ntree data --sort size\n# ├── ajperlis_epigrams.txt\n# ├── pwrds.csv\n# ├── pwrds.tsv\n# ├── README.md\n# ├── music_vids.tsv\n# ├── vg_hof.tsv\n# ├── trees.tsv\n# ├── roxanne.txt\n# ├── wu_tang.dat\n# ├── who_tb_data.tsv\n# ├── who_tb_data.txt\n# ├── wu_tang.txt\n# ├── wu_tang.csv\n# ├── wu_tang.tsv\n# ├── roxanne_rev.txt\n# └── roxanne_orig.txt\n# \n# 1 directory, 16 files\n\nExample 2\ngrep --help displays usage information for the grep command, helping users understand available options and syntax.\n\ngrep --help\n#&gt; usage: grep [-abcdDEFGHhIiJLlMmnOopqRSsUVvwXxZz] [-A num] [-B num] [-C[num]]\n#&gt;  [-e pattern] [-f file] [--binary-files=value] [--color=when]\n#&gt;  [--context[=num]] [--directories=action] [--label] [--line-buffered]\n#&gt;  [--null] [pattern] [file ...]\n\nExample 3\nThe --version option is commonly used to get version information for various commands.\n\ngrep --version  \n# grep (BSD grep, GNU compatible) 2.6.0-FreeBSD\n\nNote that not all commands support the long-form option syntax.\n\n\nModifying behavior\nSome options modify the commands behavior.\nExample\ncp-n source.txt dest.txt does not overwrite the destination file if it already exists.\n\ncat myfile.txt\n# This is my file\n\n\ncp -n myfile.txt myfile2.txt\n\nThe -n option modifies the default behavior of the cp command.\n\ncat myfile2.txt\n# This is my 2nd file\n\n\n\nModifying output\nOther options modify a commands output.\nExample 1\nls -a lists all files, including hidden ones (those starting with a dot). This option alters the command’s output by showing files that are not listed by default.\n\nls -a data\n# .\n# ..\n# README.md\n# ajperlis_epigrams.txt\n# music_vids.tsv\n# pwrds.csv\n# pwrds.tsv\n# roxanne.txt\n# roxanne_orig.txt\n# roxanne_rev.txt\n# trees.tsv\n# vg_hof.tsv\n# who_tb_data.tsv\n# who_tb_data.txt\n# wu_tang.csv\n# wu_tang.dat\n# wu_tang.tsv\n# wu_tang.txt\n\nExample 2\ndf -h shows disk space usage in human-readable form (e.g., KB, MB, GB), modifying the default output to be more easily understood.\n\ndf -h\n# Filesystem                          Size    Used   Avail Capacity iused ifree %iused  Mounted on\n# /dev/disk1s1s1                     466Gi   9.5Gi    40Gi    20%    404k  422M    0%   /\n# devfs                              201Ki   201Ki     0Bi   100%     696     0  100%   /dev\n# /dev/disk1s3                       466Gi   2.4Gi    40Gi     6%    5.1k  422M    0%   /System/Volumes/Preboot\n# /dev/disk1s5                       466Gi   1.0Gi    40Gi     3%       1  422M    0%   /System/Volumes/VM\n# /dev/disk1s6                       466Gi    19Mi    40Gi     1%      19  422M    0%   /System/Volumes/Update\n# /dev/disk1s2                       466Gi   411Gi    40Gi    92%    6.3M  422M    1%   /System/Volumes/Data\n# map auto_home                        0Bi     0Bi     0Bi   100%       0     0     -   /System/Volumes/Data/home\n\n\n\nEnvironment-specific options\nThe -u option in sort removes duplicate lines. -us behavior (considering case sensitivity) might vary depending on the locale and environment settings.\nExample\nsort -u data/roxanne.txt sorts the lines in data/roxanne.txt, removing duplicate lines.1\n\nsort -u data/roxanne.txt\n# I have to tell you just how I feel\n# I know my mind is made up\n# I loved you since I knew you\n# I won't share you with another boy\n# I wouldn't talk down to you\n# It's a bad way\n# Ro...\n# Roxanne\n# Roxanne (Put on the red light)\n# Roxanne (You don't have to put on the red light)\n# So put away your make up\n# Those days are over\n# Told you once I won't tell you again\n# Walk the streets for money\n# You don't care if it's wrong or if it's right\n# You don't have to put on the red light\n# You don't have to sell your body to the night\n# You don't have to wear that dress tonight",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#combining-options",
    "href": "options.html#combining-options",
    "title": "Options",
    "section": "Combining Options",
    "text": "Combining Options\nMultiple short options can be combined after a single dash, without spaces (e.g., -lrt) allowing users to use multiple options at once, and reducing the need to type multiple dashes.\n\nMultiple short options\nOptions can be combined with arguments when they are followed by a space and then the argument (e.g., -o filename).\nExample\nls -lrt combines three options: -l (long listing format), -r (reverse order), and -t (sort by modification time), providing a detailed, reverse-chronological list of files.\n\nls -lrt data\n# total 224\n# -rw-r--r--@ 1 mjfrigaard  staff    381 Mar 28  2023 who_tb_data.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff   1315 Apr  6 05:38 roxanne.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    263 Apr 10 09:34 wu_tang.csv\n# -rw-r--r--@ 1 mjfrigaard  staff    281 Apr 10 09:38 wu_tang.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    263 Apr 10 09:39 wu_tang.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff   4417 Apr 10 14:01 trees.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff   6122 Apr 10 14:04 music_vids.tsv\n# -rw-r--r--  1 mjfrigaard  staff   4814 Apr 10 14:07 vg_hof.tsv\n# -rw-r--r--  1 mjfrigaard  staff  12531 Apr 13 20:39 ajperlis_epigrams.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    462 Apr 15 14:07 wu_tang.dat\n# -rw-r--r--@ 1 mjfrigaard  staff    113 Apr 18 09:27 roxanne_orig.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    117 Apr 18 09:27 roxanne_rev.txt\n# -rw-r--r--@ 1 mjfrigaard  staff  12057 Apr 22 14:11 pwrds.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff  12057 Apr 22 14:11 pwrds.csv\n# -rw-r--r--@ 1 mjfrigaard  staff   9067 Apr 26 22:13 README.md\n# -rw-r--r--@ 1 mjfrigaard  staff    381 May  1 10:17 who_tb_data.txt\n\n\n\nOptions with values\nSome options require or accept an argument to specify a value related to the option’s action.\nExample\ngrep-i \"FILE\" myfile.txt uses -i to ignore case when searching for \"FILE\" in myfile.txt:\n\ngrep -i \"FILE\" myfile.txt\n# This is my file\n\nThe \"FILE\" here is an argument for the -i option.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#recap",
    "href": "options.html#recap",
    "title": "Options",
    "section": "Recap",
    "text": "Recap\nOptions greatly enhance the power and versatility of Unix commands, allowing users to tailor operations to their specific needs and preferences.\nNot all Unix-like systems or shells may support the same options for a given command, and behavior can vary between implementations. It’s important to refer to a command’s manual page (using man command or command --help) for the most accurate and comprehensive list of options and their effects.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.\n\n\n\n\n\nrm myfile.txt\nrm myfile2.txt",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "options.html#footnotes",
    "href": "options.html#footnotes",
    "title": "Options",
    "section": "",
    "text": "data/roxanne.txt contains the lyrics to the 1978 song Roxanne by The Police.↩︎",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Options</span>"
    ]
  },
  {
    "objectID": "pipes.html",
    "href": "pipes.html",
    "title": "Pipes",
    "section": "",
    "text": "Fundamental Concept\nThe pipe is placed between two commands and directs the standard output (stdout) of the command to the left of the pipe to the standard input (stdin) of the command to the right.\nExample\necho \"Hello, World!\" | wc -w sends the output of the echo command to wc, which then counts the words.\necho \"Hello, World!\" | wc -w\n#        2\nThe output is 2, indicating there are two words in “Hello, World!”.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pipes</span>"
    ]
  },
  {
    "objectID": "pipes.html#fundamental-concept",
    "href": "pipes.html#fundamental-concept",
    "title": "Pipes",
    "section": "",
    "text": "Standard Input and Output\n\n\n\n\n\n\n\n\nstdin (standard input) is a text stream from which a command reads its input. By default, it’s the keyboard, but it can be redirected to read from a file or another command’s output.\nstdout (standard output) is a text stream where a command writes its output. Typically, this is the terminal screen, but it can be redirected to a file or another command’s input.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pipes</span>"
    ]
  },
  {
    "objectID": "pipes.html#combining-pipes",
    "href": "pipes.html#combining-pipes",
    "title": "Pipes",
    "section": "Combining Pipes",
    "text": "Combining Pipes\nCommands can be chained together using multiple pipes, allowing for the creation of command pipelines where data is processed in stages.\nExample\npsaux |grep httpd lists all processes, filters those containing “httpd” (HTTPD = web server processes running):\n\nps aux | grep httpd\n# mjfrigaard       30613   0.0  0.0 33597016    632   ??  S     1:36PM   0:00.00 grep httpd\n# mjfrigaard       30611   0.0  0.0 33597548    904   ??  S     1:36PM   0:00.00 bash -c ps aux | grep httpd\n# mjfrigaard       30610   0.0  0.0 33597548    924   ??  S     1:36PM   0:00.01 sh -c 'bash'  -c 'ps aux | grep httpd' 2&gt;&1\n\nExample\nwc-l counts the number of lines:\n\nps aux | grep httpd | wc -l\n#        3\n\n\nFiltering and Processing\nExample 1\ncatdata/roxanne.txt |grep\"night\" displays lines from data/roxanne.txt that contain the number \"2\".\n\ncat data/roxanne.txt | grep \"night\"\n# You don't have to sell your body to the night\n# You don't have to wear that dress tonight\n\nHere, cat outputs the file’s contents, which grep filters.\nExample 2\nls-l data |sort-r lists the files in data in a detailed format, then sorts them in reverse order.\n\nls -l data | sort -r\n# total 184\n# -rw-r--r--@ 1 mjfrigaard  staff  12057 Apr 22 14:11 pwrds.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff  12057 Apr 22 14:11 pwrds.csv\n# -rw-r--r--@ 1 mjfrigaard  staff   8834 May  2 10:32 README.md\n# -rw-r--r--@ 1 mjfrigaard  staff   6122 Apr 10 14:04 music_vids.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff   4417 Apr 10 14:01 trees.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff   1315 Apr  6 05:38 roxanne.txt\n# -rw-r--r--@ 1 mjfrigaard  staff    462 Apr 15 14:07 wu_tang.psv\n# -rw-r--r--@ 1 mjfrigaard  staff    263 Apr 10 09:39 wu_tang.tsv\n# -rw-r--r--@ 1 mjfrigaard  staff    263 Apr 10 09:34 wu_tang.csv\n# -rw-r--r--  1 mjfrigaard  staff  12531 Apr 13 20:39 ajperlis_epigrams.txt\n# -rw-r--r--  1 mjfrigaard  staff   4814 Apr 10 14:07 vg_hof.tsv\n\nIt showcases how to reverse the listing of directory contents.\n\n\nTransformation and Reduction\nExample\nfind. -type f |xargsdu -sh |sort-h finds files (-type f) in the current directory and subdirectories, calculates their sizes (du -sh), and sorts them by size (sort -h):\n\nfind data -type f | xargs du -sh | sort -h\n# 4.0K  data/roxanne.txt\n# 4.0K  data/wu_tang.csv\n# 4.0K  data/wu_tang.psv\n# 4.0K  data/wu_tang.tsv\n# 8.0K  data/music_vids.tsv\n# 8.0K  data/trees.tsv\n# 8.0K  data/vg_hof.tsv\n#  12K  data/README.md\n#  12K  data/pwrds.csv\n#  12K  data/pwrds.tsv\n#  16K  data/ajperlis_epigrams.txt\n\nThis pipeline not only identifies files but also sorts them by their disk usage, illustrating a complex operation made simple through pipes.\n\n\nReal-time Streaming and Monitoring\nExample\ncat /var/log/system.log | grep DEAD_PROCESS prints the system.log file, continuously monitoring for new entries, filters for those containing DEAD_PROCESS, then counts the number of lines:1\n\ncat /var/log/system.log | grep \"DEAD_PROCESS\" \n## Apr 10 06:35:23 Users-MacBook-Pro login[3596]: DEAD_PROCESS: 3596 ttys000\n## Apr 10 06:35:25 Users-MacBook-Pro sessionlogoutd[19895]: DEAD_PROCESS: 225 console\n## Apr 10 10:20:25 Users-MacBook-Pro login[715]: DEAD_PROCESS: 715 ttys000\n\n\n\nData Manipulation\nExample\ncut -d':' -f1 data/roxanne.txt | sort | uniq extracts the first field from each line in data/roxanne.txt, sorts the contents alphabetically, and removes duplicates.\n\ncut -d':' -f1 data/roxanne.txt | sort | uniq\n# I have to tell you just how I feel\n# I know my mind is made up\n# I loved you since I knew you\n# I won't share you with another boy\n# I wouldn't talk down to you\n# It's a bad way\n# Ro...\n# Roxanne\n# Roxanne (Put on the red light)\n# Roxanne (You don't have to put on the red light)\n# So put away your make up\n# Those days are over\n# Told you once I won't tell you again\n# Walk the streets for money\n# You don't care if it's wrong or if it's right\n# You don't have to put on the red light\n# You don't have to sell your body to the night\n# You don't have to wear that dress tonight\n\nThis sequence is an example of performing data extraction and deduplication.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pipes</span>"
    ]
  },
  {
    "objectID": "pipes.html#pipes-with-loops",
    "href": "pipes.html#pipes-with-loops",
    "title": "Pipes",
    "section": "Pipes with Loops",
    "text": "Pipes with Loops\nThe example below demonstrates how to use the while loop with pipes with find, echo, grep, and wc.\n\nFilter with find\nfind data -name \"*.tsv\" starts in the data directory, looking for all files that end with the .tsv extension. The search is recursive, meaning it includes all subdirectories of data as well. Produces a list of paths to .tsv files, each path on a new line. This list is piped to the next command.\n\nfind data -name \"*.tsv\" \n# data/pwrds.tsv\n# data/music_vids.tsv\n# data/vg_hof.tsv\n# data/trees.tsv\n# data/wu_tang.tsv\n\n\n\nIterate with while and do\n| while read fname; do: The pipe (|) feeds the output from the find command into a while loop, which reads each line (file name) into the variable fname, one at a time. For each iteration of the loop (i.e., for each file name read into fname), the commands within the do ... done block are executed.\n\nfind data -name \"*.tsv\" | while read fname; do\n  # do this!\ndone\n\n\n\nPrint with echo\necho -n \"$fname: \": Prints the current file’s name being processed. echo-n outputs the value of fname (the path to the current .tsv file) followed by a colon and a space, without adding a newline at the end. This means the count returned by wc will be printed on the same line, right after the file name.\n\nfind data -name \"*.tsv\" | while read fname; do\n  echo -n \"$fname: \"\ndone\n# data/pwrds.tsv: data/music_vids.tsv: data/vg_hof.tsv: data/trees.tsv: data/wu_tang.tsv:\n\n\n\nSearch with grep\ngrep \"RZA\" \"$fname\": Searches for a specific pattern within the file. grep looks through the contents of the file (whose path is in fname) for lines containing the string “RZA”. Only the lines that match this pattern are printed to stdout, which is then piped to wc.\n\nfind data -name \"*.tsv\" | while read fname; do\n  echo -n \"$fname: \"\n  grep \"RZA\" \"$fname\"\ndone\n# data/pwrds.tsv: data/music_vids.tsv: data/vg_hof.tsv: data/trees.tsv: data/wu_tang.tsv: RZA   Robert Diggs\n\n\n\nCount with wc\nwc: For each file processed by the loop, wc outputs three numbers: the line count, word count, and character/byte count of the lines that grep found to contain “RZA”. Since no specific option is given to wc, it defaults to displaying all three counts.\n\nfind data -name \"*.tsv\" | while read fname; do\n  echo -n \"$fname: \"\n  grep \"RZA\" \"$fname\" | wc \ndone\n# data/pwrds.tsv:        0       0       0\n# data/music_vids.tsv:        0       0       0\n# data/vg_hof.tsv:        0       0       0\n# data/trees.tsv:        0       0       0\n# data/wu_tang.tsv:        1       3      17\n\nThis Bash command sequence combines find, a while loop, echo, grep, and wc to search through .tsv (Tab-Separated Values) files for lines containing a specific pattern (“RZA”) and reports the count of lines, words, and characters for each occurrence. Combining pipelines with loops is an efficient way to sift through a potentially large set of files within a directory, facilitating a detailed aggregation of specified conditions across multiple files.\n\n\n\n\n\n\nConsiderations when using pipes\n\n\n\n\n\n\nEfficiency and Performance\nWhile pipes are incredibly powerful, their use can impact performance, especially when processing large amounts of data. Each pipe involves creating a new subprocess, and data is copied between processes, which can lead to overhead.\nError Handling\nError handling in pipes can be non-trivial, as each command in a pipeline executes independently. Users need to consider how each command handles errors and ensure that the pipeline as a whole behaves as expected even when errors occur.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pipes</span>"
    ]
  },
  {
    "objectID": "pipes.html#recap",
    "href": "pipes.html#recap",
    "title": "Pipes",
    "section": "Recap",
    "text": "Recap\nPipes (|) allow the output of one command (stdout) to be used as the input (stdin) to another, enabling the chaining of commands to perform complex tasks with the output of one serving as the input for the next. Unix pipes embody the concept of composability in Unix, enabling users to build complex workflows out of simple, single-purpose programs. They are a testament to the flexibility and power of the Unix command line, facilitating a wide range of tasks from simple text processing to sophisticated data analysis and system monitoring.\nThis framework of commands, arguments, options, and the interplay of input (stdin), output (stdout) , and pipes enables sophisticated data processing and manipulation directly from the terminal.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pipes</span>"
    ]
  },
  {
    "objectID": "pipes.html#footnotes",
    "href": "pipes.html#footnotes",
    "title": "Pipes",
    "section": "",
    "text": "tail -f /var/log/syslog | grep sshd is useful for real-time monitoring of SSH daemon logs.↩︎",
    "crumbs": [
      "Syntax",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pipes</span>"
    ]
  },
  {
    "objectID": "symbols_patterns.html",
    "href": "symbols_patterns.html",
    "title": "Symbols & Patterns",
    "section": "",
    "text": "Wildcards\nWildcards (also known as glob patterns) are mostly used in commands to match filenames, paths, or filter text (ls, cp, mv, rm, etc.). Arguments can include wildcards, which the shell expands into a list of files or directories that match the pattern.",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Symbols & Patterns</span>"
    ]
  },
  {
    "objectID": "symbols_patterns.html#sec-wildcards",
    "href": "symbols_patterns.html#sec-wildcards",
    "title": "Symbols & Patterns",
    "section": "",
    "text": "Asterisk: *\n* is a wildcard for matching zero or more characters.\nExample\nls *.md lists all files in the data/ directory that end with .md:\n\nls data/*.md\n# data/README.md\n\n\n\nQuestion Mark: ?\n? is the wildcard for matching exactly one character.\nExample\nls myfile?.txt lists files like myfile2.txt, but not myfile.txt and my file 3.txt:\n\nls myfile?.txt\n# myfile2.txt\n\n\n\nSquare brackets: []\n[abc]: Matches any one character listed (a, b, or c).\nExample\n[a-z]: Matches any one character (n, e, or w).\n\nls [new]*.txt\n# newfile.txt\n\nExample\nNatch any one character in range (a to p).\n\nls data/[a-p]*\n# data/ajperlis_epigrams.txt\n# data/music_vids.tsv\n# data/pwrds.csv\n# data/pwrds.tsv",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Symbols & Patterns</span>"
    ]
  },
  {
    "objectID": "symbols_patterns.html#sec-regexp",
    "href": "symbols_patterns.html#sec-regexp",
    "title": "Symbols & Patterns",
    "section": "Regular Expressions",
    "text": "Regular Expressions\nRegular expressions (or the singular ‘regex’) are powerful tools for searching and manipulating text data. A regex is made up of special symbols that define specific patterns to be identified or transformed.\nRegular expressions operate on text–the sequence of characters that can include letters, digits, punctuation, and other character types. Text serves as the ‘data’ or ‘medium’ for which the patterns the regex describes are searched.\nRegular expressions are more complex than wildcards, and are typically used with tools like grep (global regular expression print), sed (stream editor), and awk.\n\nDot: .\n. matches any single character except a newline.\nExamples\nMatches lines containing “password” or similar patterns where any character stands between ‘p’ and ‘ssword’.\n\ngrep \"p.ssword\" data/pwrds.csv\n# password,rank,strength,online_crack\n# password,1,8,6.91 years\n\nReplaces “password” where any character is between ‘p’ and ‘ssword’ with “p@ssword”.\n\nsed 's/p.ssword/p@ssword/' data/pwrds.csv | head -n2\n# p@ssword,rank,strength,online_crack\n# p@ssword,1,8,6.91 years\n\nSelect records where “password” or similar patterns appear with any character between ‘p’, ‘ssw and rd’.\n\nawk '/p.ssw.rd/' data/pwrds.csv\n# password,rank,strength,online_crack\n# password,1,8,6.91 years\n# passw0rd,500,28,92.27 years\n\n\n\nAsterisk: *\n* matches zero or more of the preceding element.\nExamples\nWe can use grep to find lines where “i” is followed by zero or more “l”s (including none):\n\ngrep 'il*' data/wu_tang.txt\n# RZA   Robert Diggs\n# GZA   Gary Grice\n# Method Man    Clifford Smith\n# Ghostface Killah  Dennis Coles\n# U-God     Lamont Hawkins\n# Masta Killa   Jamel Irief\n# Cappadonna    Darryl Hill\n# Ol Dirty Bastard  Russell Tyrone Jones\n\nWe can use sed to replace two or more \"l\"s with 11:\n\nsed 's/lll*/11/g' data/wu_tang.txt\n# Member    Name\n# RZA   Robert Diggs\n# GZA   Gary Grice\n# Method Man    Clifford Smith\n# Raekwon the Chef  Corey Woods\n# Ghostface Ki11ah  Dennis Coles\n# Inspectah Deck    Jason Hunter\n# U-God     Lamont Hawkins\n# Masta Ki11a   Jamel Irief\n# Cappadonna    Darryl Hi11\n# Ol Dirty Bastard  Russe11 Tyrone Jones\n\nPrint lines that start with one or more \"R\"s\n\nawk '/^ *R/' data/wu_tang.txt\n# RZA   Robert Diggs\n# Raekwon the Chef  Corey Woods\n\n\n\nPlus: +\n+ matches one or more occurrences of the preceding element.\nExamples\nUse grep with extended regular expressions to find ‘i’ followed by one or more ’l’s:\n\ngrep -E 'il+' data/wu_tang.txt\n# Ghostface Killah  Dennis Coles\n# Masta Killa   Jamel Irief\n# Cappadonna    Darryl Hill\n\nReplace one or more \"a\"s with the @:\n\nsed -E 's/a+/@/g' data/wu_tang.txt\n# Member    N@me\n# RZA   Robert Diggs\n# GZA   G@ry Grice\n# Method M@n    Clifford Smith\n# R@ekwon the Chef  Corey Woods\n# Ghostf@ce Kill@h  Dennis Coles\n# Inspect@h Deck    J@son Hunter\n# U-God     L@mont H@wkins\n# M@st@ Kill@   J@mel Irief\n# C@pp@donn@    D@rryl Hill\n# Ol Dirty B@st@rd  Russell Tyrone Jones\n\nThe + operator needs the -E option to enable extended regular expressions.\nPrint lines with text containing one or more \"Z\"s:\n\nawk '/Z+/' data/wu_tang.txt\n# RZA   Robert Diggs\n# GZA   Gary Grice\n\n\n\nQuestion Mark: ?\n? makes the preceding element optional (matches zero or one occurrence).\nExamples\nUse grep with extended regular expressions to find lines with ‘Killah’ or ‘Killah’:\n\ngrep -E 'Kill?' data/wu_tang.txt\n# Ghostface Killah  Dennis Coles\n# Masta Killa   Jamel Irief\n\nsed: Replace Ghostface with Ghost Face:\n\nsed -E 's/Ghostface?/Ghost Face/' data/wu_tang.txt\n# Member    Name\n# RZA   Robert Diggs\n# GZA   Gary Grice\n# Method Man    Clifford Smith\n# Raekwon the Chef  Corey Woods\n# Ghost Face Killah Dennis Coles\n# Inspectah Deck    Jason Hunter\n# U-God     Lamont Hawkins\n# Masta Killa   Jamel Irief\n# Cappadonna    Darryl Hill\n# Ol Dirty Bastard  Russell Tyrone Jones\n\nawk: Print lines with one or more digits.\n\nawk '/[0-9]+/' data/wu_tang.txt\n\n\n\nCharacter Set: [abc]\n[abc] matches any single character listed in the set.\nExample\nUse grep to find lines containing ‘a’, ‘b’, or ‘c’:\ngrep '[abc]' filename.txt\n\n\nCaret: ^\n^ matches the start of a line.\nExample\nUse grep to find lines that start with ‘start’:\ngrep '^start' filename.txt\n\n\nDollar: $\n$ matches the end of a line.\nExample\nUse grep to find lines that end with ‘end’:\ngrep 'end$' filename.txt\nThese patterns are extremely powerful in scripting and command-line operations for filtering and manipulating text data efficiently. Here’s how you might use them in combination across different tools:\n\nsed for substitution: Replace ‘foo’ with ‘bar’ only if ‘foo’ appears at the beginning of a line:\n\nsed 's/^foo/bar/' filename.txt\n\nawk for selection: Print lines where the first field matches ‘start’:\n\nawk '/^start/ {print $0}' filename.txt\n\nperl for advanced manipulation: Increment numbers found at the end of each line:\n\nperl -pe 's/(\\d+)$/ $1+1 /e' filename.txt",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Symbols & Patterns</span>"
    ]
  },
  {
    "objectID": "symbols_patterns.html#special-characters",
    "href": "symbols_patterns.html#special-characters",
    "title": "Symbols & Patterns",
    "section": "Special Characters",
    "text": "Special Characters\nSpecial Characters: Characters such as spaces, quotes, and others have special meanings in the shell. They need to be treated carefully when used within arguments.\n\nBraces: {}\nBrace Expansion: Similar to wildcards, brace expansion ({}) allows the creation of multiple text strings from a pattern containing braces.\nExample\ncat wu_tang.{txt,csv}\n\ncat data/wu_tang.{tsv,dat}\n# Member    Name\n# RZA   Robert Diggs\n# GZA   Gary Grice\n# Method Man    Clifford Smith\n# Raekwon the Chef  Corey Woods\n# Ghostface Killah  Dennis Coles\n# Inspectah Deck    Jason Hunter\n# U-God Lamont Hawkins\n# Masta Killa   Jamel Irief\n# Cappadonna    Darryl Hill\n# Ol Dirty Bastard  Russell Tyrone Jones\n# |Member           |Name                 |\n# |RZA              |Robert Diggs         |\n# |GZA              |Gary Grice           |\n# |Method Man       |Clifford Smith       |\n# |Raekwon the Chef |Corey Woods          |\n# |Ghostface Killah |Dennis Coles         |\n# |Inspectah Deck   |Jason Hunter         |\n# |U-God            |Lamont Hawkins       |\n# |Masta Killa      |Jamel Irief          |\n# |Cappadonna       |Darryl Hill          |\n# |Ol Dirty Bastard |Russell Tyrone Jones |\n\nExpands into:\n\ncat data/wu_tang.tsv \ncat data/wu_tang.dat\n# Member    Name\n# RZA   Robert Diggs\n# GZA   Gary Grice\n# Method Man    Clifford Smith\n# Raekwon the Chef  Corey Woods\n# Ghostface Killah  Dennis Coles\n# Inspectah Deck    Jason Hunter\n# U-God Lamont Hawkins\n# Masta Killa   Jamel Irief\n# Cappadonna    Darryl Hill\n# Ol Dirty Bastard  Russell Tyrone Jones\n# |Member           |Name                 |\n# |RZA              |Robert Diggs         |\n# |GZA              |Gary Grice           |\n# |Method Man       |Clifford Smith       |\n# |Raekwon the Chef |Corey Woods          |\n# |Ghostface Killah |Dennis Coles         |\n# |Inspectah Deck   |Jason Hunter         |\n# |U-God            |Lamont Hawkins       |\n# |Masta Killa      |Jamel Irief          |\n# |Cappadonna       |Darryl Hill          |\n# |Ol Dirty Bastard |Russell Tyrone Jones |\n\n\n\nBackslash: \\\n\\ escapes the following character, nullifying its special meaning\nExample\necho \"File name with spaces \\& special characters\" prints the text with spaces and the ampersand:\n\necho \"File name with spaces \\& special characters\"\n# File name with spaces & special characters\n\n\n\nSingle quotes: ''\nSingle quotes (' ') treat every character literally, ignoring the special meaning of all characters.\nExample\necho '$HOME' prints $HOME, not the path to the home directory:\n\necho '$HOME'\n# $HOME\n\n\n\nDouble quotes: \"\"\nDouble quotes (\" \") allow for the inclusion of special characters in an argument, except for the dollar sign ($), backticks (` `), and backslash (\\).\nExample\necho \"$HOME\" prints the path to the home directory:\n\necho \"$HOME\"\n#&gt; /Users/username\n\n\n\nTilde: ~\n~ represents the home directory of the current user.\nExample\nList the items in the user’s home directory:\n\nls ~\n#&gt; Applications\n#&gt; Creative Cloud Files\n#&gt; Desktop\n#&gt; Documents\n#&gt; Downloads\n#&gt; Dropbox\n#&gt; Fonts\n#&gt; Library\n#&gt; Movies\n#&gt; Music\n#&gt; Pictures\n#&gt; Public\n#&gt; R\n#&gt; Themes\n\n\n\nDollar Sign: $\n$ indicates a variable.\nExample\necho $PATH prints the value of the PATH environment variable:\n\necho $PATH\n\n\n\nAmpersand: &\n& runs a command in the background.\nExample\nfirefox & opens Firefox in the background, allowing the terminal to be used for other commands.\n\nfirefox &\n\n\n\nSemicolon: ;\n; separates multiple commands to be run in sequence.\nExample\ncd data; ls changes the directory to data and then lists its contents:\n\ncd data; ls\n# README.md\n# ajperlis_epigrams.txt\n# music_vids.tsv\n# pwrds.csv\n# pwrds.tsv\n# roxanne.txt\n# roxanne_orig.txt\n# roxanne_rev.txt\n# trees.tsv\n# vg_hof.tsv\n# who_tb_data.tsv\n# who_tb_data.txt\n# wu_tang.csv\n# wu_tang.dat\n# wu_tang.tsv\n# wu_tang.txt\n\n\n\nGreater Than: &gt;\nRedirection operators: &gt; directs output to a file or a device.\nExample\necho \"This is my 2nd file\" &gt; myfile2.txt writes \"This is my 2nd file\" into myfile2.txt:\n\necho \"This is my 2nd file\" &gt; myfile2.txt\n\n\n\nLess Than: &lt;\nRedirection operators: &lt; takes input from a file or a device.\nExample\nThen wc &lt; myfile2.txt counts the words in myfile2.txt:\n\nwc &lt; myfile2.txt\n#        1       5      20\n\n\n\nParentheses: ()\nParentheses can be used to group commands or for command substitution with $( ).\nExample\n(cd /data; ls) runs ls in /data without changing the current directory:\n\n(cd data; ls)\n# README.md\n# ajperlis_epigrams.txt\n# music_vids.tsv\n# pwrds.csv\n# pwrds.tsv\n# roxanne.txt\n# roxanne_orig.txt\n# roxanne_rev.txt\n# trees.tsv\n# vg_hof.tsv\n# who_tb_data.tsv\n# who_tb_data.txt\n# wu_tang.csv\n# wu_tang.dat\n# wu_tang.tsv\n# wu_tang.txt\n\n$(command) uses the output of command.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Symbols & Patterns</span>"
    ]
  },
  {
    "objectID": "text_commands.html",
    "href": "text_commands.html",
    "title": "Manipulating Text",
    "section": "",
    "text": "The Text Stream\nUnix/Linux conceptualizes text as a stream, a continuous sequence of characters that can be manipulated in real-time. Streams are crucial for understanding how Unix/Linux commands process text. A text stream can originate from files, input devices, or even the output of other commands. Treating text as a steady stream of inputs offers a versatile and powerful method for text manipulation.",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Manipulating Text</span>"
    ]
  },
  {
    "objectID": "text_commands.html#the-text-stream",
    "href": "text_commands.html#the-text-stream",
    "title": "Manipulating Text",
    "section": "",
    "text": "Refresher: Standard Input and Standard Output\n\n\n\n\n\n\nTwo key concepts in Unix text processing are standard input (stdin) and standard output (stdout). stdin is the default input stream, which often comes from the keyboard or the output of another command. stdout is the default output stream, typically the terminal screen. Many Unix commands read from stdin when no file is specified and write to stdout, allowing the output of one command to become the input of another. This design facilitates the chaining of commands (piping) to perform complex operations in a streamlined manner.\n\nInput generally refers to the data fed into a command, which can come from stdin or be specified as arguments.\nOutput is the data produced by a command, displayed on stdout unless redirected.\n\nThis interconnectivity of stdin and stdout, all communicating through text streams, exemplifies the efficiency and flexibility of Unix-like systems.",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Manipulating Text</span>"
    ]
  },
  {
    "objectID": "text_commands.html#text-manipulation",
    "href": "text_commands.html#text-manipulation",
    "title": "Manipulating Text",
    "section": "Text Manipulation",
    "text": "Text Manipulation\nText manipulation commands embody the Unix philosophy of ‘do one thing and do it well’ and demonstrate the system’s power in processing text streams. This section will explore these fundamental commands, illustrating how they easily interact with text streams, standard input (stdin), and standard output (stdout) to perform complex text manipulations.\n\nWu-Tang\nThe data/ folder contains three different file formats of the members of the Wu-Tang American hip hop collective. We’ll use the tree command to view the contents of the data/ directory:1\n\ntree -P 'wu*' data\n\nThe -P 'wu*' option tells tree to include only files and directories that match the pattern 'wu*'. The pattern here uses a wildcard (*), meaning it will match any file or directory name that starts with \"wu\". The pattern is case-sensitive by default.\n\n# data\n# ├── wu_tang.csv\n# ├── wu_tang.dat\n# ├── wu_tang.tsv\n# └── wu_tang.txt\n# \n# 1 directory, 4 files\n\nWe can add the -f option to instruct tree to display the full path for each file and directory relative to the root of the tree, instead of just showing the names.\n\ntree -Pf 'wu*' data\n\nThe command summarizes the content by showing “1 directory, 4 files”.2\n\n# data\n# ├── data/wu_tang.csv\n# ├── data/wu_tang.dat\n# ├── data/wu_tang.tsv\n# └── data/wu_tang.txt\n# \n# 1 directory, 4 files\n\nThe wu_tang.dat file contains the members in pipe-delimited format:\n\ncat data/wu_tang.dat\n# |Member           |Name                 |\n# |RZA              |Robert Diggs         |\n# |GZA              |Gary Grice           |\n# |Method Man       |Clifford Smith       |\n# |Raekwon the Chef |Corey Woods          |\n# |Ghostface Killah |Dennis Coles         |\n# |Inspectah Deck   |Jason Hunter         |\n# |U-God            |Lamont Hawkins       |\n# |Masta Killa      |Jamel Irief          |\n# |Cappadonna       |Darryl Hill          |\n# |Ol Dirty Bastard |Russell Tyrone Jones |\n\nWe can use head and tail to view specific ‘rows’ of the data:\n\nhead -n8 data/wu_tang.dat | tail -n4\n# |Raekwon the Chef |Corey Woods          |\n# |Ghostface Killah |Dennis Coles         |\n# |Inspectah Deck   |Jason Hunter         |\n# |U-God            |Lamont Hawkins       |\n\n\n\nEpigrams\necho prints its arguments to the standard output (stdout). It can be used in scripts and on the command line to display messages or variables.\n\necho \"Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\"\n# Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\n\necho can also be used to write text to a file created with touch. The quote below comes from Alan Perlis’s 1982 article, “Epigrams on Programming.”3\n\ntouch data/turing_tarpit.txt\necho \"Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\" &gt; data/turing_tarpit.txt\n\ncat displays the content of files straight to the screen, useful for checking what’s in a file quickly.\n\ncat data/turing_tarpit.txt\n# Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\n\nAll 130 epigrams are stored and numbered in the data/perlis_epigrams.txt file.\nhead and tail allow us to view the top and bottom of any text file:\n\nhead data/ajperlis_epigrams.txt\n# One man’s constant is another man’s variable.\n# Functions delay binding; data structures induce binding. Moral: Structure data late in the programming process.\n# Syntactic sugar causes cancer of the semicolon.\n# Every program is a part of some other program and rarely fits.\n# If a program manipulates a large amount of data, it does so in a small number of ways.\n# Symmetry is a complexity-reducing concept (co-routines include subroutines); seek it everywhere.\n# It is easier to write an incorrect program than understand a correct one.\n# A programming language is low level when its programs require attention to the irrelevant.\n# It is better to have 100 functions operate on one data structure than 10 functions on 10 data structures.\n# Get into a rut early: Do the same process the same way. Accumulate idioms. Standardize. The only difference(!) between Shakespeare and you was the size of his idiom list - not the size of his vocabulary.\n\n\ntail data/ajperlis_epigrams.txt\n# In seeking the unattainable, simplicity only gets in the way. If there are epigrams, there must be meta-epigrams.\n# Epigrams are interfaces across which appreciation and insight flow.\n# Epigrams parametrize auras.\n# Epigrams are macros, since they are executed at read time.\n# Epigrams crystallize incongruities.\n# Epigrams retrieve deep semantics from a data base that is all procedure.\n# Epigrams scorn detail and make a point: They are a superb high-level documentation.\n# Epigrams are more like vitamins than protein.\n# Epigrams have extremely low entropy.\n# The last epigram? Neither eat nor drink them, snuff epigrams.\n\nApplying a ‘trust, but verify’ to the previous claim about the Turing tar-pit quote involves using grep (“global regular expression print”) to confirm the text in turing_tarpit.txt is also in dataperlis_epigrams.txt.\ngrep reads from stdin (or a list of files) and outputs the lines that match a specified pattern. Lets see how many epigrams in data/perlis_epigrams.txt include the word “Turing”:\n\ngrep Turing data/ajperlis_epigrams.txt\n# Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\n# What is the difference between a Turing machine and the modern computer? It’s the same as that between Hillary’s ascent of Everest and the establishment of a Hilton hotel on its peak.\n\n\nNumber sequences\nWe’ll add numbers to each of the 130 epigrams in data/ajperlis_epigrams.txt to make them easier to reference. We can use the ?sec-seq command piped into a formatting command like awk:\n\nseq 130 | awk '{print $1\".\"}' | head\n# 1.\n# 2.\n# 3.\n# 4.\n# 5.\n# 6.\n# 7.\n# 8.\n# 9.\n# 10.\n\nseq 130 generates the sequence of numbers from 1 to 130, then awk '{print $1\") \"}' takes uses the numbers from seq as the input to awk ($1) and appends a period . to each number. We can add the &gt; operator redirects the output to data/numbered_lines.txt.\n\nseq 130 | awk '{print $1\".\"}' &gt; data/numbered_lines.txt\n\nWe can now use paste to combine data/numbered_lines.txt and data/ajperlis_epigrams.txt. The -d option stands is the delimiter to be placed between the pasted lines (which we’ll use to specify a space \" \").\nWe’ll preview the head and tail of our paste before writing to a file:\n\npaste -d \" \" data/numbered_lines.txt data/ajperlis_epigrams.txt | head -n5\n# 1. One man’s constant is another man’s variable.\n# 2. Functions delay binding; data structures induce binding. Moral: Structure data late in the programming process.\n# 3. Syntactic sugar causes cancer of the semicolon.\n# 4. Every program is a part of some other program and rarely fits.\n# 5. If a program manipulates a large amount of data, it does so in a small number of ways.\n\n\npaste -d \" \" data/numbered_lines.txt data/ajperlis_epigrams.txt | tail -n5\n# 126. Epigrams retrieve deep semantics from a data base that is all procedure.\n# 127. Epigrams scorn detail and make a point: They are a superb high-level documentation.\n# 128. Epigrams are more like vitamins than protein.\n# 129. Epigrams have extremely low entropy.\n# 130. The last epigram? Neither eat nor drink them, snuff epigrams.\n\nAll 130 epigrams line up, so we’ll assign the output to the data/numbered_epigrams.txt file.\n\npaste -d \" \" data/numbered_lines.txt data/ajperlis_epigrams.txt &gt; \\\n  data/numbered_epigrams.txt\n\nNow we can re-check our Turing pattern in data/numbered_epigrams.txt:\n\ngrep Turing data/numbered_epigrams.txt\n# 54. Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\n# 83. What is the difference between a Turing machine and the modern computer? It’s the same as that between Hillary’s ascent of Everest and the establishment of a Hilton hotel on its peak.\n\n\n\n\nRoxanne\nThe data/roxanne.txt file contains the lyrics to the 1979 song Roxanne by The Police. We’ll use this file to explore several powerful Unix/Linux command-line utilities that are invaluable for searching, editing, and manipulating text data in files.\n\nGlobal substitutions\nawk is a powerful text processing tool. Here’s an example where awk uses gsub (global substitution) to replace the phrase “red light” with “green light” in the lyrics:\n\nawk '{gsub(/red light/, \"green light\"); print}' \\\n  data/roxanne.txt | head -4\n# Roxanne\n# You don't have to put on the green light\n# Those days are over\n# You don't have to sell your body to the night\n\ngsub(/red light/, \"green light\") tells awk to substitute \"red light\" with \"green light\" globally within each line. print outputs the modified line. Without this, awk would not display anything. The entire command is enclosed in single quotes to prevent the shell from interpreting any special characters.\nIf we wanted to replace \"Roxanne\" with \"Dianne\" throughout the song, we’d use:\n\nsed 's/Roxanne/Dianne/g' data/roxanne.txt\n\n\n\nshow/hide output\n# Dianne\n# You don't have to put on the red light\n# Those days are over\n# You don't have to sell your body to the night\n# Dianne\n# You don't have to wear that dress tonight\n# Walk the streets for money\n# You don't care if it's wrong or if it's right\n# Dianne\n# You don't have to put on the red light\n# Dianne\n# You don't have to put on the red light\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Ro...\n# I loved you since I knew you\n# I wouldn't talk down to you\n# I have to tell you just how I feel\n# I won't share you with another boy\n# I know my mind is made up\n# So put away your make up\n# Told you once I won't tell you again\n# It's a bad way\n# Dianne\n# You don't have to put on the red light\n# Dianne\n# You don't have to put on the red light\n# Dianne (You don't have to put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (You don't have to put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n# Dianne (Put on the red light)\n\n\nThe s stands for substitute, the pattern to be replaced (\"Roxanne\") is followed by the new text (\"Dianne\"), and the g at the end of the command tells sed to perform the substitution globally on each line, rather than stopping after the first occurrence.\nsort arranges lines of text alphabetically or numerically and uniq filters out adjacent repeated lines in a file (often used in conjunction with sort).\n\nsort data/roxanne.txt | uniq\n\n\n\nshow/hide output\n# I have to tell you just how I feel\n# I know my mind is made up\n# I loved you since I knew you\n# I won't share you with another boy\n# I wouldn't talk down to you\n# It's a bad way\n# Ro...\n# Roxanne\n# Roxanne (Put on the red light)\n# Roxanne (You don't have to put on the red light)\n# So put away your make up\n# Those days are over\n# Told you once I won't tell you again\n# Walk the streets for money\n# You don't care if it's wrong or if it's right\n# You don't have to put on the red light\n# You don't have to sell your body to the night\n# You don't have to wear that dress tonight\n\n\nThe commands above sort the lines in the file first, then filter repeated lines.\nWe’ll use awk to add line numbers to data/roxanne.txt. NR is the record number variable in awk, which counts the lines. $0 represents the entire current line, and combining them with print will print the line number followed by the original line.\n\nawk '{print NR, $0}' data/roxanne.txt &gt; data/roxanne_lined.txt\nhead -n5 data/roxanne_lined.txt \n# 1 Roxanne\n# 2 You don't have to put on the red light\n# 3 Those days are over\n# 4 You don't have to sell your body to the night\n# 5 Roxanne\n\n\n\nTwo-file commands\nWe’ll create a ‘metadata’ file for Roxanne in data/roxanne_meta.txt and add some content:\n\ntouch data/roxanne_meta.txt\necho \"1 &lt;Song Title&gt;\n2 &lt;Chorus&gt;\n3 &lt;Verse 1&gt;\n4 &lt;Verse 2&gt;\n5 &lt;Song Title&gt;\" &gt; data/roxanne_meta.txt\ncat data/roxanne_meta.txt\n# 1 &lt;Song Title&gt;\n# 2 &lt;Chorus&gt;\n# 3 &lt;Verse 1&gt;\n# 4 &lt;Verse 2&gt;\n# 5 &lt;Song Title&gt;\n\njoin is used to combine two files based on a common field. Assuming there’s another file with additional details for some lines, you would use:\n\njoin -1 1 -2 1 data/roxanne_lined.txt data/roxanne_meta.txt\n# 1 Roxanne &lt;Song Title&gt;\n# 2 You don't have to put on the red light &lt;Chorus&gt;\n# 3 Those days are over &lt;Verse 1&gt;\n# 4 You don't have to sell your body to the night &lt;Verse 2&gt;\n# 5 Roxanne &lt;Song Title&gt;\n\n-1 1 specifies that the join field for the first file (data/roxanne_lined.txt) is the first column, and -2 1 means that the join field for the second file (data/roxanne_meta.txt) is also the first column.\ncomm is used to compare two sorted files line by line and outputs three columns by default:\n1. Lines unique to the first file. 2. Lines unique to the second file. 3. Lines common to both files.\nLet’s assume we have two versions of the song “Roxanne”. The original version is stored in roxanne_orig.txt, and a revised version with some lines changed, added, or removed is stored in roxanne_rev.txt.\nroxanne_orig.txt\n\ntouch data/roxanne_orig.txt\necho \"Roxanne\nYou don't have to put on the red light\nThose days are over\nYou don't have to sell your body to the night\" &gt; data/roxanne_orig.txt\n\nroxanne_rev.txt\n\ntouch data/roxanne_rev.txt\necho \"Roxanne\nYou don't have to put on the green light\nThose days are over\nYou don't need to sell your dreams to the night\" &gt; data/roxanne_rev.txt\n\nThese files are structured to have similar content with minor differences.\n\n\n\n\n\n\n# Roxanne\n# You don't have to put on the red light\n# Those days are over\n# You don't have to sell your body to the night\n\n\n# Roxanne\n# You don't have to put on the green light\n# Those days are over\n# You don't need to sell your dreams to the night\n\n\n\nFirst, ensure both files are sorted (if not already). For simplicity, let’s assume these are sorted or have matching line orders. Then, use comm:\n\ncomm data/roxanne_orig.txt data/roxanne_rev.txt\n#       Roxanne\n#   You don't have to put on the green light\n#   Those days are over\n# You don't have to put on the red light\n# Those days are over\n# You don't have to sell your body to the night\n#   You don't need to sell your dreams to the night\n\nOutput:\n\nThe first column shows lines that are only in the original file (roxanne_orig.txt).\n\n\n\n\n\n\n\nThe second column shows lines that are only in the revised file (roxanne_rev.txt).\n\n\n\n\n\n\n\nThe third column shows lines that are common in both files.\n\n\n\n\n\n\nWe can suppress any of these columns using the -1, -2, or -3 options. For example, comm -12 data/roxanne_orig.txt data/roxanne_rev.txt will show only the lines that are common in both files:\n\ncomm -12 data/roxanne_orig.txt data/roxanne_rev.txt\n# Roxanne\n\nOr\n\ncomm -1 -2 data/roxanne_orig.txt data/roxanne_rev.txt\n# Roxanne\n\nNOTE: ensure the files are sorted on the lines you are comparing; otherwise, comm will not function correctly.\nTo see the line by line differences between data/roxanne_orig.txt and data/roxanne_rev.txt, pass both files to diff:\n\ndiff data/roxanne_orig.txt data/roxanne_rev.txt\n# 2c2\n# &lt; You don't have to put on the red light\n# ---\n# &gt; You don't have to put on the green light\n# 4c4\n# &lt; You don't have to sell your body to the night\n# ---\n# &gt; You don't need to sell your dreams to the night\n\nThe output of differences from diff can be interpreted as follow:\n\n2c2 indicates that a change has been made at line 2 of both files. The c stands for “change”.\n\n&lt; You don't have to put on the red light shows what line 2 looked like in the original file (roxanne_orig.txt)\n--- is a separator used by diff to distinguish between the old version and the new version of the line\n&gt; You don't have to put on the green light: shows what line 2 now looks like in the revised file (roxanne_rev.txt)\n\n4c4 indicates a change at line 4 in both documents.\n\n&lt; You don't have to sell your body to the night shows the original text at line 4 in roxanne_orig.txt\n---: Again, a separator\n&gt; You don't need to sell your dreams to the night shows the revised text at line 4 in roxanne_rev.txt\n\n\nThe -y option will display the changes side-by-side\n\ndiff -y data/roxanne_orig.txt data/roxanne_rev.txt\n# Roxanne                                                                 Roxanne\n# You don't have to put on the red light                          |       You don't have to put on the green light\n# Those days are over                                                     Those days are over\n# You don't have to sell your body to the night                   |       You don't need to sell your dreams to the night\n\nOr, if all we care about is if the files differ, we can use the -q option:\n\ndiff -q data/roxanne_orig.txt data/roxanne_rev.txt\n# Files data/roxanne_orig.txt and data/roxanne_rev.txt differ",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Manipulating Text</span>"
    ]
  },
  {
    "objectID": "text_commands.html#recap",
    "href": "text_commands.html#recap",
    "title": "Manipulating Text",
    "section": "Recap",
    "text": "Recap\nUsing these commands can dramatically enhance productivity and efficiency when working with text files in Unix/Linux environments.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Manipulating Text</span>"
    ]
  },
  {
    "objectID": "text_commands.html#footnotes",
    "href": "text_commands.html#footnotes",
    "title": "Manipulating Text",
    "section": "",
    "text": "The tree command is not available in every Shell, but you can install it using Homebrew.↩︎\nNote on Directory and File Structure: The output structure from tree -Pf 'wu*' data visually shows that the data directory contains only the files matching the pattern and no subdirectories under it that match the pattern (or any subdirectories at all in this context). The -P option does not cause tree to exclude other directories from the inspection; it only filters what is displayed based on the pattern. If there were non-matching files or subdirectories, they would not appear in the output due to the filter.↩︎\nRead the Wikipedia or download the original PDF.↩︎",
    "crumbs": [
      "Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Manipulating Text</span>"
    ]
  },
  {
    "objectID": "text_editors.html",
    "href": "text_editors.html",
    "title": "Text Editors",
    "section": "",
    "text": "nano",
    "crumbs": [
      "Text",
      "Text Editors"
    ]
  },
  {
    "objectID": "text_editors.html#vi-vim",
    "href": "text_editors.html#vi-vim",
    "title": "Text Editors",
    "section": "vi (Vim)",
    "text": "vi (Vim)\nVim, short for Vi IMproved, is an advanced text editor that is an enhanced version of the vi editor common to UNIX systems. Vim designed for both casual text editing and complex code development, making it a popular choice for developers and system administrators alike. Vim is known for its power, flexibility, and efficiency, but it can be intimidating for beginners due to its modal nature and extensive commands.\n\nGetting Started\nTo start using Vim, you can simply type vim followed by the name of the file you wish to edit or create. For example:\n\nvim example.txt\n\nThis command will open example.txt in Vim. If the file doesn’t exist, Vim will create it once you attempt to save.\n\n\nVim Modes\nVim operates in several modes, primarily:\n\nNormal Mode: The default mode where you can use vim commands. No text insertion happens in this mode.\nInsert Mode: Allows you to insert text. Enter this mode by pressing i in Normal Mode.\nCommand Mode: Accessed from Normal Mode by pressing :. Here, you can execute Vim commands and scripts.\n\n\n\nBasic Commands\nHere are some fundamental commands to get you started:\n\ni - Enter insert mode to start typing/editing the text.\nEsc - Return to normal mode from any other mode.\n:w - Save the changes made to the file.\n:q - Quit Vim.\n:wq - Save the changes and quit Vim.\ndd - Delete (cut) a line in normal mode.\nyy - Copy (yank) a line in normal mode.\np - Paste the copied or deleted line below the current line.\n\n\n\nExample Session\nLet’s consider a simple session where you edit a new file:\n\nOpen or create a file:\nvim example.txt\nEnter Insert Mode to start typing:\ni\nType your text, for instance:\nHello, this is a test file with Vim.\nPress Esc to go back to Normal Mode.\nSave and exit:\n:wq",
    "crumbs": [
      "Text",
      "Text Editors"
    ]
  },
  {
    "objectID": "text_editors.html#advanced-features",
    "href": "text_editors.html#advanced-features",
    "title": "Text Editors",
    "section": "Advanced Features",
    "text": "Advanced Features\nAs you grow more comfortable with the basics of Vim, you may explore its advanced features:\n\nMultiple Windows: Open multiple files or views using :split, :vsplit\nMacros: Automate repetitive tasks by recording them.\nCustomizable Settings: Tweak Vim’s behavior through a .vimrc file\nPlugins: Extend Vim’s functionality with plugins like NERDTree for file system navigation or YouCompleteMe for code completion.\n\nWhile Vim has a steep learning curve, mastering it can significantly enhance your productivity and efficiency in text editing tasks. Start with basic commands, gradually exploring more complex features as you become more comfortable with the editor.",
    "crumbs": [
      "Text",
      "Text Editors"
    ]
  },
  {
    "objectID": "text_editors.html#emacs",
    "href": "text_editors.html#emacs",
    "title": "Text Editors",
    "section": "emacs",
    "text": "emacs\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Text",
      "Text Editors"
    ]
  },
  {
    "objectID": "format.html",
    "href": "format.html",
    "title": "Format",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.\n\n\n\n\n\n# Create and populate the file with your data\ntouch who_tb_data.txt\necho \"country year  type  count\nAfghanistan 1999  cases 745\nAfghanistan 1999  population  19987071\nAfghanistan 2000  cases 2666\nAfghanistan 2000  population  20595360\nBrazil  1999  cases 37737\nBrazil  1999  population  172006362\nBrazil  2000  cases 80488\nBrazil  2000  population  174504898\nChina 1999  cases 212258\nChina 1999  population  1272915272\nChina 2000  cases 213766\nChina 2000  population  1280428583\" &gt; who_tb_data.txt\n\n# Get the word count values\ncounts=$(wc who_tb_data.txt | awk '{print $1, $2, $3}')\n\n# Use printf to format the output\nprintf \"   lines   words characters\\n\"\nprintf \"%8s %7s %10s\\n\" $counts\n\nTo make the commands above more generalizable so that any file can be passed as input rather than being restricted to a specific file (who_tb_data.txt), we can modify the script to take a filename as a command-line argument.\nThis way, you can use the script with any file by specifying the filename when you run the script.\n\nStep 1: Modify the Script to Take Command-Line Arguments\nHere’s how the revised script could look:\n#!/bin/bash\n\n# Check if a file name was provided as an argument\nif [ \"$#\" -ne 1 ]; then\n    echo \"Usage: $0 &lt;filename&gt;\"\n    exit 1\nfi\n\n# Check if the file exists\nif [ ! -f \"$1\" ]; then\n    echo \"File not found: $1\"\n    exit 1\nfi\n\n# Get the word count values\ncounts=$(wc \"$1\" | awk '{print $1, $2, $3}')\n\n# Use printf to format the output\nprintf \"   lines   words characters\\n\"\nprintf \"%8s %7s %10s\\n\" $counts\n\n\nStep 2: Save and Make the Script Executable\n\nSave the script in a file, for example, format_wc_output.sh.\nMake sure the script is executable:\nchmod +x format_wc_output.sh\n\n\n\nStep 3: Run the Script with a File as an Argument\n\nNow you can run the script with any file as an argument. For example:\n./format_wc_output.sh somefile.txt\n\n\n\nExplanation\n\nArgument Checking: The script now starts by checking if exactly one argument (the filename) is provided. If not, it prints a usage message and exits. This ensures the user knows how to run the script correctly.\nFile Existence Checking: It checks if the file exists before attempting to process it. If the file doesn’t exist, it prints an error message and exits. This prevents errors related to non-existent files.\nUsing Command-Line Argument: The wc command now uses $1, which is a placeholder for the first command-line argument provided to the script (i.e., the filename you want to process).\n\nThis version of the script is more flexible and useful, as it can handle any file input, making it a handy tool for quickly formatting word count output for various files across your system.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Shell Scripts",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Format</span>"
    ]
  },
  {
    "objectID": "permissions.html",
    "href": "permissions.html",
    "title": "Permissions",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.\n\n\n\n\nFile permissions with chmod\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Shell Scripts",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Permissions</span>"
    ]
  },
  {
    "objectID": "file_size.html",
    "href": "file_size.html",
    "title": "Find Large Files",
    "section": "",
    "text": "find",
    "crumbs": [
      "Use Cases",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Find Large Files</span>"
    ]
  },
  {
    "objectID": "file_size.html#du-sort",
    "href": "file_size.html#du-sort",
    "title": "Find Large Files",
    "section": "du & sort",
    "text": "du & sort",
    "crumbs": [
      "Use Cases",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Find Large Files</span>"
    ]
  },
  {
    "objectID": "file_size.html#awk-ls",
    "href": "file_size.html#awk-ls",
    "title": "Find Large Files",
    "section": "awk & ls",
    "text": "awk & ls",
    "crumbs": [
      "Use Cases",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Find Large Files</span>"
    ]
  },
  {
    "objectID": "file_size.html#ncdu",
    "href": "file_size.html#ncdu",
    "title": "Find Large Files",
    "section": "ncdu",
    "text": "ncdu",
    "crumbs": [
      "Use Cases",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Find Large Files</span>"
    ]
  },
  {
    "objectID": "file_size.html#grep-find-or-du",
    "href": "file_size.html#grep-find-or-du",
    "title": "Find Large Files",
    "section": "grep & find or du",
    "text": "grep & find or du",
    "crumbs": [
      "Use Cases",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Find Large Files</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html",
    "href": "syntax_ref.html",
    "title": "Appendix A — Syntax Reference",
    "section": "",
    "text": "Help & Documentation",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#help-documentation",
    "href": "syntax_ref.html#help-documentation",
    "title": "Appendix A — Syntax Reference",
    "section": "",
    "text": "man\nInterface to the on-line reference manuals.\n\n\nhelp\nDisplay help for built-in commands.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#files-directories",
    "href": "syntax_ref.html#files-directories",
    "title": "Appendix A — Syntax Reference",
    "section": "Files & Directories",
    "text": "Files & Directories\n\ncd\nChange directory.\n\ncd /bin # change location\n\n\n\nls\nList directory contents.\n\ncd /bin # change location\nls # what's in here?\n# [\n# bash\n# cat\n# chmod\n# cp\n# csh\n# dash\n# date\n# dd\n# df\n# echo\n# ed\n# expr\n# hostname\n# kill\n# ksh\n# launchctl\n# link\n# ln\n# ls\n# mkdir\n# mv\n# pax\n# ps\n# pwd\n# realpath\n# rm\n# rmdir\n# sh\n# sleep\n# stty\n# sync\n# tcsh\n# test\n# unlink\n# wait4path\n# zsh\n\n\n\npwd\nPrint working directory.\n\ncd /bin # change location\npwd # where am I?\n# /bin\n\n\n\nmkdir\nCreate a directory.\nmkdir out\nmkdir doc\n\n\ntree\nDisplay the directory structure of a path in a tree-like format, showing all the files and subdirectories under it.\n\ntree data\n# data\n# ├── README.md\n# ├── ajperlis_epigrams.txt\n# ├── music_vids.tsv\n# ├── pwrds.csv\n# ├── pwrds.tsv\n# ├── roxanne.txt\n# ├── roxanne_orig.txt\n# ├── roxanne_rev.txt\n# ├── trees.tsv\n# ├── vg_hof.tsv\n# ├── who_tb_data.tsv\n# ├── who_tb_data.txt\n# ├── wu_tang.csv\n# ├── wu_tang.dat\n# ├── wu_tang.tsv\n# └── wu_tang.txt\n# \n# 1 directory, 16 files\n\n\n\ncp\nCopy files or directories.\n\ncp myfile.txt data/myfile.txt\n# confirm copy\nls data\n# cp: myfile.txt: No such file or directory\n# README.md\n# ajperlis_epigrams.txt\n# music_vids.tsv\n# pwrds.csv\n# pwrds.tsv\n# roxanne.txt\n# trees.tsv\n# vg_hof.tsv\n# wu_tang.csv\n# wu_tang.psv\n# wu_tang.tsv\n\n\n\nmv\nMove or rename files or directories.\n\n# create folder\nmkdir doc\n# move file\nmv data/myfile.txt doc/myfile.txt \n# confirm move\nls doc\n# mv: rename data/myfile.txt to doc/myfile.txt: No such file or directory\n\n\n\nrm\nRemove files or directories.\n\n# remove doc folder\nrm doc\n# rm: doc: is a directory\n\nBy default, rm won’t remove a directory without the -R or -r option.\n\n\n# add option \nrm -R doc\n\n\n\nln\nCreate links between files.\nln -s /path/to/original /path/to/link\n\n\nreadlink\nreadlink displays the target of a symbolic link.\nreadlink /path/to/symlink\n\n\ntouch\nCreate a new file or update the timestamp of existing files.\n\ntouch newfile.txt\n\n\n\nfind\nSearch for files in a directory hierarchy.\n\n\nlocate\nFind files by name quickly using a database.\n\nlocate bin\n\n\n\nfile\nDetermine file type.\n\nfile /usr/bin\n\n\n\nstat\nDisplay file or file system status.\n\n\nmore\nView file contents interactively.\n\nmore data/vg_hof.tsv\n\n\n\n\nmore iTerm2\n\n\n\n\nless\nView file contents interactively.\n\nless data/vg_hof.tsv\n\n\n\n\nless iTerm2\n\n\n\n\nhead\nOutput the first part of files.\n\n\ntail\nOutput the last part of files.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#system-administration",
    "href": "syntax_ref.html#system-administration",
    "title": "Appendix A — Syntax Reference",
    "section": "System Administration",
    "text": "System Administration\n\nwho\nShow who is logged in.\n\nwho\n#  username       console      Apr  9 03:20 \n#  username       ttys000      Apr  9 03:42\n\n\n\nwhoami\nDisplay current user name.\n\nwhoami\n# username\n\n\n\nhostname\nShow or set system’s network name.\n\nhostname\n#  Users-MacBook-Pro-2.local\n\n\n\ndate\nDisplay or set the date and time.\n\ndate\n#  Wed Apr 10 03:39:52 MST 2024\n\n\n\nuptime\nShow how long the system has been running.\n\nuptime\n#  3:39  up 11:23, 2 users, load averages: 3.82 3.21 3.00\n\n\n\ncal\nShows a calendar of the current month or a specified month and year.\n\ncal\n#&gt;      April 2024       \n#&gt; Su Mo Tu We Th Fr Sa  \n#&gt;     1  2  3  4  5  6  \n#&gt;  7  8  9 10 11 12 13  \n#&gt; 14 15 16 17 18 19 20  \n#&gt; 21 22 23 24 25 26 27  \n#&gt; 28 29 30    \n\n\n\ndf\nReport file system disk space usage.\n\ndf\n# Filesystem                       512-blocks      Used Available Capacity iused     ifree %iused  Mounted on\n# /dev/disk1s1s1                    976490576  20002808  84066432    20%  403755 420332160    0%   /\n# devfs                                   395       395         0   100%     684         0  100%   /dev\n# /dev/disk1s3                      976490576   5059576  84066432     6%    5077 420332160    0%   /System/Volumes/Preboot\n# /dev/disk1s5                      976490576   4194808  84066432     5%       2 420332160    0%   /System/Volumes/VM\n# /dev/disk1s6                      976490576     39800  84066432     1%      19 420332160    0%   /System/Volumes/Update\n# /dev/disk1s2                      976490576 860380424  84066432    92% 6238430 420332160    1%   /System/Volumes/Data\n# map auto_home                             0         0         0   100%       0         0     -   /System/Volumes/Data/home\n\n\n\ndu\nShow disk usage statistics.\n\n\nps\nReport a snapshot of the current processes.\n\nps\n#   PID TTY           TIME CMD\n# 13692 ttys000    0:00.03 -zsh\n# 13703 ttys001    0:00.03 -zsh\n\n\n\nfree\n\n\ntop\nDisplay tasks and system status interactively.\n\ntop\n\n\n\n\ntop in iTerm2\n\n\n\n\niotop\nMonitor disk I/O usage by processes.\n\n\nhtop\nInteractive process viewer, similar to top but more configurable.\n\n\nvmstat\nReport virtual memory statistics.\n\n\ndmesg\nDisplay the kernel-related messages.\n\n\nclear\nClear the terminal screen.\n\nclear\n\n\n\nyes\nOutput a string repeatedly until killed.\n\nyes\n\n\n\nexit\nExits the shell or current command session.\n\nexit\n\n\n\nxargs\nBuild and execute command lines from standard input.\n\n\ncrontop\nSchedule periodic background work.\n\n\nsystemctl\nControl the systemd system and service manager.\n\n\njournalctl\nQuery and display messages from the journal.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#text-processing",
    "href": "syntax_ref.html#text-processing",
    "title": "Appendix A — Syntax Reference",
    "section": "Text Processing",
    "text": "Text Processing\n\ncat\nConcatenate files and print on the standard output.\n\n\necho\nOutputs the strings it is given to the terminal.\n\n\ngrep\nPrint lines matching a pattern.\n\n\nsort\nSort lines of text files.\n\n\nuniq\nReport or omit repeated lines.\n\n\ncut\nRemove sections from each line of files.\n\nwho | cut -c 1-16,26-42\n# mjfrigaard           May 14 05:50\n# mjfrigaard           May 14 05:51\n\n\n\npaste\nMerge lines of files.\n\n\njoin\nJoin lines of two files on a common field.\n\n\ncomm\nCompare two sorted files line by line.\n\n\ndiff\nCompare files line by line.\n\n\nwc\nPrint newline, word, and byte counts for each file.\n\n\nsed\nStream editor for filtering and transforming text.\n\n\nawk\nProgramming language for data extraction and reporting.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#networking",
    "href": "syntax_ref.html#networking",
    "title": "Appendix A — Syntax Reference",
    "section": "Networking",
    "text": "Networking\n\nping\nCheck the network connection to another host.\n\n\nnetstat\nDisplay network connections, routing tables, interface statistics, masquerade connections, and multicast memberships.\n\n\nss\nUtility to investigate sockets.\n\n\nifconfig\nConfigure or display network interface parameters for a network using TCP/IP.\n\n\ntraceroute\nPrint the route packets take to network host.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#file-archiving-transfer",
    "href": "syntax_ref.html#file-archiving-transfer",
    "title": "Appendix A — Syntax Reference",
    "section": "File Archiving & Transfer",
    "text": "File Archiving & Transfer\n\ntar\nStore and extract files from a tape or disk archive.\n\n\ngzip\nCompress or expand files.\n\n\nzip\nPackage and compress (archive) files.\n\n\nscp\nSecure copy (remote file copy program).\n\n\nrsync\nFast, versatile, remote (and local) file-copying tool.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#user-management",
    "href": "syntax_ref.html#user-management",
    "title": "Appendix A — Syntax Reference",
    "section": "User Management",
    "text": "User Management\n\nchown\nThe chown command changes the owner and/or group of specified files or directories, thereby altering who has control over these resources.\n\n\nchmod\nThe chmod command changes the access permissions of file system objects (files and directories), allowing users to specify who can read, write, or execute them.\n\n\nsudo\nExecute a command as another user, typically the superuser.\n\n\nuseradd\nCreate a new user or update default new user information.\n\n\nusermod\nModify a user’s system information.\n\n\nuserdel\nDelete a user account and related files.\n\n\ngroupadd\nCreate a new group.\n\n\npasswd\nUpdate a user’s authentication tokens/password.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "syntax_ref.html#disk-management",
    "href": "syntax_ref.html#disk-management",
    "title": "Appendix A — Syntax Reference",
    "section": "Disk Management",
    "text": "Disk Management\n\nfdisk\nPartition table manipulator for Linux.\n\n\nparted\nA partition manipulation program.\n\n\nmkfs\nBuild a Linux filesystem on a device, usually a hard disk partition.\n\n\nmount\nMount a filesystem.\n\n\numount\nUnmount file systems.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syntax Reference</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Bash\nBash, short for Bourne Again SHell, is a command line interface and scripting language for operating systems, enabling direct command input and task automation. Originally created for the GNU project and known for its flexibility and powerful features, Bash is the standard shell on many Linux distributions and was the default shell in the Terminal on macOS until the Catalina release.",
    "crumbs": [
      "Appendices",
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#sec-nfs",
    "href": "glossary.html#sec-nfs",
    "title": "Glossary",
    "section": "Network File System (NFS)",
    "text": "Network File System (NFS)\nNFS is a file system protocol that enables a user to access files over a network. It provides a central location for storing and sharing files across multiple computers and allows users to work with files on remote servers as if they were on their local machine.",
    "crumbs": [
      "Appendices",
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#sec-onprem",
    "href": "glossary.html#sec-onprem",
    "title": "Glossary",
    "section": "On-prem",
    "text": "On-prem\nOn-prem or on-premises refers to software and technology installed and running on computers on the premises of the user instead of a remote facility. On-prem can include data centers, servers, and other hardware within a company’s property. It is chosen for greater control over the computing environment and compliance reasons, as the organization is responsible for managing the security, maintenance, and updating of the systems.",
    "crumbs": [
      "Appendices",
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#sec-quarto",
    "href": "glossary.html#sec-quarto",
    "title": "Glossary",
    "section": "Quarto",
    "text": "Quarto\nQuarto is an open-source scientific and technical publishing framework designed to work with R, Python, Julia, Observable JavaScript, and more, making it a versatile tool for data scientists, researchers, and anyone involved in data analysis.",
    "crumbs": [
      "Appendices",
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#sec-yaml",
    "href": "glossary.html#sec-yaml",
    "title": "Glossary",
    "section": "YAML",
    "text": "YAML\nYAML: YAML is a human-friendly data format for configuration files and data exchange, using key-value pairs, lists, and indentation to organize data.\nkey: value\n  key: value\nIt’s readable and easily parsed by machines, making it popular for application configuration and data sharing.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Appendices",
      "Glossary"
    ]
  },
  {
    "objectID": "macos.html",
    "href": "macos.html",
    "title": "Appendix B — macOS",
    "section": "",
    "text": "macOS Terminal\nThe default Terminal app on macOS provides a solid interface to access the Unix command line, but there are other terminal emulators available (like iTerm2) that offer additional features such as split panes, search, and customization options.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>macOS</span>"
    ]
  },
  {
    "objectID": "macos.html#recap",
    "href": "macos.html#recap",
    "title": "Appendix B — macOS",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>macOS</span>"
    ]
  },
  {
    "objectID": "use_cases.html",
    "href": "use_cases.html",
    "title": "Use Cases",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.",
    "crumbs": [
      "Use Cases"
    ]
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "Basics",
    "section": "",
    "text": "Basic Unix/Linux Commands\nDirectories covers how to navigating the file system.\nFiles discusses common file operations.\nSystem touches on system monitoring.\nVariables dives into the definition, types, and use of variables. Environment variables (PATH, HOME, USER), commands used with variables (printenv, set, unset, export, env), scripts, integer and strings, and best practices.",
    "crumbs": [
      "Basics"
    ]
  },
  {
    "objectID": "basics.html#recap",
    "href": "basics.html#recap",
    "title": "Basics",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Basics"
    ]
  },
  {
    "objectID": "text.html",
    "href": "text.html",
    "title": "Text",
    "section": "",
    "text": "Plain Text Files\nPlain text files are vital to Unix/Linux systems, embodying their philosophy of ‘simple and beautiful.’1 These files only contain text, making them versatile and powerful. Unix/Linux believes that ‘everything is a file,’ including devices, configurations, and processes.2 Plain text files are the universal interface between systems, programs, and users. Standard Unix/Linux tools can easily create, manipulate, and read plain text, making it an essential interface for system administration, programming, and process management.",
    "crumbs": [
      "Text"
    ]
  },
  {
    "objectID": "text.html#plain-text-files",
    "href": "text.html#plain-text-files",
    "title": "Text",
    "section": "",
    "text": "Simple and Efficient\nPlain text files are simple, versatile, and easy to work with. They can be edited with any text editor and don’t require specialized software, making actions transparent and learning accelerated. Plain text can also be easily manipulated using standard Unix/Linux text-processing tools such as grep, sed, and awk. With simple one-liners from the command line, users can search for a specific line, replace text across multiple files, or transform data formats.\n\n\nCommunication Between Programs\nUnix/Linux philosophy values specialized programs that work together efficiently. Plain text files are used as inputs or outputs in pipelines of simple, single-purpose programs to perform complex operations.",
    "crumbs": [
      "Text"
    ]
  },
  {
    "objectID": "text.html#text-streams",
    "href": "text.html#text-streams",
    "title": "Text",
    "section": "Text Streams",
    "text": "Text Streams\nA text stream in Unix and Linux is a simple, sequential flow of characters. Text streams can be inputs from keyboards, outputs to a display screen, or the data within a file. The concept of text streams is fundamental to the Unix philosophy; it allows for the chaining together of commands, where the output of one command can be seamlessly passed as input to another through a mechanism known as piping.",
    "crumbs": [
      "Text"
    ]
  },
  {
    "objectID": "text.html#text-editors-and-the-unix-philosophy",
    "href": "text.html#text-editors-and-the-unix-philosophy",
    "title": "Text",
    "section": "Text Editors and The Unix Philosophy",
    "text": "Text Editors and The Unix Philosophy\nThe Unix philosophy emphasizes simplicity, clarity, and the principle of “doing one thing well.” Plain text embodies this philosophy, serving as a simple, straightforward, and versatile means of interaction between the user, the system, and the programs running on it. This philosophy also underpins the design of Unix text editors, which range from the simple (like nano) to the powerful and extensible (like vi and emacs).\nText commands and editors are not just tools but the medium through which users communicate with the system and manipulate it to their will. Mastering these commands and editors opens up a world of possibilities for efficient system management, programming, and beyond.\nThis section will explore the core text commands that every Unix and Linux user should know, from file manipulation to text processing and searching. We will also introduce the most popular text editors, guiding you through their primary usage and highlighting their unique features.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Text"
    ]
  },
  {
    "objectID": "text.html#footnotes",
    "href": "text.html#footnotes",
    "title": "Text",
    "section": "",
    "text": "Doug McIlroy on Unix programming: “Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface.” - Wikipedia↩︎\nLinus Torvalds (creator and lead developer of the Linux kernel) has clarified this to, “The UNIX philosophy is often quoted as ”everything is a file”, but that really means ”everything is a stream of bytes.”↩︎",
    "crumbs": [
      "Text"
    ]
  },
  {
    "objectID": "scripts.html",
    "href": "scripts.html",
    "title": "Shell Scripts",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is under development. Thank you for your patience.\n\n\n\n\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Shell Scripts"
    ]
  },
  {
    "objectID": "setups.html",
    "href": "setups.html",
    "title": "Set-Ups",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is being revised. Thank you for your patience.\n\n\n\n\nThis section details the practical aspects of preparing your environment to work with Unix/Linux systems. It is designed to guide you through various setup processes, catering to different preferences and requirements. We’ll cover working with shells and terminals, setting up a virtual machine, and using Quarto documents to execute Bash commands.\n\nShells and Terminals\nShells and Terminals covers the differences between shells and terminals, and some common options for both.\n\n\nVirtual Machines\nVirtual Machines covers how to run Unix on a virtual machine.\n\n\nQuarto\nQuarto covers how to setup up the Quarto publishing system.\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Set-Ups"
    ]
  },
  {
    "objectID": "syntax.html",
    "href": "syntax.html",
    "title": "Syntax",
    "section": "",
    "text": "Caution\n\n\n\n\n\n\nThis section is being revised. Thank you for your patience.\n\n\n\n\nIn Unix-like operating systems, the terms commands, arguments, and options refer to the components of the syntax you type into the terminal. Pipes give us the ability to combine commands.\n\nCommands\n\n\nArguments\n\n\nOptions\n\n\nPipes\n\n\n\n\n\n\nSee a typo, error, or something missing?\n\n\n\n\n\n\nPlease open an issue on GitHub.",
    "crumbs": [
      "Syntax"
    ]
  }
]